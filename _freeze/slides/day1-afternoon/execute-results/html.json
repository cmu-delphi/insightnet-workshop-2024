{
  "hash": "b0c9209a53f294feea7f719a71d429df",
  "result": {
    "markdown": "---\ntalk-title: \"Explore, clean & transform data\"\ntalk-short-title: \"{{< meta talk-title >}}\"\ntalk-subtitle: \"\"\nauthor: \"\"\nother-authors: \"\"\nrepo-address: \"cmu-delphi/insightnet-workshop-2024\"\ntalk-date: \"\"\nformat: revealjs\nexecute:\n  cache: false\n---\n\n  \n  <!-- Set any of the above to \"\" to omit them -->\n  \n  <!-- Or adjust the formatting in _titleslide.qmd -->\n---\n---\n\n\\DeclareMathOperator*{\\minimize}{minimize}\n\n\n\n\n\n\n\n::: flex\n::: w-20\n\n:::\n::: w-80\n## {{< meta talk-title >}} {background-image=\"gfx/cover-art-1.svg\" background-position=\"bottom\"}\n\n### {{< meta talk-subtitle >}}\n\n<br>\n\n#### {{< meta author >}} \n{{< meta other-authors >}}\n\n{{< meta talk-date >}}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Outline\n\n1. Essentials of `dplyr` and `tidyr` \n\n1. Epiverse software ecosystem\n\n1. Panel and versioned data in the epiverse\n\n1. Basic Nowcasting using `epiprocess`\n\n1. Motivating case study \n\n\n# Essentials of `dplyr` and `tidyr` \n\n## Down with Spreadsheets for Data Manipulation\n\n* Spreadsheets make it difficult to rerun analyses consistently.\n* Using R (and `dplyr`) allows for:\n  * Reproducibility \n  * Ease of modification\n* **Recommendation**: Avoid manual edits; instead, use code for transformations.\n\n\n## Introduction to `dplyr`\n\n* `dplyr` is a powerful package in R for data manipulation.\n* It is part of the **tidyverse**, which includes a collection of packages designed to work together.\n* We focus on basic operations like selecting and filtering data.\n* Make sure to load the necessary libraries before using `dplyr`.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Meet the Palmers\n![](gfx/meet_the_palmers.png){style=\"width: 70%;\"}\n\n<small>[Illustration from the palmerpenguins website](https://allisonhorst.github.io/palmerpenguins/)</small>\n\n## Working with the `palmerpenguins` Dataset\n\n* The `palmerpenguins` dataset is included in the `palmerpenguins` package.\n* Load both the `tidyverse` and `palmerpenguins` libraries to access and explore the dataset.\n* The dataset includes measurements of penguins such as species, bill length, flipper length, and body mass.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n:::\n:::\n\n\n## Ways to Inspect the Dataset\n\n* Use `head()` to view the first 6 row of the data (`tail()` to view the last 6 rows)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(penguins)  # First 6 rows\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n:::\n\n```{.r .cell-code}\n#tail(penguins)  # Last 6 rows\n```\n:::\n\n\n## Ways to Inspect the Dataset\n* `glimpse()` to get a compact overview of the dataset.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglimpse(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n:::\n:::\n\n\n\n## Creating Tibbles\n\n* **Tibbles**: Modern data frames with enhanced features.\n* Rows represent **observations** (or cases).\n* Columns represent **variables** (or features).\n* You can create tibbles manually using the `tibble()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(x = letters, y = 1:26)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 26 × 2\n   x         y\n   <chr> <int>\n 1 a         1\n 2 b         2\n 3 c         3\n 4 d         4\n 5 e         5\n 6 f         6\n 7 g         7\n 8 h         8\n 9 i         9\n10 j        10\n# ℹ 16 more rows\n```\n:::\n:::\n\n\n## Selecting Columns with `select()`\n\n* The `select()` function is used to pick specific columns from your dataset.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nselect(penguins, species, body_mass_g)  # Select the 'species' and 'body_mass_g' columns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 344 × 2\n   species body_mass_g\n   <fct>         <int>\n 1 Adelie         3750\n 2 Adelie         3800\n 3 Adelie         3250\n 4 Adelie           NA\n 5 Adelie         3450\n 6 Adelie         3650\n 7 Adelie         3625\n 8 Adelie         4675\n 9 Adelie         3475\n10 Adelie         4250\n# ℹ 334 more rows\n```\n:::\n:::\n\n\n## Selecting Columns with `select()`\n\n* You can exclude columns by prefixing the column names with a minus sign `-`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nselect(penguins, -species)  # Exclude the 'species' column from the dataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 344 × 7\n   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n   <fct>           <dbl>         <dbl>             <int>       <int> <fct> <int>\n 1 Torge…           39.1          18.7               181        3750 male   2007\n 2 Torge…           39.5          17.4               186        3800 fema…  2007\n 3 Torge…           40.3          18                 195        3250 fema…  2007\n 4 Torge…           NA            NA                  NA          NA <NA>   2007\n 5 Torge…           36.7          19.3               193        3450 fema…  2007\n 6 Torge…           39.3          20.6               190        3650 male   2007\n 7 Torge…           38.9          17.8               181        3625 fema…  2007\n 8 Torge…           39.2          19.6               195        4675 male   2007\n 9 Torge…           34.1          18.1               193        3475 <NA>   2007\n10 Torge…           42            20.2               190        4250 <NA>   2007\n# ℹ 334 more rows\n```\n:::\n:::\n\n\n* So, this is useful when you want to keep only certain columns or remove unnecessary ones.\n\n## Extracting Columns with `pull()`\n\n* `pull()`: Extract a column as a vector.\n* Let's try this with the `species` column...\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npull(penguins, species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo\n```\n:::\n:::\n\n\n\n## Filtering Rows with `filter()`\n\n* The `filter()` function allows you to select rows that meet specific conditions.\n* Conditions can involve column values, such as selecting only \"Gentoo\" penguins or filtering based on measurements like flipper length.\n* This enables you to narrow down your dataset to focus on relevant data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfilter(penguins, species == \"Gentoo\", flipper_length_mm < 208)  # Filter Gentoo penguins with flipper length < 208mm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>   <fct>           <dbl>         <dbl>             <int>       <int>\n1 Gentoo  Biscoe           45.1          14.5               207        5050\n2 Gentoo  Biscoe           48.4          14.4               203        4625\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n:::\n:::\n\n\n## Combining `select()` and `filter()` Functions\n\n* You can combine `select()` and `filter()` functions to refine the dataset further.\n* Use `select()` to choose columns and `filter()` to narrow rows based on conditions.\n* This helps in extracting the exact data needed for analysis.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nselect(filter(penguins, species == \"Gentoo\", flipper_length_mm < 208), species, flipper_length_mm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  species flipper_length_mm\n  <fct>               <int>\n1 Gentoo                207\n2 Gentoo                203\n```\n:::\n:::\n\n\n## Using the Pipe Operator `%>%`\n\n* The pipe operator (`%>%`) makes code more readable by chaining multiple operations together.\n* The output of one function is automatically passed to the next function.\n* This allows you to perform multiple steps (e.g., `select()` followed by `filter()`) in a clear and concise manner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This code reads more like poetry!\npenguins %>% \n  select(species, flipper_length_mm) %>%\n  filter(species == \"Gentoo\", flipper_length_mm < 208)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  species flipper_length_mm\n  <fct>               <int>\n1 Gentoo                207\n2 Gentoo                203\n```\n:::\n:::\n\n\n## Key Practices in `dplyr`\n\n* Use **tibbles** for easier data handling.\n* Use **`select()`** and **`filter()`** for data manipulation.\n* Use **`pull()`** to extract columns as vectors.\n* Use **`head()`**, **`tail()`**, and **`glimpse()`** for quick data inspection.\n* Chain functions with **`%>%`** for cleaner code.\n\n## Grouping Data with `group_by()`\n\n* Use `group_by()` to group data by one or more columns.\n* Allows performing operations on specific groups of data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins %>%\n  group_by(species) %>%\n  filter(body_mass_g == min(body_mass_g, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 8\n# Groups:   species [3]\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>     <fct>           <dbl>         <dbl>             <int>       <int>\n1 Adelie    Biscoe           36.5          16.6               181        2850\n2 Adelie    Biscoe           36.4          17.1               184        2850\n3 Gentoo    Biscoe           42.7          13.7               208        3950\n4 Chinstrap Dream            46.9          16.6               192        2700\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n:::\n:::\n\n\n## Penguin bill length and depth\n<div style=\"text-align: center;\">\n![](gfx/bill_length_depth.png){style=\"width: 60%;\"}\n\n<small>[Illustration from the palmerpenguins website](https://allisonhorst.github.io/palmerpenguins/)</small>\n</div>\n\n## Creating New Columns with `mutate()`\n\n* `mutate()` is used to create new columns.\n* Perform calculations using existing columns and assign to new columns.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins %>%\n  mutate(bill_size_mm2 = bill_depth_mm * bill_length_mm) %>% \n  select(-c(flipper_length_mm, body_mass_g, sex)) # too many cols to print\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 344 × 6\n   species island    bill_length_mm bill_depth_mm  year bill_size_mm2\n   <fct>   <fct>              <dbl>         <dbl> <int>         <dbl>\n 1 Adelie  Torgersen           39.1          18.7  2007          731.\n 2 Adelie  Torgersen           39.5          17.4  2007          687.\n 3 Adelie  Torgersen           40.3          18    2007          725.\n 4 Adelie  Torgersen           NA            NA    2007           NA \n 5 Adelie  Torgersen           36.7          19.3  2007          708.\n 6 Adelie  Torgersen           39.3          20.6  2007          810.\n 7 Adelie  Torgersen           38.9          17.8  2007          692.\n 8 Adelie  Torgersen           39.2          19.6  2007          768.\n 9 Adelie  Torgersen           34.1          18.1  2007          617.\n10 Adelie  Torgersen           42            20.2  2007          848.\n# ℹ 334 more rows\n```\n:::\n:::\n\n\n## Creating New Columns with `mutate()`\n\n* `mutate()` can create multiple new columns in one step.\n* Logical comparisons (e.g., `sex == \"male\"`) can be used within `mutate()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins %>%\n  mutate(bill_size_mm2 = bill_depth_mm * bill_length_mm, \n         TF = sex == \"male\") %>% \n  select(-c(flipper_length_mm, body_mass_g, year)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 344 × 7\n   species island    bill_length_mm bill_depth_mm sex    bill_size_mm2 TF   \n   <fct>   <fct>              <dbl>         <dbl> <fct>          <dbl> <lgl>\n 1 Adelie  Torgersen           39.1          18.7 male            731. TRUE \n 2 Adelie  Torgersen           39.5          17.4 female          687. FALSE\n 3 Adelie  Torgersen           40.3          18   female          725. FALSE\n 4 Adelie  Torgersen           NA            NA   <NA>             NA  NA   \n 5 Adelie  Torgersen           36.7          19.3 female          708. FALSE\n 6 Adelie  Torgersen           39.3          20.6 male            810. TRUE \n 7 Adelie  Torgersen           38.9          17.8 female          692. FALSE\n 8 Adelie  Torgersen           39.2          19.6 male            768. TRUE \n 9 Adelie  Torgersen           34.1          18.1 <NA>            617. NA   \n10 Adelie  Torgersen           42            20.2 <NA>            848. NA   \n# ℹ 334 more rows\n```\n:::\n:::\n\n\n## Combining `group_by()` and `mutate()`\n\n* First, group data using `group_by()`.\n* Then, use `drop_na()` from `tidyr` to exclude rows with missing values.\n* Finally, `mutate` to perform calculations for each group.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins %>%\n  drop_na() %>% # Remove all non-complete rows\n  group_by(species) %>%\n  mutate(body_mass_median = median(body_mass_g)) %>% \n  select(-c(flipper_length_mm, body_mass_g, sex)) %>% \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 6\n# Groups:   species [1]\n  species island    bill_length_mm bill_depth_mm  year body_mass_median\n  <fct>   <fct>              <dbl>         <dbl> <int>            <dbl>\n1 Adelie  Torgersen           39.1          18.7  2007             3700\n2 Adelie  Torgersen           39.5          17.4  2007             3700\n3 Adelie  Torgersen           40.3          18    2007             3700\n4 Adelie  Torgersen           36.7          19.3  2007             3700\n5 Adelie  Torgersen           39.3          20.6  2007             3700\n6 Adelie  Torgersen           38.9          17.8  2007             3700\n```\n:::\n:::\n\n\n## Conditional Calculations in `mutate()` with `if_else()`\n* `if_else()` allows conditional logic within `mutate()`.\n* Perform different operations depending on conditions, like \"big\" or \"small.\"\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt <- 800\npenguins %>%\n  mutate(bill_size_mm2 = bill_depth_mm * bill_length_mm, \n         TF = sex == \"male\",\n         bill_size_binary = if_else(bill_size_mm2 > t, \"big\", \"small\")) %>% \n  select(-c(flipper_length_mm, body_mass_g, sex)) %>% # too many cols to print\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm  year bill_size_mm2 TF   \n  <fct>   <fct>              <dbl>         <dbl> <int>         <dbl> <lgl>\n1 Adelie  Torgersen           39.1          18.7  2007          731. TRUE \n2 Adelie  Torgersen           39.5          17.4  2007          687. FALSE\n3 Adelie  Torgersen           40.3          18    2007          725. FALSE\n4 Adelie  Torgersen           NA            NA    2007           NA  NA   \n5 Adelie  Torgersen           36.7          19.3  2007          708. FALSE\n6 Adelie  Torgersen           39.3          20.6  2007          810. TRUE \n# ℹ 1 more variable: bill_size_binary <chr>\n```\n:::\n:::\n\n\n## Summarizing Data with `summarise()`\n* `summarise()` reduces data to summary statistics (e.g., mean, median).\n* Typically used after `group_by()` to summarize each group.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins %>%\n  drop_na() %>%\n  group_by(species) %>%\n  summarise(body_mass_median = median(body_mass_g))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  species   body_mass_median\n  <fct>                <dbl>\n1 Adelie                3700\n2 Chinstrap             3700\n3 Gentoo                5050\n```\n:::\n:::\n\n\n\n## Using `summarise()` with Multiple Calculations\n* Use `summarise()` to calculate multiple summary statistics at once.\n* Include multiple columns and functions in a single call.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins %>%\n  drop_na() %>%\n  group_by(species) %>%\n  summarise(body_mass_median = median(body_mass_g), \n            bill_depth_median = median(bill_depth_mm),\n            flipper_length_median = median(flipper_length_mm),\n            bill_length_mm = median(bill_length_mm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  species   body_mass_median bill_depth_median flipper_length_median\n  <fct>                <dbl>             <dbl>                 <dbl>\n1 Adelie                3700              18.4                   190\n2 Chinstrap             3700              18.4                   196\n3 Gentoo                5050              15                     216\n# ℹ 1 more variable: bill_length_mm <dbl>\n```\n:::\n:::\n\n* Yikes! This can get long. Is there a way to condense this?\n\n## Using `across()` to Apply Functions to Multiple Columns in one Swoop\n* `across()` applies a function (e.g., median) to multiple columns.\n* It's especially useful for summarizing multiple numeric columns in one step.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins %>%\n  drop_na() %>% \n  group_by(species) %>%\n  summarise(across(where(is.numeric), median))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  <fct>              <dbl>         <dbl>             <dbl>       <dbl> <dbl>\n1 Adelie              38.8          18.4               190        3700  2008\n2 Chinstrap           49.6          18.4               196        3700  2008\n3 Gentoo              47.4          15                 216        5050  2008\n```\n:::\n:::\n\n\n## Using `count()` to Aggregate Data\n* `count()` is a shortcut for grouping and summarizing the data:\n\nFor example, if we want to count the number of penguins by species, then\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins_count <- penguins %>%\n  group_by(species) %>%\n  summarize(count = n())\n```\n:::\n\n\nis equivalent to \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins_count <- penguins %>%\n  count(species)\n\npenguins_count # Let's see what the counts are.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n```\n:::\n:::\n\n\n## Tidy Data\n\n> \"Happy families are all alike; every unhappy family is unhappy in its own way.\" — Leo Tolstoy  \n\n* **Tidy datasets** are like happy families: consistent, standardized, and easy to work with.  \n* **Messy datasets** are like unhappy families: each one messy in its own unique way.  \nIn this section:\n* We'll define what makes data *tidy* and how to transform between the tidy and messy formats.\n\n## What is Tidy Data?\n\n* Tidy data follows a consistent structure: **each row represents one observation, and each column represents one variable.**\n\n<!-- * **Example:** Suppose we have the following messy/wide dataset of the counts of two species of penguins on two islands: -->\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Simple example of messy data (wide format)\npenguins_wide <- tibble(\n  island = c(\"Biscoe\", \"Biscoe\", \"Dream\"),\n  year = c(2007, 2008, 2007),\n  Adelie = c(10, 18, 20),\n  Gentoo = c(34, 46, 0)\n)\npenguins_wide\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  island  year Adelie Gentoo\n  <chr>  <dbl>  <dbl>  <dbl>\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0\n```\n:::\n:::\n\n\n##  Tidying Messy Data with `pivot_longer()`\n* To turn messy data into tidy data, we often use the `tidyr` package in the tidyverse.\n* Use `pivot_longer()` to convert data from **wide format** (multiple columns for the same variable) to **long format** (one column per variable).\n* This makes it easier to perform group-based calculations or create visualizations.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  island  year species count\n  <chr>  <dbl> <chr>   <dbl>\n1 Biscoe  2007 Adelie     10\n2 Biscoe  2007 Gentoo     34\n3 Biscoe  2008 Adelie     18\n4 Biscoe  2008 Gentoo     46\n5 Dream   2007 Adelie     20\n6 Dream   2007 Gentoo      0\n```\n:::\n:::\n\n\n## Making Data Wider with `pivot_wider()`\n\n* Sometimes, you need to convert data from long format to wide format using `pivot_wider()`.\n* This can be useful when you want to separate variables into individual columns.\n* Let's try converting `penguins_tidy` back to `penguins_wide`!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Pivoting long data back to wide format\npenguins_wide_back <- penguins_tidy %>%\n  pivot_wider(names_from = species, values_from = count)\n\npenguins_wide_back\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  island  year Adelie Gentoo\n  <chr>  <dbl>  <dbl>  <dbl>\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0\n```\n:::\n:::\n\n\n## `complete()` and `fill()` to Handle Missing Data\n<div style=\"font-size: 0.7em;\">\n1. **`complete()`**: Adds missing rows for combinations of specified variables.\n2. **`fill()`**: Fills missing values in columns, typically from previous or next available values (default is LOCF).\n</div>\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# First, use complete() to add missing year (2008 for Dream)\npenguins_complete <- penguins_wide %>%\n  complete(island, year)\npenguins_complete\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  island  year Adelie Gentoo\n  <chr>  <dbl>  <dbl>  <dbl>\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0\n4 Dream   2008     NA     NA\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Then, use fill() to fill the missing penguin counts\npenguins_complete %>%\n  fill(Adelie, Gentoo)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  island  year Adelie Gentoo\n  <chr>  <dbl>  <dbl>  <dbl>\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0\n4 Dream   2008     20      0\n```\n:::\n:::\n\n\n## Introduction to Joins in `dplyr`\n<div style=\"font-size: 0.8em;\">\n* Joining datasets is a powerful tool for combining info. from multiple sources.\n* In R, `dplyr` provides several functions to perform different types of joins.\n* We'll demonstrate joining `penguins_complete` (our penguin counts dataset) with `island_info` (dataset containing additional info. about the islands).\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Island information dataset\nisland_info <- tibble(\n  island = c(\"Biscoe\", \"Dream\", \"Torgersen\"),\n  location = c(\"Antarctica\", \"Antarctica\", \"Antarctica\"),\n  region = c(\"West\", \"East\", \"East\")\n)\n\nisland_info\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  island    location   region\n  <chr>     <chr>      <chr> \n1 Biscoe    Antarctica West  \n2 Dream     Antarctica East  \n3 Torgersen Antarctica East  \n```\n:::\n:::\n\n* Notice that the `island_info` dataset includes an island, Torgersen, that is not in `penguins_complete`.\n</div>\n\n## Left Join: Keep All Rows from the First Dataset\n\n* A **left join** keeps all rows from the **first dataset** (`penguins_complete`), and adds matching data from the second dataset (`island_info`).\n* So **all rows from the first dataset** (`penguins_complete`) will be preserved.\n* The datasets are joined by matching the `island` column, specified by the by argument.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Left join: combining penguins data with island info\npenguins_with_info <- penguins_complete %>%\n  left_join(island_info, by = \"island\")\n\npenguins_with_info\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  island  year Adelie Gentoo location   region\n  <chr>  <dbl>  <dbl>  <dbl> <chr>      <chr> \n1 Biscoe  2007     10     34 Antarctica West  \n2 Biscoe  2008     18     46 Antarctica West  \n3 Dream   2007     20      0 Antarctica East  \n4 Dream   2008     NA     NA Antarctica East  \n```\n:::\n:::\n\n\n## Right Join: Keep All Rows from the Second Dataset\n\n* A **right join** keeps all rows from the **second dataset** (`island_info`), and adds matching data from the first dataset (`penguins_complete`).\n* If a row in the second dataset doesn't have a match in the first, then the columns from the first will be filled with NA. \n* We can see this for the `Torgersen` row from `island_info`...\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Right join: keep all rows from island_info\npenguins_right_join <- penguins_complete %>%\n  right_join(island_info, by = \"island\")\n\npenguins_right_join\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 6\n  island     year Adelie Gentoo location   region\n  <chr>     <dbl>  <dbl>  <dbl> <chr>      <chr> \n1 Biscoe     2007     10     34 Antarctica West  \n2 Biscoe     2008     18     46 Antarctica West  \n3 Dream      2007     20      0 Antarctica East  \n4 Dream      2008     NA     NA Antarctica East  \n5 Torgersen    NA     NA     NA Antarctica East  \n```\n:::\n:::\n\n\n## Inner Join: Only Keeping Matching Rows\n* An inner join will only keep rows where there is a match in both datasets.\n* So, if an island in `island_info` does not have a corresponding entry in `penguins_complete`, then that row will be excluded.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Inner join: only matching rows are kept\npenguins_inner_join <- penguins_complete %>%\n  inner_join(island_info, by = \"island\")\n\npenguins_inner_join\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  island  year Adelie Gentoo location   region\n  <chr>  <dbl>  <dbl>  <dbl> <chr>      <chr> \n1 Biscoe  2007     10     34 Antarctica West  \n2 Biscoe  2008     18     46 Antarctica West  \n3 Dream   2007     20      0 Antarctica East  \n4 Dream   2008     NA     NA Antarctica East  \n```\n:::\n:::\n\n\n## Full Join: Keeping All Rows from Both Datasets\n\n* A full join will keep all rows from both datasets.\n* If an island in either dataset has no match in the other, the missing values will be filled with NA.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Full join: keep all rows from both datasets\npenguins_full_join <- penguins_complete %>%\n  full_join(island_info, by = \"island\")\n\npenguins_full_join\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 6\n  island     year Adelie Gentoo location   region\n  <chr>     <dbl>  <dbl>  <dbl> <chr>      <chr> \n1 Biscoe     2007     10     34 Antarctica West  \n2 Biscoe     2008     18     46 Antarctica West  \n3 Dream      2007     20      0 Antarctica East  \n4 Dream      2008     NA     NA Antarctica East  \n5 Torgersen    NA     NA     NA Antarctica East  \n```\n:::\n:::\n\n\n## Summary of the Four Join Functions\n\n* **Left join:** All rows from the left dataset and matching rows from the right dataset.\n* **Right join:** All rows from the right dataset and matching rows from the left dataset.\n* **Inner join:** Only matching rows from both datasets.\n* **Full join:** All rows from both datasets, with NA where no match exists.\n\n## Final thoughts on joins\n* Joins are an essential part of data wrangling in R.\n* The choice of join depends on the analysis you need to perform:\n    + Use **left joins** when you want to keep all data from the first dataset.\n    + Use **right joins** when you want to keep all data from the second dataset.\n    + Use **inner joins** when you're only interested in matching rows.\n    + Use **full joins** when you want to preserve all information from both datasets.\n\n## Goodbye palmer penguins\n<!-- Penguins are great at `group_by()` – they always know how to stick together in a `summarize()`d form! 🐧 -->\n**What's a penguin's favorite tool?** \n\n%>% — to keep the fish moving, from one catch to the next! 🐧 \n\n<div style=\"text-align: center;\">\n![](gfx/penguins_logo.png){style=\"width: 30%;\"}\n\n<small>[Logo from the palmerpenguins website](https://allisonhorst.github.io/palmerpenguins/)</small>\n</div>\n\n# Epiverse software ecosystem\n\n## The epiverse ecosystem\nInterworking, community-driven, packages for epi tracking & forecasting.\n\n![](gfx/epiverse_packages_flow.jpg){style=\"width: 60%;\"}\n\n<!-- 1. Fetch data: epidatr, epidatpy, and other sources, 2. Explore, clean, transform & backtest 3. Pre-built forecasters, modular forecasting framework: epipredict -->\n  \n  \n  \n# Panel and versioned data in the epiverse\n  \n## What is panel data?\n<div style=\"font-size: 0.8em;\">\n* Recall that [panel data](https://en.wikipedia.org/wiki/Panel_data), or longitudinal data, \ncontain cross-sectional measurements of subjects over time. \n* Built-in example: [`covid_case_death_rates`](\n  https://cmu-delphi.github.io/epidatasets/reference/covid_case_death_rates.html) \ndataset, which is a snapshot **as of** May 31, 2022 that contains daily state-wise measures of `case_rate` and `death_rate` for COVID-19 in 2021:\n  \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n* <chr>     <date>         <dbl>      <dbl>\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 as        2020-12-31       0        0    \n5 az        2020-12-31      76.8      1.10 \n6 ca        2020-12-31      96.0      0.751\n```\n:::\n:::\n\n\n* How do we store & work with such snapshots in the epiverse software ecosystem?\n</div>\n  \n  \n## `epi_df`: snapshot of a data set\n\n* You can convert panel data into an `epi_df` with the required `geo_value` and `time_value` columns\n\nTherefore, an `epi_df` is...\n* a tibble that requires columns `geo_value` and `time_value`.\n* arbitrary additional columns containing [measured values]{.primary}\n* additional [keys]{.primary} to index (`age_group`, `ethnicity`, etc.)\n\n::: {.callout-note}\n## `epi_df`\n\nRepresents a [snapshot]{.primary} that\ncontains the most [up-to-date values]{.primary} of the signal variables, [as of]{.primary} a given time.\n:::\n  \n  \n## `epi_df`: snapshot of a data set\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedf <- covid_case_death_rates\nedf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAn `epi_df` object, 20,496 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 20,496 × 4\n   geo_value time_value case_rate death_rate\n * <chr>     <date>         <dbl>      <dbl>\n 1 ak        2020-12-31      35.9      0.158\n 2 al        2020-12-31      65.1      0.438\n 3 ar        2020-12-31      66.0      1.27 \n 4 as        2020-12-31       0        0    \n 5 az        2020-12-31      76.8      1.10 \n 6 ca        2020-12-31      96.0      0.751\n 7 co        2020-12-31      35.8      0.649\n 8 ct        2020-12-31      52.1      0.819\n 9 dc        2020-12-31      31.0      0.601\n10 de        2020-12-31      64.3      0.912\n# ℹ 20,486 more rows\n```\n:::\n:::\n\n\n## Sliding examples on `epi_df`\n\n### Growth rates\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedf <- filter(edf, geo_value %in% c(\"ut\", \"ca\")) %>%\n  group_by(geo_value) %>%\n  mutate(gr_cases = growth_rate(time_value, case_rate, method = \"trend_filter\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## `epi_archive`: collection of `epi_df`s\n\n* full version history of a data set\n* acts like a bunch of `epi_df`s --- but stored [compactly]{.primary}\n* allows similar functionality as `epi_df` but using only [data that would have been available at the time]{.primary}\n\n::: {.callout-note}\n## Revisions\n\nEpidemiology data gets revised frequently.\n\n* We may want to use the data [as it looked in the past]{.primary} \n* or we may want to examine [the history of revisions]{.primary}.\n:::\n\n## `epi_archive`: collection of `epi_df`s\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narchive_cases_dv_subset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-06-01 / 2021-11-30\nℹ First/last version with update: 2020-06-02 / 2021-12-01\nℹ Versions end: 2021-12-01\nℹ A preview of the table (129638 rows x 5 columns):\nKey: <geo_value, time_value, version>\n        geo_value time_value    version percent_cli case_rate_7d_av\n           <char>     <Date>     <Date>       <num>           <num>\n     1:        ca 2020-06-01 2020-06-02          NA        6.628329\n     2:        ca 2020-06-01 2020-06-06    2.140116        6.628329\n     3:        ca 2020-06-01 2020-06-07    2.140116        6.628329\n     4:        ca 2020-06-01 2020-06-08    2.140379        6.628329\n     5:        ca 2020-06-01 2020-06-09    2.114430        6.628329\n    ---                                                            \n129634:        tx 2021-11-26 2021-11-29    1.858596        7.957657\n129635:        tx 2021-11-27 2021-11-28          NA        7.174299\n129636:        tx 2021-11-28 2021-11-29          NA        6.834681\n129637:        tx 2021-11-29 2021-11-30          NA        8.841247\n129638:        tx 2021-11-30 2021-12-01          NA        9.566218\n```\n:::\n:::\n\n  \n## Revision patterns\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Finalized data\n<div style=\"font-size: 0.9em;\">\n  * Counts are revised as time proceeds\n* Want to know the [final]{.primary} value \n* Often not available until weeks/months later\n</div>\n  Forecasting\n: At time $t$, predict the final value for time $t+h$, $h > 0$\n  \n  <br>\n  \n  Backcasting\n: At time $t$, predict the final value for time $t-h$, $h < 0$\n\n  <br>\n  \n  Nowcasting\n: At time $t$, predict the final value for time $t$\n\n# Basic Nowcasting in the epiverse\n\n<!-- predicting a finalized value from a provisional value and making predictions. -->\n## Backfill Canadian edition\n  \n* Every week the BC CDC releases COVID-19 hospitalization data.\n\n* Following week they revise the number upward (by ~25%) due to lagged reports.\n\n![](gfx/bc_hosp_admissions_ex.jpg){style=\"width: 60%;\"}\n<!-- Newest iteration of \"backfill”, Canada edition. Every week the BC CDC releases hospitalization data. The following week they revise the number upward (by about 25%) due to lagging reports. Every single week, the newspaper says “hospitalizations have declined”. This week the BC CDC’s own report said “hospitalizations have declined”. The takeaway in the news is that hospitalizations ALWAYS fall from the previous week, but once backfilled, they’re rarely down -->\n\n* **Takeaway**: Once the data is backfilled, hospitalizations rarely show a decline, challenging the common media narrative.\n\n## Aside on Nowcasting\n\n* To some Epis, \"nowcasting\" can be equated with \"estimate the time-varying instantaneous reproduction number, $R_t$\"\n\n* Example using the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. \n<!-- This data is the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. The values are.up-to-date as of August 2023. -->\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcasting-1.svg){fig-align='center' height=400px}\n:::\n:::\n\n\n* Group built [`{rtestim}`](https://dajmcdon.github.io/rtestim) doing for this nonparametrically.\n\n* We may come back to this later...\n\n## Mathematical setup\n\n* Suppose today is time $t$\n\n* Let $y_i$ denote a series of interest observed at times $i=1,\\ldots, t$.\n\n::: {.callout-important icon=\"false\"}\n## Our goal\n\n* Produce a **point nowcast** for the finalized values of $y_t$.\n* Accompany with time-varying prediction intervals\n\n:::\n\n* We also have access to $p$ other time series \n$x_{ij},\\; i=1,\\ldots,t, \\; j = 1,\\ldots,p$\n\n* All may be subject to revisions.\n\n# Nowcasting with one variable\n\n## Nowcasting Simple Ratio Ex: NCHS Mortality \n\n* In this example, we'll demonstrate the concept of nowcasting using **NHCS mortality data**\n(the number of weekly new deaths with confirmed or presumed COVID-19, per 100,000 population).\n* We will work with **provisional** data (real-time reports) and compare them to **finalized** data (final reports).\n* The goal is to estimate or **nowcast the mortality rate** for weeks when only provisional data is available.\n  \n## Fetch Versioned Data\n\nLet's fetch versioned mortality data from the API (`pub_covidcast`) for CA (`geo_values = \"ca\"`) and the signal of interest (`deaths_covid_incidence_num`) over early 2024.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fetch the versioned NCHS mortality data (weekly)\nmortality_archive <- pub_covidcast(\n  source = \"nchs-mortality\",\n  signals = \"deaths_covid_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"week\",\n  geo_values = \"ca\",  # California (CA)\n  time_values = epirange(202401, 202413),  \n  issues = \"*\"\n) %>% \n  select(geo_value, time_value, version = issue, mortality = value) %>% \n  as_epi_archive(compactify = TRUE)\n\n# Set the start and end days for the analysis \n# corresponding to the weeks entered in time_values\nstart_time = as.Date(\"2023-12-31\")\nend_time = as.Date(\"2024-03-24\")\n```\n:::\n\n\n## Latency in Reporting - Minimum Lag\n\n* A quick inspection reveals that mortality rates are systematically 7 days latent (**fixed lag**).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmortality_revision_inspect = mortality_archive$DT %>% mutate(version_time_diff = version - time_value)\n\n# Look at the first revision for each week\nmortality_revision_inspect %>% group_by(time_value) %>% slice(1) %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n# Groups:   time_value [6]\n  geo_value time_value version    mortality version_time_diff\n  <chr>     <date>     <date>         <dbl> <drtn>           \n1 ca        2023-12-31 2024-01-07        48 7 days           \n2 ca        2024-01-07 2024-01-14        28 7 days           \n3 ca        2024-01-14 2024-01-21        47 7 days           \n4 ca        2024-01-21 2024-01-28        41 7 days           \n5 ca        2024-01-28 2024-02-04        31 7 days           \n6 ca        2024-02-04 2024-02-11        47 7 days           \n```\n:::\n:::\n\n\n* Use `revision_summary()` from `epiprocess` to generate basic statistics about the revision behavior for the dataset.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Latency in Reporting - Finalized Value Attainment\n<div style=\"font-size: 0.8em;\">\n* **Question:** When is the **finalized value** first attained for each date? Would we have access to any in real-time?\n* How fast are the final values attained & what's the pattern for these times, if any?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  geo_value time_value min_version diff    \n  <chr>     <date>     <date>      <drtn>  \n1 ca        2023-12-31 2024-10-13  287 days\n2 ca        2024-01-07 2024-10-27  294 days\n3 ca        2024-01-14 2024-06-09  147 days\n4 ca        2024-01-21 2024-08-11  203 days\n5 ca        2024-01-28 2024-07-07  161 days\n6 ca        2024-02-04 2024-10-13  252 days\n```\n:::\n:::\n\nAnd here's a numerical summary:\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   84.0   147.0   189.0   192.8   259.0   294.0 \n```\n:::\n:::\n\n\n* **Conclusion**: Tends to take a long time & varies. Even for this relatively small time period... Goes as low as 84 days or as high as 294 days. Yikes.\n* So if we were doing this in real-time, then we wouldn't have access to the finalized data.\n</div>\n\n## Comparison of Final vs. Multiple Revisions\nThis figure shows the finalized rates in comparison to **multiple revisions** to see how the data changes over time:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/final-vs-revisions-plot-1.svg){fig-align='center' height=500px}\n:::\n:::\n\n\n## Comparison of Final vs. One Revision\n<div style=\"font-size: 0.8em;\">\nThe below figure compares the finalized rates (in black) to **one revision** (in yellow) for March 3, 2024.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/one-revision-final-plot-1.svg){fig-align='center' height=400px}\n:::\n:::\n\nThe real-time data is biased downwards (systematically below the true value). That is, the signal tends to get scaled up with future revisions.\n</div>\n\n## Calculate One Ratio: Provisional vs. Finalized Data\n<!-- * Let's start simple with computing one ratio. -->\n<div style=\"font-size: 0.8em;\">\n* Suppose that the day is March 10, 2024. Then, because the data is 7 days latent, we can compute the ratio between provisional and finalized data for **March 3, 2024**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas_of_date = as.Date(\"2024-03-10\"); fixed_lag = 7\n\n# Load the finalized mortality data for CA\nca_finalized <- mortality_latest %>%\n  filter(time_value == (as_of_date - fixed_lag)) %>%\n  dplyr::select(mortality)\n\n# Load the provisional mortality data for the same week\nmortality_old = epix_as_of(mortality_archive, max_version = as_of_date)\n\nca_provisional <- mortality_old %>%\n  filter(time_value == (as_of_date - fixed_lag)) %>%\n  dplyr::select(mortality)\n\n# Calculate ratio between provisional and finalized cases for the week of interest\nratio <- ca_provisional$mortality / ca_finalized$mortality\nratio\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3611111\n```\n:::\n:::\n\n\n**Conclusion**: The real-time rate tend to be far below the finalized for this time (26 vs 72 here).\n\n**Question**: Can we generalize this over many days? \n</div>\n\n## Calculating the Ratio over Multiple Dates\n* Let's move from calculating the ratio for one day to multiple day with the goal to use it to nowcast for Mar. 10, which has a **provisional value** of 11\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas_of_date = as.Date(\"2024-03-17\")\n\nprovisional <- epix_as_of(mortality_archive, max_version = as_of_date) %>%\n  filter(time_value == as_of_date - 7) %>%\n  pull(mortality)\nprovisional\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11\n```\n:::\n:::\n\n\nand a **finalized value** of 73\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfinalized <- mortality_latest %>%\n  filter(time_value == as_of_date - 7) %>%\n  pull(mortality)\nfinalized\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 73\n```\n:::\n:::\n\n\n## Calculating the Ratio over Multiple Dates\nFirst, let's download the real-time rates for CA up to Mar. 3, and compare them to their finalized version.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndates <- seq(start_time, (as_of_date - 7), by = \"day\")\nmortality_real_time <- function(date) {\n  epix_as_of(mortality_archive, max_version = (date + 7L)) %>%\n    filter(time_value == date)\n}\nmortality_real_time_df <- map_dfr(dates, mortality_real_time)\nhead(mortality_real_time_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAn `epi_df` object, 6 x 3 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-01-07\n\n# A tibble: 6 × 3\n  geo_value time_value mortality\n* <chr>     <date>         <dbl>\n1 ca        2023-12-31        48\n2 ca        2024-01-07        28\n3 ca        2024-01-14        47\n4 ca        2024-01-21        41\n5 ca        2024-01-28        31\n6 ca        2024-02-04        47\n```\n:::\n:::\n\n\n## Calculating the Ratio over Multiple Dates\nNow, let's plot the real-time vs the finalized mortality rates:\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/real-time-vs-finalized-1.svg){fig-align='center'}\n:::\n:::\n\n* **Takeaways**: The real-time counts are biased **well below** the finalized counts.\n* Systematic underreporting tends to lessen over time (the gap between the lines decreases).\n\n## Realistic Limitation of Nowcasting - Finalized Data\n* Recall that real-time access to finalized data is limited as finalized values can take months to report (e.g., Jan. 7 is finalized 294 days later).\n* To nowcast accurately, we must rely on the **best available approximation of finalized data** at the time of estimation (March 10).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmortality_as_of_mar10 <- epix_as_of(mortality_archive, max_version = (as_of_date - 7))\nhead(mortality_as_of_mar10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAn `epi_df` object, 6 x 3 with metadata:\n* geo_type  = state\n* time_type = week\n* as_of     = 2024-03-10\n\n# A tibble: 6 × 3\n  geo_value time_value mortality\n* <chr>     <date>         <dbl>\n1 ca        2023-12-31       215\n2 ca        2024-01-07       186\n3 ca        2024-01-14       187\n4 ca        2024-01-21       146\n5 ca        2024-01-28       133\n6 ca        2024-02-04       137\n```\n:::\n:::\n\n\n## Ratio calculation & summary\n<div style=\"font-size: 0.8em;\">\nWe then use the \"finalized\" and real-time values to compute the mean ratio:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmortality_real_time_df = mortality_real_time_df %>% filter(time_value != \"2024-03-10\") # exclude date we're nowcasting for\nratio_real_time_to_mar10 <- mortality_real_time_df$mortality / mortality_as_of_mar10$mortality\nsummary(ratio_real_time_to_mar10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1505  0.2257  0.2642  0.3393  0.3275  1.0000 \n```\n:::\n:::\n\nOn average, the real-time rates are ~33.9% of the finalized.\n</div>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/boxplot-ratio-1.svg){fig-align='center'}\n:::\n:::\n\n<div style=\"font-size: 0.8em;\">\nTells us the distribution is right-skewed (mean > median) and so we should opt for the median.\n</div>\n\n## Nowcasting on March 10\nSince the **median ratio** between real-time and finalized values is **0.264** (i.e., real-time values are typically 26.4% of the finalized), then the nowcast is\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Now we can nowcast properly:\nnowcast <- provisional *\n  1 / median(ratio_real_time_to_mar10)\nnowcast\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 41.63155\n```\n:::\n:::\n\nSo, this **nowcast is 41.63**, which is closer to the **finalized value of 73** than the **provisional value of 11**.\n\n## Summary of three main steps \nSo the main steps for this type of fixed lag nowcasting are...\n\n1. Obtain the **provisional value** for the target.\n\n2. Estimate the ratio using the **real-time** and **\"finalized\"** data (for all previous dates that follow a consistent pattern in reporting). \n\n3. Profit.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Expand for the accompanying code\"}\n# Today\nas_of_date = as.Date(\"2024-03-17\")\n\n# 1. Obtain the provisional value\nprovisional <- epix_as_of(mortality_archive, max_version = as_of_date) %>%\n  filter(time_value == as_of_date - 7) %>%\n  pull(mortality)\nprovisional\n\n# 2. Estimate the ratio \nmortality_real_time_df <- map_dfr(dates, mortality_real_time) %>% filter(time_value != \"2024-03-10\") # Real-time data\nmortality_as_of_mar10 <- epix_as_of(mortality_archive, max_version = (as_of_date - 7))  # \"Finalized\" data\n\nratio_real_time_to_mar10 <- mortality_real_time_df$mortality / mortality_as_of_mar10$mortality\n\n# 3. Profit.\nnowcast <- provisional *\n  1 / median(ratio_real_time_to_mar10)\nnowcast\n```\n:::\n\n\n## Nowcasting Mortality for Multiple Dates\n\n* **Define Nowcast Function**:\n  * **Input**: Takes in the dates to nowcast and the fixed lag\n  * **Output**: The nowcasted mortality rates based on the ratio of real-time to finalized data.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nnowcast_function <- function(nowcast_date, fixed_lag) {\n  as_of_date = nowcast_date + fixed_lag\n  \n  # 1. Obtain the provisional value for the target.\n  provisional <- epix_as_of(mortality_archive, max_version = as_of_date) %>%\n    filter(time_value == as_of_date - fixed_lag) %>%\n    pull(mortality)\n  \n  #2. Estimate the ratio multiplier using\n  # real-time\n  dates_seq <- seq(start_time, (nowcast_date - fixed_lag), by = \"week\")\n  mortality_real_time <- map_dfr(dates_seq, mortality_real_time)\n  \n  # and \"finalized\" data\n  finalized <- epix_as_of(mortality_archive, max_version = as_of_date) %>% filter(time_value >= start_time & time_value <= (nowcast_date - fixed_lag)) \n  \n  ratios <- mortality_real_time$mortality / finalized$mortality\n  \n  # Remove infinite or NaN ratios (i.e., keep only finite values)\n  median_ratio <- median(ratios[is.finite(ratios)])\n  \n  #3. Profit.\n  nowcast <- provisional * (1 / median_ratio)\n  \n  # Return a dataframe with the nowcast and date\n  tibble(\n    time_value = nowcast_date,\n    nowcast_mortality = nowcast\n  )\n}\n```\n:::\n\n\n## Map Nowcast Over Multiple Dates\n* We can use `map2()` to apply the function to a series of weeks (e.g., Jan. 28 to Mar. 24).\n* Returns a **dataframe** with nowcasted results.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Apply Nowcast Function Over Multiple Dates\nnowcast_dates <- seq(as.Date(\"2024-01-28\"), as.Date(\"2024-03-24\"), by = \"week\")\nfixed_lag <- 7\nnowcast_results_df <- map2(nowcast_dates, fixed_lag, nowcast_function) %>% list_rbind()\n```\n:::\n\n\n## Map Nowcast Over Multiple Dates\nLet's smooth with a rolling trailing mean (window size 4) & see the results:\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 × 2\n  time_value nowcast_mortality\n  <date>                 <dbl>\n1 2024-01-28             116. \n2 2024-02-04             145. \n3 2024-02-11             114. \n4 2024-02-18             109. \n5 2024-02-25             110. \n6 2024-03-03              92.8\n7 2024-03-10              90.3\n8 2024-03-17              82.6\n9 2024-03-24              65.4\n```\n:::\n:::\n\n\n## Visualize nowcast, real-time, and finalized values\nFinally, we can compare these nowcast results to the real-time and finalized values:\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcast-fun-plot-results-1.svg){fig-align='center'}\n:::\n:::\n\nThe real-time counts tend to be biased below the finalized counts. Nowcasted values tend to provide a much better approximation of the truth (at least for these dates).\n\n# Nowcasting with two variables\n\n## Nowcasting: Moving from One Signal to Two\n\n* Recall that in nowcasting the goal is to predict a finalized value from a provisional value.\n* Now, we'll move from one signal to two, creating a simple linear model to nowcast.\n* Exogenous features (predictors) could include relevant signals, such as Google symptom search trends.\n* We will use these signals to nowcast hospital admissions related to influenza.\n\n\n## Data Sources: Google Search Trends & Hospital Admissions\n\n* **Google Search Trends**: Symptoms like cough, fever, and shortness of breath.\n  * **s01**: Cough, Phlegm, Sputum, Upper respiratory tract infection  \n  * **s02**: Nasal congestion, Post nasal drip, Sinusitis, Common cold\n\n* **Hospital Admissions**: Data from the Department of Health & Human Services on confirmed influenza admissions.\n\n* Using these, we will **nowcast** hospital admissions by using Google symptom search trends for GA from April to June 2023.\n\n* The first step is to fetch this data...\n\n## Data Sources: Google Search Trends & Hospital Admissions\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Fetch Google symptom data for s01 and s02\nx1 <- pub_covidcast(\n  source = \"google-symptoms\",\n  signals = \"s01_smoothed_search\", \n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = epirange(20230401, 20230701),\n  issues = \"*\"\n) %>%\n  select(geo_value, time_value, version = issue, avg_search_vol_s01 = value) %>%\n  as_epi_archive(compactify = FALSE)\n\nx2 <- pub_covidcast(\n  source = \"google-symptoms\",\n  signals = \"s02_smoothed_search\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = epirange(20230401, 20230701),\n  issues = \"*\"\n) %>%\n  select(geo_value, time_value, version = issue, avg_search_vol_s02 = value) %>%\n  as_epi_archive(compactify = FALSE)\n\n# Fetch hospital admissions data\ny1 <- pub_covidcast(\n  source = \"hhs\",\n  signals = \"confirmed_admissions_influenza_1d\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = epirange(20230401, 20230701),\n  issues = \"*\"\n) %>%\n  select(geo_value, time_value, version = issue, admissions = value) %>%\n  as_epi_archive(compactify = FALSE)\n```\n:::\n\n\n## Merging the Archives\n\n* We'll merge the symptom search trends (`x1`, `x2`) with hospital admissions data (`y`) using `epix_merge()` from `epiprocess`.\n* This allows us to match data by time and geography, & fill any missing values with the most recent observation (LOCF).\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Linear Model: A Simple Approach for Nowcasting\n\n* Aside from ratios, one of the simplest approach to nowcasting is to use a **linear regression model**.\n* We model the relationship between provisional (predictor) data and response data.\n* This model helps us make **predictions** for the finalized data based on the current (provisional) signals.\n\n## Linear Regression\n* **Goal**: Estimate the coefficients $\\beta_0$ and $\\beta_1$ that describe the relationship between the predictor $x_i$ and the outcome $y_i$.\n* **Linear Model**: The relationship is assumed to be:\n\n\n  $$y_i \\approx \\beta_0 + \\beta_1 x_i $$\n\n  \n  where\n  $\\beta_0$ is the intercept,\n  $\\beta_1$ is the slope.\n* **In R**: Use `lm(y ~ x)` to estimate the coefficients, where `y` is the outcome variable and `x` is the predictor.\n\n## Multiple Linear Regression\n* **Goal**: Estimate coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$ that describe the relationship between multiple predictors $x_{i1}, x_{i2}, \\dots, x_{ip}$ and the outcome $y_i$.\n* **Model**: The relationship is assumed to be:\n\n\n  $$y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}$$\n\n  \n  where:\n  $\\beta_0$ is the intercept,\n  $\\beta_1, \\dots, \\beta_p$ are the coefficients.\n* **In R**: Use `lm(y ~ x1 + x2 + ... + xp)` to estimate the coefficients, where `y` is the outcome and `x1, x2, ..., xp` are the predictors.\n\n## Multiple Linear Regression Model\n\n* A linear model is a good choice to describe the relationship between search trends and hospital admissions.\n* The model will include two predictors (s01 and s02).\n* We'll use these two search trend signals to predict hospital admissions (response).\n\n<!-- A linear regression model will be used to predict hospital admissions from search trends (s01 and s02). -->\n\n## Multiple Linear Regression Model\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the function for lm model fit and prediction\nlm_mod_pred <- function(data, gk, rtv, ...) {\n  \n  # Fit the linear model\n  model <- lm(admissions ~ avg_search_vol_s01 + avg_search_vol_s02, data = data)\n  \n  # Make predictions\n  predictions = predict(model,\n                        newdata = data %>%\n                          # Use tidyr::fill() for LOCF if predictor data is incomplete \n                          fill(avg_search_vol_s01, .direction = \"down\") %>% \n                          fill(avg_search_vol_s02, .direction = \"down\") %>%\n                          filter(time_value == max(time_value)),\n                        interval = \"prediction\", level = 0.9\n  )\n\n  # Pull off true time value for comparison to target\n  real_time_val = data %>% filter(time_value == max(time_value)) %>% pull(admissions)\n\n  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val))\n}\n```\n:::\n\nNote that this code is intentionally simple; while it can be refined to handle cases like negatives or other boundary conditions, we aim to avoid unnecessary complexity.\n\n## Nowcasting with `epix_slide()`\n\n* We will use `epix_slide()` to create a sliding window of training data.\n* The model will be trained on a 14-day window before the target date, and predictions will be made for the target date.\n* The beauty of this function is that it is version-aware - the sliding computation at any given reference time **t** is performed on data that would have been available as of **t** automatically. \n\n## Nowcasting with `epix_slide()`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the reference time points for nowcasting\ntargeted_nowcast_dates <- seq(as.Date(\"2023-04-15\"), as.Date(\"2023-06-15\"), by = \"1 week\")\nref_time_values = targeted_nowcast_dates + 2  # Adjust for the systematic 2-day latency in the response\n# Determine this from revision_summary(y1, print_inform = TRUE) \n\n# Perform nowcasting using epix_slide\nnowcast_res <- archive %>%\n  group_by(geo_value) %>%\n  epix_slide(\n    .f = lm_mod_pred,\n    .before = 14,  # 14-day training period\n    .versions = ref_time_values, \n    .new_col_name = \"res\"\n  ) %>%\n  unnest() %>% # Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns. \n  mutate(targeted_nowcast_date = targeted_nowcast_dates, time_value = actual_nowcast_date) %>%\n  ungroup()\n\n# View results\nhead(nowcast_res, n=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 9\n  geo_value version      fit    lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>     <dbl>  <dbl> <dbl> <date>                      <dbl>\n1 ga        2023-04-17  4.64 -0.122  9.39 2023-04-15                      4\n2 ga        2023-04-24  7.36  1.56  13.2  2023-04-22                      4\n# ℹ 2 more variables: targeted_nowcast_date <date>, time_value <date>\n```\n:::\n:::\n\n\n## Compare with the Actual Admissions \n\n* After making predictions, we compare them to the actual hospital admissions.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Left join with latest results \n# Latest snapshot of data (with the latest/finalized admissions)\nx_latest <- epix_as_of(archive, max_version = max(archive$DT$version)) %>% select(-c(avg_search_vol_s01, avg_search_vol_s02))\n\nres <- nowcast_res %>% left_join(x_latest, by = c(\"geo_value\", \"time_value\"))\nhead(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 10\n  geo_value version      fit    lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>     <dbl>  <dbl> <dbl> <date>                      <dbl>\n1 ga        2023-04-17  4.64 -0.122  9.39 2023-04-15                      4\n2 ga        2023-04-24  7.36  1.56  13.2  2023-04-22                      4\n3 ga        2023-05-01  6.06  1.57  10.5  2023-04-29                      5\n4 ga        2023-05-08  5.01  1.28   8.74 2023-05-06                      6\n5 ga        2023-05-15  8.14  5.69  10.6  2023-05-11                      8\n6 ga        2023-05-22  3.43 -2.35   9.21 2023-05-20                      4\n# ℹ 3 more variables: targeted_nowcast_date <date>, time_value <date>,\n#   admissions <dbl>\n```\n:::\n:::\n\n\n## Visualizing the Nowcast Results\n<div style=\"font-size: 0.9em;\">\nWe can then visualize the nowcast results alongside the true values using `ggplot2`:\n</div>\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-lr-nowcast-res-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Key Takeaways: Linear Regression Nowcasting Example\n\n* **Provisional Data as Predictors**: Using **Google symptom search trends** to predict **influenza hospital admissions**.\n* **Simple Linear Model**: A linear regression model captures the relationship between symptom searches and hospital admissions.\n* **Actionable Predictions**: Nowcasts provide **timely insights** for hospital admissions, even before data is finalized.\n* **Sliding Window Approach**: Predictions are based on **data up to the current time**, ensuring no future information influences the nowcast.\n* **Evaluation**: Predictions are compared with actual admissions visually.\n\n# Case Study - Nowcasting case rates using %CLI \n\n## Goal of this Case Study\n\n**Goal**: Nowcast COVID-19 Case Rates using %CLI for Massachusetts.\n\n* %CLI is contained in the Epidata API.\n* Case rates by specimen collection date are not. They are from the MA gov website.\n* Case rates in the API (JHU) are aligned by report date, not specimen collection/test date.\n* Working with cases aligned by **test date** allows us to avoid the more unpredictable delays introduced by the **report date**.\n\n## Summary of main steps\n<div style=\"font-size: 0.8em;\">\nThe workflow is similar to the previous example where we nowcasted using two variables, only more involved. \nThe main steps are...\n\n1. **Fetch Data**: Retrieve %CLI and COVID-19 case data (by specimen collection date) for MA.\n\n2. **Merge Data**: Align %CLI and case rate data using `epix_merge`, filling missing values via last observation carried forward (LOCF).\n\n3. **Model & Prediction**: Fit a linear model to predict case rates based on %CLI, trained on a 30-day rolling window.\n\n4. **Nowcast Execution**: Use `epix_slide` to nowcast the case rates dynamically. \n\n5. **Visualization**: Plot actual vs. nowcasted case rates with confidence intervals to assess model accuracy.\n\nSo the first step is to fetch the data...\n</div>\n\n## Construct an `epi_archive` from scratch\n<div style=\"font-size: 0.9em;\">\n[Here's](\"https://www.mass.gov/info-details/archive-of-covid-19-cases-2020-2021\") the archive of COVID-19 case excel files from the MA gov website, which we'll use to construct our own `epi_archive`.\n<br>\nBrief summary of this data:\n\n* **First release**: Raw .xlsx data was first released early January 2021.\n\n* **Change in reporting**: Starting **July 1, 2021**, the dashboard shifted from **7 days/week** to **5 days/week** (Monday-Friday).\n\n* **Friday, Saturday, and Sunday** data is included in the **Monday** dashboard.\n\n* When **Monday** is a holiday, the **Friday through Monday** data is posted on **Tuesday**.\n</div>\n\n## Construct an `epi_archive` from scratch\n<div style=\"font-size: 0.8em;\">\n* **Purpose**: To create an `epi_archive` object for storing versioned time series data.\n* **Required Columns**:\n  * `geo_value`: Geographic data (e.g., region).\n  * `time_value`: Time-related data (e.g., date, time).\n  * `version`: Tracks when the data was available (enables version-aware forecasting).\n* **Constructor**:\n  * `new_epi_archive()`: For manual construction of `epi_archive` (assumes validation of inputs).\n* **Recommended Method**:\n  * `as_epi_archive()`: Simplifies the creation process, ensuring proper formatting and validation. We'll use this one when we download some data from the MA gov website!\n</div>\n\n## Main steps to construct the `epi_archive`\n<div style=\"font-size: 0.8em;\">\n1. **Load necessary Libraries**: Such as `tidyverse`, `readxl`, `epiprocess`.\n2. **Process Each Date's Data**: \n   * A function we'll make (`process_covid_data`) downloads and processes daily COVID-19 data from the MA gov Excel files on their website.\n   * The data is cleaned and formatted with columns: `geo_value`, `time_value`, `version`, and values.\n3. **Handle Missing Data**: Checks if a date's data is available (handle 404 errors).\n4. **Create `epi_archive`**: \n   * Combine processed data into a tibble.\n   * Convert the tibble to an `epi_archive` object using `as_epi_archive()`.\n</div>\n\n## Fetch Data - Code for one date\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)\nlibrary(tibble)\nlibrary(epiprocess)\n\n# Function to download and process each Excel file for a given date\nprocess_covid_data <- function(Date) {\n  # Generate the URL for the given date\n  url <- paste0(\"https://www.mass.gov/doc/covid-19-raw-data-\", tolower(gsub(\"-0\", \"-\", format(Date, \"%B-%d-%Y\"))), \"/download\") \n  # Applies gsub(\"-0\", \"-\", ...) to replace any occurrence of -0 (such as in \"April-01\") with just - (resulting in \"April-1\").\n  \n  # Check if the URL exists (handle the 404 error by skipping that date)\n  response <- GET(url)\n  \n  if (status_code(response) != 200) {\n    return(NULL)  # Skip if URL doesn't exist (404)\n  }\n  \n  # Define the destination file path for the Excel file\n  file_path <- tempfile(fileext = \".xlsx\")\n  \n  # Download the Excel file\n  GET(url, write_disk(file_path, overwrite = TRUE))\n  \n  # Read the relevant sheet from the Excel file\n  data <- read_excel(file_path, sheet = \"CasesByDate (Test Date)\")\n  \n  # Process the data: rename columns and convert Date\n  data <- data %>%\n    rename(\n      Date = `Date`,\n      Positive_Total = `Positive Total`,\n      Positive_New = `Positive New`,\n      Case_Average_7day = `7-day confirmed case average`\n    ) %>%\n    mutate(Date = as.Date(Date))  # Convert to Date class\n  \n  # Create a tibble with the required columns for the epi_archive\n  tib <- tibble(\n    geo_value = \"ma\",  # Massachusetts (geo_value)\n    time_value = data$Date,  # Date from the data\n    version = Date,  # The extracted version date\n    case_rate_7d_av = data$Case_Average_7day  # 7-day average case value\n  )\n  \n  return(tib)\n}\n```\n:::\n\n\n## Fetch Data - Code breakdown \n<div style=\"font-size: 0.8em;\">\n* This purpose of this function is to download and process each Excel file as of a date.\n* **URL Creation**: Dynamically generates the URL based on the date, removing leading zeros in day values (e.g., \"April-01\" → \"April-1\").\n* **Check URL**: Sends a request (`GET(url)`) and skips the date if the URL returns a non-200 status (e.g., 404 error).\n* **Download File**: Saves the Excel file to a temporary path using `tempfile()` and `GET()`.\n* **Read Data**: Loads the relevant sheet (\"CasesByDate\") from the Excel file using `read_excel()`.\n* **Tibble Creation**: Constructs a tibble with `geo_value`, `time_value`, `version`, and `case_rate_7d_av` to later compile into an `epi_archive` (you can think of an `epi_archive` as being a comprised of many `epi_df`s).\n</div>\n\n## Fetch Data - Process range of dates\n* Note that `process_covid_data()` works on one date at a time.\n* So now, we need a function that iterates over a date range and applies `process_covid_data()` to each date & combines the resulting tibbles into an `epi_archive`.\n* We call this function `process_data_for_date_range()`...\n\n## Fetch Data - Process range of dates\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Function to process data for a range of dates\nprocess_data_for_date_range <- function(start_date, end_date) {\n  # Generate a sequence of dates between start_date and end_date\n  date_sequence <- seq(as.Date(start_date), as.Date(end_date), by = \"day\")\n  \n  # Process data for each date and combine results\n  covid_data_list <- lapply(date_sequence, function(Date) {\n    process_covid_data(Date)  # Skip over dates with no data (NULLs will be ignored)\n  })\n  \n  # Combine all non-null individual tibbles into one data frame\n  combined_data <- bind_rows(covid_data_list[!sapply(covid_data_list, is.null)])\n  \n  # Convert the combined data into an epi_archive object\n  if (nrow(combined_data) > 0) {\n    epi_archive_data <- combined_data %>%\n      as_epi_archive(compactify = FALSE)\n    \n    return(epi_archive_data)\n  } else {\n    message(\"No valid data available for the given date range.\")\n    return(NULL)\n  }\n}\n```\n:::\n\n\n## Fetch Data - Code breakdown\nHere's a summary of what `process_data_for_date_range()` does:\n1. **Generates Date Range**: Creates a sequence of dates between `start_date` and `end_date`.\n\n2. **Processes Data**: Applies the `process_covid_data` function to each date in the range (skip over dates with no data).\n\n3. **Combines Results**: Combines all valid (non-NULL) tibbles into one single data frame.\n\n4. **Creates `epi_archive`**: Converts the combined data into an `epi_archive` object.\n\n## Fetch Data - Run the function & inspect archive\n<div style=\"font-size: 0.75em;\">\n* Now, let's run the function & inspect the resulting `epi_archive` of 7-day averaged COVID-19 case counts:\n* Expect building the archive to take a nontrivial amount of time (enough for a cup of coffee or to meditate on life).\n</div>\n<!-- To wonder why you chose Expect building the archive to take a nontrivial amount of time (enough for a cup of coffee or to wonder why you chose coding in the first place). -->\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Example usage: process data between Jan. 10, 2021, and Dec. 1, 2021\ny <- process_data_for_date_range(\"2021-01-10\", \"2021-12-01\")  # Raw .xlsx data is first released on Jan. 4, 2021\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-01-29 / 2021-11-30\nℹ First/last version with update: 2021-01-10 / 2021-12-01\nℹ Versions end: 2021-12-01\nℹ A preview of the table (135549 rows x 4 columns):\nKey: <geo_value, time_value, version>\n        geo_value time_value    version case_rate_7d_av\n           <char>     <Date>     <Date>           <num>\n     1:        ma 2020-01-29 2021-01-10              NA\n     2:        ma 2020-01-29 2021-01-11              NA\n     3:        ma 2020-01-29 2021-01-12              NA\n     4:        ma 2020-01-29 2021-01-13              NA\n     5:        ma 2020-01-29 2021-01-14              NA\n    ---                                                \n135545:        ma 2021-11-28 2021-11-30        2196.000\n135546:        ma 2021-11-28 2021-12-01        2352.286\n135547:        ma 2021-11-29 2021-11-30        1735.714\n135548:        ma 2021-11-29 2021-12-01        2371.286\n135549:        ma 2021-11-30 2021-12-01        1972.143\n```\n:::\n:::\n\n\n## Fetch Data - % Outpatient Doctors Visits for CLI\n<div style=\"font-size: 0.9em;\">\n* Now, from the Epidata API, let's download the [estimated percentage of outpatient doctor visits](\"https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/doctor-visits.html\") primarily for COVID-related symptoms, based on health system data.\n* Comes pre-smoothed in time using a Gaussian linear smoother\n* This will be the predictor when we nowcast COVID-19 Case Rates in MA.\n</div>\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 1: Fetch Versioned Data \nx <- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ma\", # Just for MA to keep it simple (& to go with the case data by test date for that state)\n  time_value = epirange(20210301, 20212101), \n  issues = epirange(20210301, 20212101)\n) %>%\n  select(geo_value, time_value,\n         version = issue,\n         percent_cli = value\n  ) %>%\n  as_epi_archive(compactify = FALSE)\n```\n:::\n\n\n## Use `epix_merge()` to merge the two archives\nNow we'll use `epix_merge()` to combine the two `epi_archive`s that share the same `geo_value` & `time_value`.\n\n<!-- LOCF is used to ensure missing data is handled by filling forward. -->\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narchive <- epix_merge(\n  x, y,\n  sync = \"locf\",\n  compactify = FALSE\n)\narchive\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-01-29 / 2021-12-13\nℹ First/last version with update: 2021-01-10 / 2021-12-17\nℹ Versions end: 2021-12-17\nℹ A preview of the table (139190 rows x 5 columns):\nKey: <geo_value, time_value, version>\n        geo_value time_value    version percent_cli case_rate_7d_av\n           <char>     <Date>     <Date>       <num>           <num>\n     1:        ma 2020-01-29 2021-01-10          NA              NA\n     2:        ma 2020-01-29 2021-01-11          NA              NA\n     3:        ma 2020-01-29 2021-01-12          NA              NA\n     4:        ma 2020-01-29 2021-01-13          NA              NA\n     5:        ma 2020-01-29 2021-01-14          NA              NA\n    ---                                                            \n139186:        ma 2021-12-11 2021-12-16    2.306966              NA\n139187:        ma 2021-12-11 2021-12-17    2.281141              NA\n139188:        ma 2021-12-12 2021-12-16    2.333759              NA\n139189:        ma 2021-12-12 2021-12-17    2.369756              NA\n139190:        ma 2021-12-13 2021-12-17    2.256551              NA\n```\n:::\n:::\n\n\n## Fitting and Predicting with Linear Model\n<div style=\"font-size: 0.8em;\">\n* Define `lm_mod_pred()`: A function that fits a linear model to forecast case rates based on the `percent_cli` predictor.\n* Use `predict()` with a 90% prediction interval.\n* Save the actual case rates to compare to the nowcasts later.\n</div>\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_mod_pred <- function(data, ...) {\n  # Linear model\n  model <- lm(case_rate_7d_av ~ percent_cli, data = data)\n\n  # Make predictions\n  predictions = predict(model,\n                        newdata = data %>%\n                          fill(percent_cli, .direction = \"down\") %>% \n                          filter(time_value == max(time_value)),\n                        interval = \"prediction\", level = 0.9)\n  \n  # Pull off real-time value for later comparison to the nowcast value\n  real_time_val = data %>% filter(time_value == max(time_value)) %>% pull(case_rate_7d_av)\n  \n  # Could clip predictions and bounds at 0\n  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val)) \n}\n```\n:::\n\n\n## Nowcasting with `epix_slide()`\n* **Specify targets**: Define the target dates for nowcasting (e.g., 1st of each month) & adjust training data to include the lag for the latent case data.\n* **Sliding window**: Use `epix_slide()` to apply the linear model across a sliding window of data for each region.\n* **Training-test split**: Use the last 30 days of data to train and predict case rates for each target nowcast date.\n\n## Nowcasting with `epix_slide()`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the reference time points (to give the training/test split)\ntargeted_nowcast_dates <- seq(as.Date(\"2021-04-01\"), as.Date(\"2021-11-01\"), by = \"1 month\") \nref_time_values = targeted_nowcast_dates + 1 # + 1 because the case data is 1 day latent. \n# Determine this from revision_summary(y)\n\n# Use epix_slide to perform the nowcasting with a training-test split\nnowcast_res <- archive %>%\n  group_by(geo_value) %>%\n  epix_slide(\n    .f = lm_mod_pred,  # Pass the function defined above\n    .before = 30,   # Training period of 30 days\n    .versions = ref_time_values, # Determines the day where training data goes up to (not inclusive)\n    .new_col_name = \"res\"\n  ) %>%\n  unnest() %>%\n  mutate(targeted_nowcast_date = targeted_nowcast_dates,\n         time_value = actual_nowcast_date)\n\n# Take a peek at the results\nhead(nowcast_res, n = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 9\n# Groups:   geo_value [1]\n  geo_value version      fit   lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>     <dbl> <dbl> <dbl> <date>                      <dbl>\n1 ma        2021-04-02 2114. 1975. 2254. 2021-04-01                  1556.\n# ℹ 2 more variables: targeted_nowcast_date <date>, time_value <date>\n```\n:::\n:::\n\n\n## Visualizing Nowcasts vs. Actual Values\nMerge the nowcast results with the latest data for more direct comparison:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx_latest <- epix_as_of(archive, max_version = max(archive$DT$version)) %>%\n  select(-percent_cli) \n\nres <- nowcast_res %>% left_join(x_latest, by = c(\"geo_value\", \"time_value\"))\n\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 10\n# Groups:   geo_value [1]\n  geo_value version       fit    lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>      <dbl>  <dbl> <dbl> <date>                      <dbl>\n1 ma        2021-04-02 2114.  1975.  2254. 2021-04-01                  1556.\n2 ma        2021-05-02 1083.   851.  1316. 2021-05-01                   869.\n3 ma        2021-06-02  353.   164.   541. 2021-06-01                   117.\n4 ma        2021-07-02   57.1   11.3  103. 2021-07-01                    59 \n5 ma        2021-08-02  513.   284.   742. 2021-08-01                   572.\n6 ma        2021-09-02 1207.   888.  1527. 2021-09-01                  1099.\n7 ma        2021-10-02 1575.  1357.  1793. 2021-09-30                  1069 \n8 ma        2021-11-02 1299.  1257.  1340. 2021-11-01                   891.\n# ℹ 3 more variables: targeted_nowcast_date <date>, time_value <date>,\n#   case_rate_7d_av <dbl>\n```\n:::\n:::\n\n\n## Visualizing Nowcasts vs. Actual Values\n<div style=\"font-size: 0.9em;\">\nFinally, plot the predictions & real-time values on top of latest COVID-19 case rates using `ggplot2`:\n</div>\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/unnamed-chunk-22-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Takeaways\n\n**Goal**: Predict COVID-19 case rates using %CLI, overcoming delays in report data.\n\nMain Steps:\n\n1. **Fetch Data**: Collect case rates and %CLI data.\n\n2. **Merge Data**: Align datasets with epix_merge() and fill missing values.\n\n3. **Model**: Fit a linear model to predict case rates.\n\n4. **Nowcast**: Apply dynamic forecasting with epix_slide().\n\n5. **Visualize**: Plot nowcasts vs. actual case rates with confidence intervals.\n\nOverall, nowcasting, based on the linear model, provided a closer approximation of true case rates compared to the real-time values.\n\n## Final slide {.smaller}\n\n### Thanks:\n\n\n\n\n\n- The whole [CMU Delphi Team](https://delphi.cmu.edu/about/team/) (across many institutions)\n- Optum/UnitedHealthcare, Change Healthcare.\n- Google, Facebook, Amazon Web Services.\n- Quidel, SafeGraph, Qualtrics.\n- Centers for Disease Control and Prevention.\n- Council of State and Territorial Epidemiologists\n\n\n::: {layout-row=1 fig-align=\"center\"}\n![](gfx/delphi.jpg){height=\"100px\"}\n![](gfx/berkeley.jpg){height=\"100px\"}\n![](gfx/cmu.jpg){height=\"100px\"}\n![](gfx/ubc.jpg){width=\"250px\"}\n![](gfx/stanford.jpg){width=\"250px\"}\n:::\n\n\n",
    "supporting": [
      "day1-afternoon_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}