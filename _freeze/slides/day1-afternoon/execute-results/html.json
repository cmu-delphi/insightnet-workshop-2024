{
  "hash": "b6809cd50abb2ca741a9edd6eded7d27",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntalk-title: \"Data Cleaning, Versioning, and Nowcasting\"\ntalk-short-title: \"Nowcasting\"\ntalk-subtitle: \"InsightNet Forecasting Workshop 2024\"\ntalk-date: \"11 December -- Afternoon\"\nformat: revealjs\n---\n\n---\n---\n\n\n\\DeclareMathOperator*{\\minimize}{minimize}\n\n\n\n\n\n\n\n\n\n::: flex\n::: w-20\n\n:::\n::: w-80\n## {{< meta talk-title >}} {background-image=\"gfx/cover-art-1.svg\" background-position=\"bottom\"}\n\n### {{< meta talk-subtitle >}}\n\n<br>\n\n#### {{< meta author >}} \n[with thanks to Delphi Tooling & Forecasting Team: Logan Brooks, Nat DeFries, Dmitry Shemetov, David Webber]{.fstyle}\n\n\n{{< meta talk-date >}}\n\n\n:::\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Outline\n\n\n1. Epiverse Software ecosystem\n\n1. Panel and Versioned Data in the epiverse\n\n1. Basic Nowcasting using `{epiprocess}`\n\n1. Nowcasting with exogenous features\n\n\n\n  \n\n## Epi. data processing with `epiprocess`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n* `epiprocess` is a package that offers additional functionality to pre-process such epidemiological data.\n* You can work with an `epi_df` like you can with a tibble by using dplyr verbs.\n* For example, on `cases_df`, we can easily use `epi_slide_mean()` to calculate trailing 14 day averages of cases:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncase_rates_df <- case_rates_df |>\n  as_epi_df(as_of = as.Date(\"2024-01-01\")) |>\n  group_by(geo_value) |>\n  epi_slide_mean(scaled_cases, .window_size = 14, na.rm = TRUE) |>\n  rename(smoothed_scaled_cases = slide_value_scaled_cases)\nhead(case_rates_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 6 x 6 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-01-01\n\n# A tibble: 6 × 6\n# Groups:   geo_value [1]\n  geo_value time_value raw_cases      pop scaled_cases smoothed_scaled_cases\n* <chr>     <date>         <dbl>    <dbl>        <dbl>                 <dbl>\n1 ca        2022-03-01      4310 39512223        10.9                   10.9\n2 ca        2022-03-02      7044 39512223        17.8                   14.4\n3 ca        2022-03-03      7509 39512223        19.0                   15.9\n4 ca        2022-03-04      3586 39512223         9.08                  14.2\n5 ca        2022-03-05      1438 39512223         3.64                  12.1\n6 ca        2022-03-06      6465 39512223        16.4                   12.8\n```\n\n\n:::\n:::\n\n\n\n## Epi. data processing with `epiprocess`\nIt is easy to produce an autoplot the smoothed confirmed daily cases for each `geo_value`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncase_rates_df |>\n  autoplot(smoothed_scaled_cases)\n```\n\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/autoplot-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Epi. data processing with `epiprocess`\n\nAlternatively, we can display both the smoothed and the original daily case rates:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/smoothed-original-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\nNow, before exploring some more features of `epiprocess`, let's have a look at the epiverse software ecosystem it's part of...\n\n# Epiverse Software Ecosystem\n\n## The epiverse ecosystem\nInterworking, community-driven, packages for epi tracking & forecasting.\n\n![](gfx/epiverse_packages_flow.jpg){style=\"width: 60%; display: block; margin-left: auto; margin-right: auto;\"}\n\n<!-- 1. Fetch data: epidatr, epidatpy, and other sources, 2. Explore, clean, transform & backtest 3. Pre-built forecasters, modular forecasting framework: epipredict -->\n  \n  \n  \n# Panel and Versioned Data in the Epiverse\n  \n## What is panel data?\n\n* Recall that [panel data](https://en.wikipedia.org/wiki/Panel_data), or longitudinal data, \ncontain cross-sectional measurements of subjects over time. \n* Built-in example: [`covid_case_death_rates`](\n  https://cmu-delphi.github.io/epidatasets/reference/covid_case_death_rates.html) \ndataset, which is a snapshot [**as of**]{.primary} May 31, 2022 that contains daily state-wise measures of `case_rate` and `death_rate` for COVID-19 over 2021:\n  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n  <chr>     <date>         <dbl>      <dbl>\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 az        2020-12-31      76.8      1.10 \n5 ca        2020-12-31      96.0      0.751\n6 co        2020-12-31      35.8      0.649\n```\n\n\n:::\n:::\n\n\n\n* How do we store & work with such snapshots in the epiverse software ecosystem?\n\n  \n  \n## `epi_df`: Snapshot of a dataset\n\n* You can convert panel data into an `epi_df` with the required `geo_value` and `time_value` columns\n\nTherefore, an `epi_df` is...\n\n* a tibble that requires columns `geo_value` and `time_value`.\n\n* arbitrary additional columns containing [measured values]{.primary}\n\n* additional [keys]{.primary} to index (`age_group`, `ethnicity`, etc.)\n\n::: {.callout-note}\n## `epi_df`\n\nRepresents a [snapshot]{.primary} that\ncontains the most [up-to-date values]{.primary} of the signal variables, [as of]{.primary} a given time.\n:::\n\n## `epi_df`: Snapshot of a dataset\n\n* Consider the same dataset we just encountered on JHU daily COVID-19 cases and deaths rates from all states [as of]{.primary} May 31, 2022.\n\n* We can see that it meets the criteria `epi_df` (has `geo_value` and `time_value` columns) and that it contains additional metadata (i.e. `geo_type`, `time_type`, `as_of`, and `other_keys`).\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedf |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n* <chr>     <date>         <dbl>      <dbl>\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 az        2020-12-31      76.8      1.10 \n5 ca        2020-12-31      96.0      0.751\n6 co        2020-12-31      35.8      0.649\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nattr(edf, \"metadata\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$geo_type\n[1] \"state\"\n\n$time_type\n[1] \"day\"\n\n$as_of\n[1] \"2022-05-31\"\n\n$other_keys\ncharacter(0)\n```\n\n\n:::\n:::\n\n\n\n::: \n\n:::: \n\n## Examples of preprocessing\n\n### EDA features\n\n1. Making locations commensurate (per capita scaling)\n1. Correlating signals across location or time \n1. Computing growth rates\n1. Detecting and removing outliers\n1. Dealing with revisions \n\n## Features - Correlations at different lags\n\n<!-- * There are always at least two ways to compute correlations in an `epi_df`: grouping by `time_value`, and by `geo_value`. \n\n* The latter is obtained by setting `cor_by = geo_value`. -->\n\n* The below plot addresses the question: \"For each state, are case and death rates linearly associated across all days?\"\n\n* To explore **lagged correlations** and how case rates associate with future death rates, we can use the `dt1` parameter in `epi_cor()` to shift case rates by a specified number of days. \n\n<!--  * For example, setting `dt1 = -14` means that case rates on June 1st will be correlated with death rates on June 15th, assessing how past case rates influence future death rates. -->\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor0 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value)\ncor14 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value, dt1 = -14)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-corr-lags-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* We can see that, in general, lagging the case rates back by 14 days improves the correlations.\n\n\n## Features - Systematic lag analysis\n\nThe analysis helps identify the lag at which case rates from the past have the strongest correlation with future death rates.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-sys-lag-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\nThe strongest correlation occurs at a lag of about 23 days, indicating that case rates are best correlated with death rates 23 days from now.\n\n## Features - Compute growth rates\n\n* Growth rate measures the relative change in a signal over time. <!-- indicating how quickly a quantity (like case rates) is increasing or decreasing. -->\n\n* We can compute time-varying growth rates for the two states, and see how this cases evolves over time.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedfg <- filter(edf, geo_value %in% c(\"ut\", \"ca\")) |>\n  group_by(geo_value) |>\n  mutate(gr_cases = growth_rate(time_value, case_rate, method = \"trend_filter\")) |>\n  ungroup()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-growth-rates-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* As expected, the peak growth rates for both states occurred during the January 2022 Omicron wave, reflecting the sharp rise in cases over that period.\n\n## Features - Outlier detection\n\n<!-- * There are multiple outliers in these data that a modeler may want to detect and correct. -->\n\n* The `detect_outlr()` function offers multiple outlier detection methods on a signal.\n\n* The simplest is `detect_outlr_rm()`, which works by calculating an outlier threshold using the rolling median and the rolling Interquartile Range (IQR) for each time point:\n\n**Threshold = Rolling Median ± (Detection Multiplier × Rolling IQR)**\n\n* Note that the default number of time steps to use in the rolling window by default is 21 and is centrally aligned. \n* The detection multiplier default is 2 and controls how far away a data point must be from the median to be considered an outlier.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedfo <- filter(edf, geo_value %in% c(\"ut\", \"ca\")) |>\n  select(geo_value, time_value, case_rate) |>\n  as_epi_df() |>\n  group_by(geo_value) |>\n  mutate(outlier_info = detect_outlr_rm(\n    x = time_value, y = case_rate\n  )) |>\n  ungroup()\n```\n:::\n\n\n\n## Features - Outlier detection\n\n* Several data points that deviate from the expected case cadence have been flagged as outliers, and may require further investigation.\n\n* However, the peak in Jan. 2022 has also been flagged as an outlier. This highlights the importance of manual inspection before correcting the data, as these may represent valid events (e.g., a genuine surge in cases).\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-outlier-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## `epi_archive`: Collection of `epi_df`s\n\n* full version history of a data set\n* acts like a bunch of `epi_df`s --- but stored [compactly]{.primary}\n* allows similar functionality as `epi_df` but using only [data that would have been available at the time]{.primary}\n\n\n::: {.callout-note}\n## Revisions\n\nEpidemiology data gets revised frequently.\n\n* We may want to use the data [as it looked in the past]{.primary} \n* or we may want to examine [the history of revisions]{.primary}.\n:::\n\n## `epi_archive`: Collection of `epi_df`s\n\nFull version history of national provisional death counts. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnchs_allver = pub_covidcast(\n  \"nchs-mortality\",\n  \"deaths_covid_incidence_num\",\n  \"state\",\n  \"week\",\n  # Recall you can specify these options to only pull\n  # a subset of data\n  geo_values = c(\"ca\", \"ut\"),\n  # This data has weekly resolution, different form of \n  # time specification\n  time_values = epirange(202001, 202440),\n  # Recall this pulls all available issues\n  issues = \"*\"\n) |>\n  select(geo_value, time_value, issue, value)\n\nnchs_archive = as_epi_archive(nchs_allver, compactify = TRUE)\n```\n:::\n\n\n\nHow is this data getting revised? \n\n\n## Revision pattern\n\nSome problem here -- NCHS's revision time is on the order of weeks, jumping by months hides a lot of the revisioning. \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-revision-patterns-1.svg){fig-align='center'}\n:::\n:::\n\n\nA lot of values at UT is censored due to privacy constraints. \n\n## Revision pattern -- alternative visualization\n\nIf the above is not satisfactory, we can plot by values of the series, accessed k weeks after the reference date...\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/rev-alt-vis-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n# Backfill projection in Epiverse\n\n\n## Types of predictions\n\n* Finalized value often not available until weeks/months later\n* Want to 'project' the [**final**]{.primary} value given the provisional [**real-time**]{.primary} value.\n\n  Forecasting\n: At time $t$, predict the final value for time $t+h$, $h > 0$\n  \n  <br>\n  \n  Backcasting\n: At time $t$, predict the final value for time $t-h$, $h < 0$\n\n  <br>\n  \n  Nowcasting\n: At time $t$, predict the final value for time $t$\n\n\n## Nowcasting via simple ratio: NCHS mortality \n\n* [**NHCS mortality data**]{.primary} contains a versioned history of number of weekly new deaths with confirmed or presumed COVID-19.\n\n<br>\n\n\n* We use this data to demonstrate the nowcasting and backcasting.\n\n<br>\n\n\n* The goal is to project [**nowcast the mortality rate**]{.primary} based on provisional/real-time data. \n  \n## Nowcasting via simple ratio: NCHS mortality\n\n* Specifically, we will make nowcasts from 2022-01 to 2024-01. \n* We will take the remainder of data as training set and for exploratory analysis. \n  \n  \n## Fetch versioned data\n\nLet's fetch versioned mortality data from the API (`pub_covidcast`) for CA (`geo_values = \"ca\"`) and the signal of interest (`deaths_covid_incidence_num`) for all the dates where versioned data is recorded.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fetch the training archive \nnchs_train_archive = pub_covidcast(\n  source = \"nchs-mortality\",\n  signal = \"deaths_covid_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"week\",\n  geo_values = \"ca\",\n  time_values = epirange(202108, 202152),\n  issues = \"*\"\n) |>\n  select(geo_value, time_value, version = issue, mortality = value) |>\n  as_epi_archive(compactify=TRUE)\n\n\n# Fetch the entire versioned history (train + test set)\nnchs_fullarchive <- pub_covidcast(\n  source = \"nchs-mortality\",\n  signals = \"deaths_covid_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"week\",\n  geo_values = \"ca\",  # California (CA)\n  time_values = epirange(202108, 202440),  \n  issues = \"*\"\n) |> \n  select(geo_value, time_value, version = issue, mortality = value) |> \n  as_epi_archive(compactify = TRUE)\n```\n:::\n\n\n\n\n\n## Exploratory analysis: latency in reporting \n\n* We inspect the latency (how long it takes to have any reported value) on our training set.\n* Why only looking at training set? Because at time of nowcast, only have information before the nowcast date.\n* Otherwise we will be using information that would not have been available. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnchs_train_archive$DT |> group_by(geo_value, time_value) |>\n  filter(version == min(version)) |>\n  mutate(lag = version - time_value) |>\n  slice_head() |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n# Groups:   geo_value, time_value [6]\n  geo_value time_value version    mortality lag   \n  <chr>     <date>     <date>         <dbl> <drtn>\n1 ca        2021-02-21 2021-02-28        78 7 days\n2 ca        2021-02-28 2021-03-07        54 7 days\n3 ca        2021-03-07 2021-03-14        43 7 days\n4 ca        2021-03-14 2021-03-21        19 7 days\n5 ca        2021-03-21 2021-03-28        21 7 days\n6 ca        2021-03-28 2021-04-04        23 7 days\n```\n\n\n:::\n:::\n\n\n\n\n\n## Exploratory analysis - Finalized value attainment\n\n* [**Question:**]{.primary} When is the [**finalized value**]{.primary} first attained for each date? Would we have access to any in real-time?\n* How fast are the final values attained & what's the pattern for these times, if any?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value min_version diff    \n  <chr>     <date>     <date>      <drtn>  \n1 ca        2021-02-21 2022-04-03  406 days\n2 ca        2021-02-28 2022-01-30  336 days\n3 ca        2021-03-07 2021-12-26  294 days\n4 ca        2021-03-14 2022-03-06  357 days\n5 ca        2021-03-21 2022-07-10  476 days\n6 ca        2021-03-28 2022-07-10  469 days\n```\n\n\n:::\n:::\n\n\nWe can also look at some quantiles: \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   84.0   217.0   280.0   295.7   336.0   714.0 \n```\n\n\n:::\n:::\n\n\n\n<br>\n\nIt generally takes long to attain finalized value. \n\n\n## Comparison of final vs. multiple revisions\n\nThe following plots gives a more direct message on how the values are revised. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/final-vs-revisions-plot-1.svg){fig-align='center' height=500px}\n:::\n:::\n\n\n\n## Ratio estimator: jumping from provisional to finalized data\n\n* A simple nowcaster is the ratio between finalized value and provisional value. \n\n* Don't know value of any given week is finalized. Need a working approximation. \n\n* From previous plot, we also see data published 49 days later is pretty close to the finalized value. \n\n* We want to use the most recent data available, since that is likely to be most informative.\n\n\n## Ratio estimator: jumping from provisional to finalized data\n\n* As a demonstration, we first do this for a single nowcast date: 2022-01-02 (the first date in 2022).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnowcast_date = as.Date(\"2022-01-02\"); window_length = 90\n\n# First step: estimate the ratio based on some past data\nfinalized_data = epix_as_of(nchs_fullarchive, nowcast_date) |>\n  filter(time_value >= nowcast_date - 49 - window_length & time_value <= nowcast_date - 49) |>\n  rename(finalized_val = mortality) |>\n  select(geo_value, time_value, finalized_val)\n\ninitial_data = nchs_fullarchive$DT |>\n  filter(time_value %in% finalized_data$time_value) |>\n  group_by(geo_value, time_value) |>\n  filter(version == min(version)) |>\n  rename(initial_val = mortality) |>\n  select(geo_value, time_value, initial_val)\n\nratio = finalized_data |>\n  inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |>\n  mutate(ratio = finalized_val / initial_val) |>\n  pull(ratio) |>\n  mean()\n\n# Recall latency! Can only project based on latest reported data\n# Given the ratio, we can multiply that with the last available reported value\nlast_avail = epix_as_of(nchs_fullarchive, nowcast_date) |>\n  slice_max(time_value) |>\n  pull(mortality) \n\nnowcast = last_avail * ratio\nnowcast\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 296.7743\n```\n\n\n:::\n:::\n\n\n\n## Ratio estimator: sliding the computation\n\nOne nowcast is not enough! We need to slide that computation across all the nowcasts. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnowcaster = function(x, g, t, wl=90, appx=49) {\n  \n  finalized_data = x$DT |>\n    group_by(geo_value, time_value) |>\n    filter(version ==  max(version)) |>\n    filter(time_value >= t - wl - appx & time_value <= t - appx) |>\n    rename(finalized_val = mortality) |>\n    select(geo_value, time_value, finalized_val)\n  \n  \n  initial_data = x$DT |>\n    group_by(geo_value, time_value) |>\n    filter(version ==  min(version)) |>\n    filter(time_value >= t - wl - appx & time_value <= t - appx) |>\n    rename(initial_val = mortality) |>\n    select(geo_value, time_value, initial_val)\n  \n  ratio = finalized_data |>\n    inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |>\n    mutate(ratio = finalized_val / initial_val) |>\n    pull(ratio) |>\n    mean(na.rm=TRUE)\n\n  last_avail = epix_as_of(x, t) |>\n    slice_max(time_value) |>\n    pull(mortality) \n  \n  res = tibble(target_date = t, nowcast = last_avail * ratio)\n  \n  return(res)\n  \n}\n```\n:::\n\n\n\n\n## Ratio estimator: sliding the computation\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nall_nowcast_dates = nchs_fullarchive$DT |>\n  filter(time_value > as.Date(\"2021-12-31\")) |>\n  distinct(time_value) |>\n  pull(time_value)\n\nnowcasts = nchs_fullarchive |>\n  epix_slide(\n    nowcaster,\n    .before=Inf,\n    .versions = all_nowcast_dates,\n    .all_versions = TRUE\n) |>\n  mutate(geo_value = \"ca\")\n```\n:::\n\n\n\n## Potential filler for recapping \n\nBLAH \n\n\n\n## Visualize nowcast, real-time, and finalized values\n\n\nWe are now finally able to compare nowcasts against first available reports:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcast-fun-plot-results-1.svg){fig-align='center'}\n:::\n:::\n\n\nThe real-time counts tend to be biased below the finalized counts. Nowcasted values tend to provide a much better approximation of the truth (at least for these dates).\n\n## Evaluation using MAE\n\n* Assume we have prediction $\\hat y_{t}$ for the provisional value at time $t$.\n\n* Then for $y_{t}$ over times $t = 1, \\dots, N$, then we may compute error metrics like mean absolute error (MAE).\n\n<!-- We'll see other error measures later on! For now, let's start with one that is simple and easy to interpret.-->\n\n* MAE measures the average absolute difference between the nowcast and finalized values. \n\n$$MAE = \\frac{1}{N} \\sum_{t=1}^N |y_{t}- \\hat y_{t}|$$\n\n* Note that it's scale-dependent, meaning it can vary depending on the units of the data (e.g., cases, deaths, etc.).\n\n## Evaluation using MAE\n\nLet's numerically evaluate our point nowcasts for the provisional values of a time series (e.g., COVID-19 mortality) using MAE.\n\n<!-- Accuracy of nowcast is assessed by how close provisional estimates are to the finalized values to gauge the model's performance. -->\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 1: Join the mortality data with nowcast data\nmae_data = finalized_val |> \n  left_join(nowcasts |> select(-version), by = join_by(time_value == target_date, geo_value)) |> \n  left_join(first_avail |> rename(real_time = mortality), by = c(\"geo_value\", \"time_value\"))\n\n# Step 2: Calculate the absolute error between actual and nowcasted values\nmae_data <- mae_data |> \n  mutate(nc_abs_error = abs(mortality - nowcast),\n         rt_abs_error = abs(mortality - real_time))  \n\n# Step 3: Compute the MAE (mean of absolute errors)\nmae_value <- mae_data |> \n  group_by(geo_value) |>\n  summarise(nc_MAE = mean(nc_abs_error, na.rm=TRUE),\n            rt_MAE = mean(rt_abs_error, na.rm=TRUE))\nknitr::kable(mae_value)\n```\n\n::: {.cell-output-display}\n\n\n|geo_value |   nc_MAE|   rt_MAE|\n|:---------|--------:|--------:|\n|ca        | 186.9966| 199.5985|\n\n\n:::\n:::\n\n\n\n## Evaluation using MAE\n\nFinally, we may visualize the distribution of absolute residuals for a subset of nowcast dates:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/abs-error-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nWe can see that the absolute errors are almost always lower for nowcasting.\n\n## Nowcasting: prediction intervals\n\nFinally, we briefly discuss how we can form prediction intervals. One idea is to use quantiles of ratios to serve as the limit of interval.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninterval = function(x, g, t, wl=90, appx=49) {\n  \n  finalized_data = x$DT |>\n    group_by(geo_value, time_value) |>\n    filter(version ==  max(version)) |>\n    filter(time_value >= t - wl - appx & time_value <= t - appx) |>\n    rename(finalized_val = mortality) |>\n    select(geo_value, time_value, finalized_val)\n  \n  \n  initial_data = x$DT |>\n    group_by(geo_value, time_value) |>\n    filter(version ==  min(version)) |>\n    filter(time_value >= t - wl - appx & time_value <= t - appx) |>\n    rename(initial_val = mortality) |>\n    select(geo_value, time_value, initial_val)\n  \n  lower_ratio = finalized_data |>\n    inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |>\n    mutate(ratio = finalized_val / initial_val) |>\n    pull(ratio) |>\n    quantile(0.05, na.rm=TRUE)\n\n  upper_ratio = finalized_data |>\n    inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |>\n    mutate(ratio = finalized_val / initial_val) |>\n    pull(ratio) |>\n    quantile(0.95, na.rm=TRUE)\n  \n  last_avail = epix_as_of(x, t) |>\n    slice_max(time_value) |>\n    pull(mortality) \n  \n  res = tibble(target_date = t, lower = lower_ratio * last_avail,\n               upper = upper_ratio * last_avail)\n  \n  return(res)\n  \n}\n```\n:::\n\n\n\n## Nowcasting: Prediction intervals\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_interval = nchs_fullarchive |>\n  epix_slide(\n    interval,\n    .before=Inf,\n    .versions = all_nowcast_dates, \n    .all_versions = TRUE\n) |>\n  mutate(geo_value = \"ca\")\n```\n:::\n\n\n\n\n\n## Nowcasting: visualizing prediction intervals \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcast-plot-intervals-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## Filler for backcasts \n\nBLAH\n\n# Nowcasting with auxilliary variables \n\n## Mathematical setup\n\n* **Nowcasting**: Predict a finalized value from a provisional value.\n\n* Suppose today is time $t$\n\n* Let $y_i$ denote a series of interest observed at times $i=1,\\ldots, t$.\n\n::: {.callout-important icon=\"false\"}\n## Our goal\n\n* Produce a [**point nowcast**]{.primary} for the finalized values of $y_t$.\n* Accompany with time-varying prediction intervals\n\n:::\n\n* We also have access to $p$ other time series \n$x_{ij},\\; i=1,\\ldots,t, \\; j = 1,\\ldots,p$\n\n* All may be subject to revisions.\n\n\n\n\n## Nowcasting: Moving from one signal to two\n\n* Recall that in nowcasting the goal is to predict a finalized value from a provisional value.\n* Now, we'll build a multiple regression model by including expgenous features. \n* Exogenous features (predictors) could include relevant signals, such as Google symptom search trends.\n* We will use google search data and hospital admission incidence as exogenous features. \n\n\n## Data Sources: Google searches & hospital admissions\n\n* [**Google Search Trends**]{.primary}: Symptoms like cough, fever, and shortness of breath.\n  * [**s05**]{.primary}: Anosmia, Dysgeusia, Ageusia.  \n\n* [**Hospital Admissions**]{.primary}: Data from the Department of Health & Human Services on confirmed covid admissions.\n\n* Using these, we will [**nowcast**]{.primary} covid related mortality for CA from Jan 2022 to Oct 2024.\n\n* The first step is to fetch this data...\n\n## Data Sources: Google searches & hospital admissions\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fetch Google symptom data for s01 and s02\nx1 <- pub_covidcast(\n  source = \"google-symptoms\",\n  signals = \"s05_smoothed_search\", \n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ca\",\n  time_values = epirange(20210301, 20241001),\n  issues = \"*\"\n) |>\n  select(geo_value, time_value, version = issue, avg_s06 = value) |>\n  # Not sure why Rachel specified not to compact originally\n  as_epi_archive(compactify = TRUE)\n\n# Somewhat annoying, HHS stopped releasing data after 2024 April... \n# what to do here? \nx2 <- pub_covidcast(\n  source = \"hhs\",\n  signals = \"confirmed_admissions_covid_1d_7dav\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ca\",\n  time_values = epirange(20210301, 20241001),\n  issues = \"*\"\n) |>\n  select(geo_value, time_value, version = issue, admissions = value) |>\n  as_epi_archive(compactify = FALSE)\n\n\n# Fetch hospital admissions data\ny1 <- pub_covidcast(\n  source = \"nchs-mortality\",\n  signals = \"deaths_covid_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"week\",\n  geo_values = \"ca\",  # California (CA)\n  time_values = epirange(202108, 202440),  \n  issues = \"*\"\n) |> \n  select(geo_value, time_value, version = issue, mortality = value) |> \n  as_epi_archive(compactify = TRUE)\n```\n:::\n\n\n\nI can understand Rachel's pickele here. Do we want to use two var, or just drop HHS altogether? \n\n## Merging the archives\n\n* We'll merge the symptom search trends (`x1`, `x2`) with hospital admissions data (`y`) using `epix_merge()` from `epiprocess`.\n* This allows us to match data by time and geography, & fill any missing values with the most recent observation (LOCF).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Linear Model: A simple approach for nowcasting\n\n* Aside from ratios, one of the simplest approach to nowcasting is to use a [**linear regression model**]{.primary}.\n* We model the relationship between provisional (predictor) data and response data.\n* This model helps us make [**predictions**]{.primary} for the finalized data based on the current (provisional) signals.\n\n## Linear regression\n* [**Goal**]{.primary}: Estimate the coefficients $\\beta_0$ and $\\beta_1$ that describe the relationship between the predictor $x_i$ and the outcome $y_i$.\n* [**Linear Model**]{.primary}: The relationship is assumed to be:\n\n  $$y_i \\approx \\beta_0 + \\beta_1 x_i $$\n  \n  where\n  $\\beta_0$ is the intercept,\n  $\\beta_1$ is the slope.\n* **In R**: Use `lm(y ~ x)` to estimate the coefficients, where `y` is the outcome variable and `x` is the predictor.\n\n## Multiple linear regression\n* [**Goal**]{.primary}: Estimate coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$ that describe the relationship between multiple predictors $x_{i1}, x_{i2}, \\dots, x_{ip}$ and the outcome $y_i$.\n* [**Model**]{.primary}: The relationship is assumed to be:\n\n  $$y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}$$\n  \n  where:\n  $\\beta_0$ is the intercept,\n  $\\beta_1, \\dots, \\beta_p$ are the coefficients.\n* [**In R**]{.primary}: Use `lm(y ~ x1 + x2 + ... + xp)` to estimate the coefficients, where `y` is the outcome and `x1, x2, ..., xp` are the predictors.\n\n## Multiple linear regression model\n\n* A linear model is a good choice to describe the relationship between search trends and hospital admissions.\n* The model will include two predictors (s01 and s02).\n* We'll use these two search trend signals to predict hospital admissions (response).\n\n<!-- A linear regression model will be used to predict hospital admissions from search trends (s01 and s02). -->\n\n## Multiple linear regression model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the function for lm model fit and prediction\nlm_mod_pred <- function(data, gk, rtv, ...) {\n  \n  # Fit the linear model\n  model <- lm(admissions ~ avg_search_vol_s01 + avg_search_vol_s02, data = data)\n  \n  # Make predictions\n  predictions = predict(model,\n                        newdata = data |>\n                          # Use tidyr::fill() for LOCF if predictor data is incomplete \n                          fill(avg_search_vol_s01, .direction = \"down\") |> \n                          fill(avg_search_vol_s02, .direction = \"down\") |>\n                          filter(time_value == max(time_value)),\n                        interval = \"prediction\", level = 0.9\n  )\n\n  # Pull off true time value for comparison to target\n  real_time_val = data |> filter(time_value == max(time_value)) |> pull(admissions)\n\n  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val))\n}\n```\n:::\n\n\nNote that this code is intentionally simple; while it can be refined to handle cases like negatives or other boundary conditions, we aim to avoid unnecessary complexity.\n\n## Nowcasting with `epix_slide()`\n\n* We will use `epix_slide()` to create a sliding window of training data.\n* The model will be trained on a 14-day window before the target date, and predictions will be made for the target date.\n* The beauty of this function is that it is version-aware - the sliding computation at any given reference time [**t**]{.primary} is performed on data that would have been available as of [**t**]{.primary} automatically. \n\n## Nowcasting with `epix_slide()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the reference time points for nowcasting\ntargeted_nowcast_dates <- seq(as.Date(\"2023-04-15\"), as.Date(\"2023-06-15\"), by = \"1 week\")\nref_time_values = targeted_nowcast_dates + 2  # Adjust for the systematic 2-day latency in the response\n# Determine this from revision_summary(y1, print_inform = TRUE) \n\n# Perform nowcasting using epix_slide\nnowcast_res <- archive |>\n  group_by(geo_value) |>\n  epix_slide(\n    .f = lm_mod_pred,\n    .before = 14,  # 14-day training period\n    .versions = ref_time_values, \n    .new_col_name = \"res\"\n  ) |>\n  unnest() |> # Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns. \n  mutate(targeted_nowcast_date = targeted_nowcast_dates, time_value = actual_nowcast_date) |>\n  ungroup()\n\n# View results\nhead(nowcast_res, n=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 9\n  geo_value version      fit    lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>     <dbl>  <dbl> <dbl> <date>                      <dbl>\n1 ga        2023-04-17  4.64 -0.122  9.39 2023-04-15                      4\n2 ga        2023-04-24  7.36  1.56  13.2  2023-04-22                      4\n# ℹ 2 more variables: targeted_nowcast_date <date>, time_value <date>\n```\n\n\n:::\n:::\n\n\n\n## Compare with the actual admissions \n\nAfter making predictions, we compare them to the actual hospital admissions.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Left join with latest results \n# Latest snapshot of data (with the latest/finalized admissions)\nx_latest <- epix_as_of(archive, max(archive$DT$version)) |> select(-c(avg_search_vol_s01, avg_search_vol_s02))\n\nres <- nowcast_res |> left_join(x_latest, by = join_by(geo_value, time_value))\nhead(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n  geo_value version      fit    lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>     <dbl>  <dbl> <dbl> <date>                      <dbl>\n1 ga        2023-04-17  4.64 -0.122  9.39 2023-04-15                      4\n2 ga        2023-04-24  7.36  1.56  13.2  2023-04-22                      4\n3 ga        2023-05-01  6.06  1.57  10.5  2023-04-29                      5\n4 ga        2023-05-08  5.01  1.28   8.74 2023-05-06                      6\n5 ga        2023-05-15  8.14  5.69  10.6  2023-05-11                      8\n6 ga        2023-05-22  3.43 -2.35   9.21 2023-05-20                      4\n# ℹ 3 more variables: targeted_nowcast_date <date>, time_value <date>,\n#   admissions <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Visualizing the nowcast results\n\nWe can then visualize the nowcast results alongside the true values using `ggplot2`:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-multiple-lr-nowcast-res-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Evaluation using MAE\n\n* As before, we can evaluate our point nowcasts numerically using MAE.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the absolute error between actual and nowcasted values\nmae_data_admissions <- res |> \n  mutate(nc_abs_error = abs(admissions - fit),  # Nowcast vs Finalized admissions\n         rt_abs_error = abs(admissions - real_time_val))  # Real-Time vs Finalized admissions\n\n# Compute the MAE (mean of absolute errors)\nmae_value_admissions <- mae_data_admissions |> \n  summarise(nc_MAE = mean(nc_abs_error),\n            rt_MAE = mean(rt_abs_error))\nknitr::kable(mae_value_admissions)\n```\n\n::: {.cell-output-display}\n\n\n|   nc_MAE|   rt_MAE|\n|--------:|--------:|\n| 1.180965| 2.333333|\n\n\n:::\n:::\n\n\n\n* Based off of comparing these simple error measures, the nowcast MAE is clearly better.\n\n## Evaluation using MAE\n\nHowever, when we visualize the distribution of errors across time, it is not so cut-and-dry:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/abs-error-plot-hosp-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nThe main driver behind the real-time MAE being greater is the \"outlier-like\" May 25 AE.\n\nSo visualizing can provide an important perspective that is missed from a simple numerical summary of error.\n\n## Key Takeaways: Linear regression nowcasting example\n\n* [**Provisional Data as Predictors**]{.primary}: Using [Google symptom search trends]{.primary} to predict [influenza hospital admissions]{.primary}.\n* [**Simple Linear Model**]{.primary}: A linear regression model captures the relationship between symptom searches and hospital admissions.\n* [**Actionable Predictions**]{.primary}: Nowcasts provide [timely insights]{.primary} for hospital admissions, even before data is finalized.\n* [**Sliding Window Approach**]{.primary}: Predictions are based on [data up to the current time]{.primary}, ensuring no future information influences the nowcast.\n* [**Evaluation**]{.primary}: Predictions are compared with actual admissions using numerical and visual perspectives.\n\n# Case Study - Nowcasting Cases Using %CLI \n\n## Goal of this case study\n\n[**Goal**]{.primary}: Nowcast COVID-19 Cases for MA using the estimated percentage of COVID-related doctor's visits (%CLI), based on outpatient data from Optum.\n\n* %CLI is contained in the Epidata API.\n* Cases by specimen collection date are not. They are from the MA gov website.\n* Cases in the API (JHU) are aligned by report date, not specimen collection/test date.\n* Working with cases aligned by [**test date**]{.primary} allows us to avoid the more unpredictable delays introduced by the [**report date**]{.primary}.\n\n## Summary of main steps\n\nThe workflow is similar to the previous example where we nowcasted using two variables, only more involved. \nThe main steps are...\n\n1. [**Fetch Data**]{.primary}: Retrieve %CLI and COVID-19 case data (by specimen collection date) for MA.\n\n2. [**Merge Data**]{.primary}: Align %CLI and case data using `epix_merge`, filling missing values via last observation carried forward (LOCF).\n\n3. [**Model & Prediction**]{.primary}: Fit a linear model to predict cases based on %CLI, trained on a 30-day rolling window.\n\n4. [**Nowcast Execution**]{.primary}: Use `epix_slide` to nowcast the cases dynamically. \n\n5. [**Visualization**]{.primary}: Plot actual vs. nowcasted cases with confidence intervals to assess model accuracy.\n\nSo the first step is to fetch the data...\n\n## Construct an `epi_archive` from scratch\n\n[Here's](\"https://www.mass.gov/info-details/archive-of-covid-19-cases-2020-2021\") the archive of COVID-19 case excel files from the MA gov website, which we'll use to construct our own `epi_archive`.\n<br>\n<br>\nBrief summary of this data:\n\n* [**First release**]{.primary}: Raw .xlsx data was first released early January 2021.\n\n* [**Change in reporting**]{.primary}: Starting [**July 1, 2021**]{.primary}, the dashboard shifted from [**7 days/week**]{.primary} to [**5 days/week**]{.primary} (Monday-Friday).\n\n* [**Friday, Saturday, and Sunday**]{.primary} data is included in the [**Monday**]{.primary} dashboard.\n\n* When [**Monday**]{.primary} is a holiday, the [**Friday through Monday**]{.primary} data is posted on [**Tuesday**]{.primary}.\n\n\n## Construct an `epi_archive` from scratch\n\n* [**Purpose**]{.primary}: To create an `epi_archive` object for storing versioned time series data.\n* [**Required Columns**]{.primary}:\n  * `geo_value`: Geographic data (e.g., region).\n  * `time_value`: Time-related data (e.g., date, time).\n  * `version`: Tracks when the data was available (enables version-aware forecasting).\n* [**Constructor**]{.primary}:\n  * `new_epi_archive()`: For manual construction of `epi_archive` (assumes validation of inputs).\n* [**Recommended Method**]{.primary}:\n  * `as_epi_archive()`: Simplifies the creation process, ensuring proper formatting and validation. We'll use this one when we download some data from the MA gov website!\n\n\n## Main steps to construct the `epi_archive`\n\n1. [**Load necessary Libraries**]{.primary}: Such as `tidyverse`, `readxl`, `epiprocess`.\n2. [**Process Each Date's Data**]{.primary}: \n   * A function we'll make (`process_covid_data`) downloads and processes daily COVID-19 data from the MA gov Excel files on their website.\n   * The data is cleaned and formatted with columns: `geo_value`, `time_value`, `version`, and values.\n3. [**Handle Missing Data**]{.primary}: Checks if a date's data is available (handle 404 errors).\n4. [**Create `epi_archive`**]{.primary}: \n   * Combine processed data into a tibble.\n   * Convert the tibble to an `epi_archive` object using `as_epi_archive()`.\n\n\n## Fetch Data - Code for one date\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)\nlibrary(tibble)\nlibrary(epiprocess)\n\n# Function to download and process each Excel file for a given date\nprocess_covid_data <- function(Date) {\n  # Generate the URL for the given date\n  url <- paste0(\"https://www.mass.gov/doc/covid-19-raw-data-\", tolower(gsub(\"-0\", \"-\", format(Date, \"%B-%d-%Y\"))), \"/download\") \n  # Applies gsub(\"-0\", \"-\", ...) to replace any occurrence of -0 (such as in \"April-01\") with just - (resulting in \"April-1\").\n  \n  # Check if the URL exists (handle the 404 error by skipping that date)\n  response <- GET(url)\n  \n  if (status_code(response) != 200) {\n    return(NULL)  # Skip if URL doesn't exist (404)\n  }\n  \n  # Define the destination file path for the Excel file\n  file_path <- tempfile(fileext = \".xlsx\")\n  \n  # Download the Excel file\n  GET(url, write_disk(file_path, overwrite = TRUE))\n  \n  # Read the relevant sheet from the Excel file\n  data <- read_excel(file_path, sheet = \"CasesByDate (Test Date)\")\n  \n  # Process the data: rename columns and convert Date\n  data <- data |>\n    rename(\n      Date = `Date`,\n      Positive_Total = `Positive Total`,\n      Positive_New = `Positive New`,\n      Case_Average_7day = `7-day confirmed case average`\n    ) |>\n    mutate(Date = as.Date(Date))  # Convert to Date class\n  \n  # Create a tibble with the required columns for the epi_archive\n  tib <- tibble(\n    geo_value = \"ma\",  # Massachusetts (geo_value)\n    time_value = data$Date,  # Date from the data\n    version = Date,  # The extracted version date\n    case_rate_7d_av = data$Case_Average_7day  # 7-day average case value\n  )\n  \n  return(tib)\n}\n```\n:::\n\n\n\n## Fetch Data - Code breakdown \n\n* This purpose of this function is to download and process each Excel file as of a date.\n* [**URL Creation**]{.primary}: Dynamically generates the URL based on the date, removing leading zeros in day values (e.g., \"April-01\" → \"April-1\").\n* [**Check URL**]{.primary}: Sends a request (`GET(url)`) and skips the date if the URL returns a non-200 status (e.g., 404 error).\n* [**Download File**]{.primary}: Saves the Excel file to a temporary path using `tempfile()` and `GET()`.\n* [**Read Data**]{.primary}: Loads the relevant sheet (\"CasesByDate\") from the Excel file using `read_excel()`.\n* [**Tibble Creation**]{.primary}: Constructs a tibble with `geo_value`, `time_value`, `version`, and `case_rate_7d_av` to later compile into an `epi_archive` (you can think of an `epi_archive` as being a comprised of many `epi_df`s).\n\n\n## Fetch Data - Process eange of dates\n* Note that `process_covid_data()` works on one date at a time.\n* So now, we need a function that iterates over a date range and applies `process_covid_data()` to each date & combines the resulting tibbles into an `epi_archive`.\n* We call this function `process_data_for_date_range()`...\n\n## Fetch Data - Process range of dates\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Function to process data for a range of dates\nprocess_data_for_date_range <- function(start_date, end_date) {\n  # Generate a sequence of dates between start_date and end_date\n  date_sequence <- seq(as.Date(start_date), as.Date(end_date), by = \"day\")\n  \n  # Process data for each date and combine results\n  covid_data_list <- lapply(date_sequence, function(Date) {\n    process_covid_data(Date)  # Skip over dates with no data (NULLs will be ignored)\n  })\n  \n  # Combine all non-null individual tibbles into one data frame\n  combined_data <- bind_rows(covid_data_list[!sapply(covid_data_list, is.null)])\n  \n  # Convert the combined data into an epi_archive object\n  if (nrow(combined_data) > 0) {\n    epi_archive_data <- combined_data |>\n      as_epi_archive(compactify = FALSE)\n    \n    return(epi_archive_data)\n  } else {\n    message(\"No valid data available for the given date range.\")\n    return(NULL)\n  }\n}\n```\n:::\n\n\n\n## Fetch Data - Code breakdown\nHere's a summary of what `process_data_for_date_range()` does:\n1. [**Generates Date Range**]{.primary}: Creates a sequence of dates between `start_date` and `end_date`.\n\n2. [**Processes Data**]{.primary}: Applies the `process_covid_data` function to each date in the range (skip over dates with no data).\n\n3. [**Combines Results**]{.primary}: Combines all valid (non-NULL) tibbles into one single data frame.\n\n4. [**Creates `epi_archive`**]{.primary}: Converts the combined data into an `epi_archive` object.\n\n## Fetch Data - Run the function & inspect archive\n\n* Now, let's run the function & inspect the resulting `epi_archive` of 7-day avg. COVID-19 case counts:\n* Expect building the archive to some time (enough for a cup of coffee or to meditate on life).\n\n<!-- To wonder why you chose Expect building the archive to take a nontrivial amount of time (enough for a cup of coffee or to wonder why you chose coding in the first place). -->\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Example usage: process data between Jan. 10, 2021, and Dec. 1, 2021\n# y <- process_data_for_date_range(\"2021-01-10\", \"2021-12-01\")  # Raw .xlsx data is first released on Jan. 4, 2021\n# y\n```\n:::\n\n\n\n* Alternatively, you may run the following to load `y` that was previously saved as an RDS file: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- readRDS(\"_data/ma_case_archive.rds\")\n```\n:::\n\n\n\n\n## Fetch Data - % Outpatient doctors visits for CLI\n\n* Now, from the Epidata API, let's download the [estimated percentage of outpatient doctor visits](\"https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/doctor-visits.html\") primarily for COVID-related symptoms, based on health system data.\n* Comes pre-smoothed in time using a Gaussian linear smoother\n* This will be the predictor when we nowcast COVID-19 cases in MA.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 1: Fetch Versioned Data \nx <- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ma\", # Just for MA to keep it simple (& to go with the case data by test date for that state)\n  time_values = epirange(20210301, 20211231),\n  issues = epirange(20210301, 20211231)\n) |>\n  select(geo_value, time_value,\n         version = issue,\n         percent_cli = value\n  ) |>\n  as_epi_archive(compactify = FALSE)\n```\n:::\n\n\n\n## Use `epix_merge()` to merge the two archives\nNow we'll use `epix_merge()` to combine the two `epi_archive`s that share the same `geo_value` & `time_value`.\n\n<!-- LOCF is used to ensure missing data is handled by filling forward. -->\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narchive <- epix_merge(\n  x, y,\n  sync = \"locf\",\n  compactify = FALSE\n)\narchive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-01-29 / 2021-12-13\nℹ First/last version with update: 2021-01-10 / 2021-12-17\nℹ Versions end: 2021-12-17\nℹ A preview of the table (139190 rows x 5 columns):\nKey: <geo_value, time_value, version>\n        geo_value time_value    version percent_cli case_rate_7d_av\n           <char>     <Date>     <Date>       <num>           <num>\n     1:        ma 2020-01-29 2021-01-10          NA              NA\n     2:        ma 2020-01-29 2021-01-11          NA              NA\n     3:        ma 2020-01-29 2021-01-12          NA              NA\n     4:        ma 2020-01-29 2021-01-13          NA              NA\n     5:        ma 2020-01-29 2021-01-14          NA              NA\n    ---                                                            \n139186:        ma 2021-12-11 2021-12-16    2.306966              NA\n139187:        ma 2021-12-11 2021-12-17    2.281141              NA\n139188:        ma 2021-12-12 2021-12-16    2.333759              NA\n139189:        ma 2021-12-12 2021-12-17    2.369756              NA\n139190:        ma 2021-12-13 2021-12-17    2.256551              NA\n```\n\n\n:::\n:::\n\n\n\n## Fitting and predicting with linear model\n\n* Define `lm_mod_pred()`: A function that fits a linear model to forecast cases based on the `percent_cli` predictor.\n* Use `predict()` with a 90% prediction interval.\n* Save the actual cases to compare to the nowcasts later.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_mod_pred <- function(data, ...) {\n  # Linear model\n  model <- lm(case_rate_7d_av ~ percent_cli, data = data)\n\n  # Make predictions\n  predictions = predict(model,\n                        newdata = data |>\n                          fill(percent_cli, .direction = \"down\") |> \n                          filter(time_value == max(time_value)),\n                        interval = \"prediction\", level = 0.9)\n  \n  # Pull off real-time value for later comparison to the nowcast value\n  real_time_val = data |> filter(time_value == max(time_value)) |> pull(case_rate_7d_av)\n  \n  # Could clip predictions and bounds at 0\n  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val)) \n}\n```\n:::\n\n\n\n## Nowcasting with `epix_slide()`\n* [**Specify targets**]{.primary}: Define the target dates for nowcasting (e.g., 1st of each month) & adjust training data to include the lag for the latent case data.\n* [**Sliding window**]{.primary}: Use `epix_slide()` to apply the linear model across a sliding window of data for each region.\n* [**Training-test split**]{.primary}: Use the last 30 days of data to train and predict cases for each target nowcast date.\n\n## Nowcasting with `epix_slide()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the reference time points (to give the training/test split)\ntargeted_nowcast_dates <- seq(as.Date(\"2021-04-01\"), as.Date(\"2021-11-01\"), by = \"1 month\") \nref_time_values = targeted_nowcast_dates + 1 # + 1 because the case data is 1 day latent. \n# Determine this from revision_summary(y)\n\n# Use epix_slide to perform the nowcasting with a training-test split\nnowcast_res <- archive |>\n  group_by(geo_value) |>\n  epix_slide(\n    .f = lm_mod_pred,  # Pass the function defined above\n    .before = 30,   # Training period of 30 days\n    .versions = ref_time_values, # Determines the day where training data goes up to (not inclusive)\n    .new_col_name = \"res\"\n  ) |>\n  unnest() |>\n  mutate(targeted_nowcast_date = targeted_nowcast_dates,\n         time_value = actual_nowcast_date)\n\n# Take a peek at the results\nhead(nowcast_res, n = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n# Groups:   geo_value [1]\n  geo_value version      fit   lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>     <dbl> <dbl> <dbl> <date>                      <dbl>\n1 ma        2021-04-02 2114. 1975. 2254. 2021-04-01                  1556.\n# ℹ 2 more variables: targeted_nowcast_date <date>, time_value <date>\n```\n\n\n:::\n:::\n\n\n\n## Visualizing nowcasts vs. actual values\nMerge the nowcast results with the latest data for more direct comparison:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx_latest <- epix_as_of(archive, max(archive$DT$version)) |>\n  select(-percent_cli) \n\nres <- nowcast_res |> left_join(x_latest, by = join_by(geo_value, time_value))\n\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 10\n# Groups:   geo_value [1]\n  geo_value version       fit    lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>      <dbl>  <dbl> <dbl> <date>                      <dbl>\n1 ma        2021-04-02 2114.  1975.  2254. 2021-04-01                  1556.\n2 ma        2021-05-02 1083.   851.  1316. 2021-05-01                   869.\n3 ma        2021-06-02  353.   164.   541. 2021-06-01                   117.\n4 ma        2021-07-02   57.1   11.3  103. 2021-07-01                    59 \n5 ma        2021-08-02  513.   284.   742. 2021-08-01                   572.\n6 ma        2021-09-02 1207.   888.  1527. 2021-09-01                  1099.\n7 ma        2021-10-02 1575.  1357.  1793. 2021-09-30                  1069 \n8 ma        2021-11-02 1299.  1257.  1340. 2021-11-01                   891.\n# ℹ 3 more variables: targeted_nowcast_date <date>, time_value <date>,\n#   case_rate_7d_av <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Visualizing nowcasts vs. actual values\n\nNow, plot the predictions & real-time values on top of latest COVID-19 cases using `ggplot2`:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-simple-lr-nowcast-res-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Evaluation using MAE\n\n* Finally, we numerically evaluate our nowcasts using MAE.\n\n* Shows that the nowcast errors are lower than those of the real-time estimates.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the absolute error between actual and nowcasted COVID-19 cases\nmae_data_cases <- res |> \n  mutate(nc_abs_error = abs(case_rate_7d_av - fit),  # Nowcast vs Finalized cases (7-day average)\n         rt_abs_error = abs(case_rate_7d_av - real_time_val))  # Real-Time vs Finalized cases\n\n# Compute the MAE (mean of absolute errors)\nmae_value_cases <- mae_data_cases |> \n  summarise(nc_MAE = mean(nc_abs_error),\n            rt_MAE = mean(rt_abs_error))\nknitr::kable(mae_value_cases)\n```\n\n::: {.cell-output-display}\n\n\n|geo_value |   nc_MAE|   rt_MAE|\n|:---------|--------:|--------:|\n|ma        | 152.5013| 228.5357|\n\n\n:::\n:::\n\n\n\n\n## Evaluation using MAE\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/abs-error-plot-cases-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nThe elevated errors at both ends highlight periods where the discrepancies between the real-time and nowcast estimates are most pronounced.\n\n## Takeaways\n\n[**Goal**]{.primary}: Predict COVID-19 cases using %CLI, overcoming delays in report data.\n\nMain Steps:\n\n1. [**Fetch Data**]{.primary}: Collect case and %CLI data.\n\n2. [**Merge Data**]{.primary}: Align datasets with `epix_merge()` and fill missing values.\n\n3. [**Model**]{.primary}: Fit a linear model to predict cases.\n\n4. [**Nowcast**]{.primary}: Apply dynamic forecasting with `epix_slide()`.\n\n5. [**Evaluate**]{.primary}: Calculate error measures and numerically and visually assess the results.\n\nOverall, nowcasting, based on the linear model, provided a closer approximation of true cases compared to the real-time values.\n\n\n## Bonus\n\n\n## Aside on nowcasting\n\n* To some Epis, \"nowcasting\" can be equated with \"estimate the time-varying instantaneous reproduction number, $R_t$\"\n\n* Ex. using the number of reported COVID-19 cases in British Columbia between Jan. 2020 and Apr. 15, 2023. \n\n<!-- This data is the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. The values are.up-to-date as of August 2023. -->\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcasting-1.svg){fig-align='center' height=400px}\n:::\n:::\n\n\n\n* Group built [`{rtestim}`](https://dajmcdon.github.io/rtestim) doing for this nonparametrically.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}