{
  "hash": "f8b19463be2e105af8b137327e91b702",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntalk-title: \"Data Cleaning, Versioning, and Nowcasting\"\ntalk-short-title: \"Nowcasting\"\ntalk-subtitle: \"InsightNet Forecasting Workshop 2024\"\ntalk-date: \"11 December -- Afternoon\"\nformat: revealjs\n---\n\n---\n---\n\n\n\\DeclareMathOperator*{\\minimize}{minimize}\n\n\n\n\n\n\n\n\n\n::: flex\n::: w-20\n\n:::\n::: w-80\n## {{< meta talk-title >}} {background-image=\"gfx/cover-art-1.svg\" background-position=\"bottom\"}\n\n### {{< meta talk-subtitle >}}\n\n<br>\n\n#### {{< meta author >}} \n[with thanks to Delphi Tooling & Forecasting Team: Logan Brooks, Nat DeFries, Dmitry Shemetov, David Webber]{.fstyle}\n\n\n{{< meta talk-date >}}\n\n\n:::\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Outline\n\n\n1. Epiverse Software ecosystem\n\n1. Panel and Versioned Data in the epiverse\n\n1. Basic Nowcasting using `{epiprocess}`\n\n1. Nowcasting with Two Variables\n\n1. Case Study - Nowcasting Cases Using %CLI\n\n\n  \n\n## Epi. data processing with `epiprocess`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n* `epiprocess` is a package that offers additional functionality to pre-process such epidemiological data.\n* You can work with an `epi_df` like you can with a tibble by using dplyr verbs.\n* For example, on `cases_df`, we can easily use `epi_slide_mean()` to calculate trailing 14 day averages of cases:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncase_rates_df <- case_rates_df |>\n  as_epi_df(as_of = as.Date(\"2024-01-01\")) |>\n  group_by(geo_value) |>\n  epi_slide_mean(scaled_cases, .window_size = 14, na.rm = TRUE) |>\n  rename(smoothed_scaled_cases = slide_value_scaled_cases)\nhead(case_rates_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 6 x 6 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-01-01\n\n# A tibble: 6 × 6\n# Groups:   geo_value [1]\n  geo_value time_value raw_cases      pop scaled_cases smoothed_scaled_cases\n* <chr>     <date>         <dbl>    <dbl>        <dbl>                 <dbl>\n1 ca        2022-03-01      4310 39512223        10.9                   10.9\n2 ca        2022-03-02      7044 39512223        17.8                   14.4\n3 ca        2022-03-03      7509 39512223        19.0                   15.9\n4 ca        2022-03-04      3586 39512223         9.08                  14.2\n5 ca        2022-03-05      1438 39512223         3.64                  12.1\n6 ca        2022-03-06      6465 39512223        16.4                   12.8\n```\n\n\n:::\n:::\n\n\n\n## Epi. data processing with `epiprocess`\nIt is easy to produce an autoplot the smoothed confirmed daily cases for each `geo_value`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncase_rates_df |>\n  autoplot(smoothed_scaled_cases)\n```\n\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/autoplot-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Epi. data processing with `epiprocess`\n\nAlternatively, we can display both the smoothed and the original daily case rates:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/smoothed-original-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\nNow, before exploring some more features of `epiprocess`, let's have a look at the epiverse software ecosystem it's part of...\n\n# Epiverse Software Ecosystem\n\n## The epiverse ecosystem\nInterworking, community-driven, packages for epi tracking & forecasting.\n\n![](gfx/epiverse_packages_flow.jpg){style=\"width: 60%; display: block; margin-left: auto; margin-right: auto;\"}\n\n<!-- 1. Fetch data: epidatr, epidatpy, and other sources, 2. Explore, clean, transform & backtest 3. Pre-built forecasters, modular forecasting framework: epipredict -->\n  \n  \n  \n# Panel and Versioned Data in the Epiverse\n  \n## What is panel data?\n\n* Recall that [panel data](https://en.wikipedia.org/wiki/Panel_data), or longitudinal data, \ncontain cross-sectional measurements of subjects over time. \n* Built-in example: [`covid_case_death_rates`](\n  https://cmu-delphi.github.io/epidatasets/reference/covid_case_death_rates.html) \ndataset, which is a snapshot [**as of**]{.primary} May 31, 2022 that contains daily state-wise measures of `case_rate` and `death_rate` for COVID-19 over 2021:\n  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n  <chr>     <date>         <dbl>      <dbl>\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 az        2020-12-31      76.8      1.10 \n5 ca        2020-12-31      96.0      0.751\n6 co        2020-12-31      35.8      0.649\n```\n\n\n:::\n:::\n\n\n\n* How do we store & work with such snapshots in the epiverse software ecosystem?\n\n  \n  \n## `epi_df`: Snapshot of a dataset\n\n* You can convert panel data into an `epi_df` with the required `geo_value` and `time_value` columns\n\nTherefore, an `epi_df` is...\n\n* a tibble that requires columns `geo_value` and `time_value`.\n\n* arbitrary additional columns containing [measured values]{.primary}\n\n* additional [keys]{.primary} to index (`age_group`, `ethnicity`, etc.)\n\n::: {.callout-note}\n## `epi_df`\n\nRepresents a [snapshot]{.primary} that\ncontains the most [up-to-date values]{.primary} of the signal variables, [as of]{.primary} a given time.\n:::\n\n## `epi_df`: Snapshot of a dataset\n\n* Consider the same dataset we just encountered on JHU daily COVID-19 cases and deaths rates from all states [as of]{.primary} May 31, 2022.\n\n* We can see that it meets the criteria `epi_df` (has `geo_value` and `time_value` columns) and that it contains additional metadata (i.e. `geo_type`, `time_type`, `as_of`, and `other_keys`).\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedf |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n* <chr>     <date>         <dbl>      <dbl>\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 az        2020-12-31      76.8      1.10 \n5 ca        2020-12-31      96.0      0.751\n6 co        2020-12-31      35.8      0.649\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nattr(edf, \"metadata\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$geo_type\n[1] \"state\"\n\n$time_type\n[1] \"day\"\n\n$as_of\n[1] \"2022-05-31\"\n\n$other_keys\ncharacter(0)\n```\n\n\n:::\n:::\n\n\n\n::: \n\n:::: \n\n## Examples of preprocessing\n\n### EDA features\n\n1. Making locations commensurate (per capita scaling)\n1. Correlating signals across location or time \n1. Computing growth rates\n1. Detecting and removing outliers\n1. Dealing with revisions \n\n## Features - Correlations at different lags\n\n<!-- * There are always at least two ways to compute correlations in an `epi_df`: grouping by `time_value`, and by `geo_value`. \n\n* The latter is obtained by setting `cor_by = geo_value`. -->\n\n* The below plot addresses the question: \"For each state, are case and death rates linearly associated across all days?\"\n\n* To explore **lagged correlations** and how case rates associate with future death rates, we can use the `dt1` parameter in `epi_cor()` to shift case rates by a specified number of days. \n\n<!--  * For example, setting `dt1 = -14` means that case rates on June 1st will be correlated with death rates on June 15th, assessing how past case rates influence future death rates. -->\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor0 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value)\ncor14 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value, dt1 = -14)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-corr-lags-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* We can see that, in general, lagging the case rates back by 14 days improves the correlations.\n\n\n## Features - Systematic lag analysis\n\nThe analysis helps identify the lag at which case rates from the past have the strongest correlation with future death rates.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-sys-lag-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\nThe strongest correlation occurs at a lag of about 23 days, indicating that case rates are best correlated with death rates 23 days from now.\n\n## Features - Compute growth rates\n\n* Growth rate measures the relative change in a signal over time. <!-- indicating how quickly a quantity (like case rates) is increasing or decreasing. -->\n\n* We can compute time-varying growth rates for the two states, and see how this cases evolves over time.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedfg <- filter(edf, geo_value %in% c(\"ut\", \"ca\")) |>\n  group_by(geo_value) |>\n  mutate(gr_cases = growth_rate(time_value, case_rate, method = \"trend_filter\")) |>\n  ungroup()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-growth-rates-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* As expected, the peak growth rates for both states occurred during the January 2022 Omicron wave, reflecting the sharp rise in cases over that period.\n\n## Features - Outlier detection\n\n<!-- * There are multiple outliers in these data that a modeler may want to detect and correct. -->\n\n* The `detect_outlr()` function offers multiple outlier detection methods on a signal.\n\n* The simplest is `detect_outlr_rm()`, which works by calculating an outlier threshold using the rolling median and the rolling Interquartile Range (IQR) for each time point:\n\n**Threshold = Rolling Median ± (Detection Multiplier × Rolling IQR)**\n\n* Note that the default number of time steps to use in the rolling window by default is 21 and is centrally aligned. \n* The detection multiplier default is 2 and controls how far away a data point must be from the median to be considered an outlier.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedfo <- filter(edf, geo_value %in% c(\"ut\", \"ca\")) |>\n  select(geo_value, time_value, case_rate) |>\n  as_epi_df() |>\n  group_by(geo_value) |>\n  mutate(outlier_info = detect_outlr_rm(\n    x = time_value, y = case_rate\n  )) |>\n  ungroup()\n```\n:::\n\n\n\n## Features - Outlier detection\n\n* Several data points that deviate from the expected case cadence have been flagged as outliers, and may require further investigation.\n\n* However, the peak in Jan. 2022 has also been flagged as an outlier. This highlights the importance of manual inspection before correcting the data, as these may represent valid events (e.g., a genuine surge in cases).\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-outlier-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## `epi_archive`: Collection of `epi_df`s\n\n* full version history of a data set\n* acts like a bunch of `epi_df`s --- but stored [compactly]{.primary}\n* allows similar functionality as `epi_df` but using only [data that would have been available at the time]{.primary}\n\n\n::: {.callout-note}\n## Revisions\n\nEpidemiology data gets revised frequently.\n\n* We may want to use the data [as it looked in the past]{.primary} \n* or we may want to examine [the history of revisions]{.primary}.\n:::\n\n## `epi_archive`: Collection of `epi_df`s\n\nFull version history of national provisional death counts. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnchs_allver = pub_covidcast(\n  \"nchs-mortality\",\n  \"deaths_covid_incidence_num\",\n  \"state\",\n  \"week\",\n  # Recall you can specify these options to only pull\n  # a subset of data\n  geo_values = c(\"ca\", \"ut\"),\n  # This data has weekly resolution, different form of \n  # time specification\n  time_values = epirange(202101, 202210),\n  # Recall this pulls all available issues\n  issues = \"*\"\n) |>\n  select(geo_value, time_value, issue, value)\n\nnchs_archive = as_epi_archive(nchs_allver, compactify = TRUE)\n```\n:::\n\n\n\nHow is this data getting revised? \n\n\n## Revision pattern\n\nSome problem here -- NCHS's revision time is on the order of weeks, jumping by months hides a lot of the revisioning. \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-revision-patterns-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Revision pattern -- alternative visualization\n\nIf the above is not satisfactory, we can plot by values of the series, accessed k weeks after the reference date...\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Finalized data\n\n* Counts are revised as time proceeds\n* Want to know the [final]{.primary} value \n* Often not available until weeks/months later\n\n  Forecasting\n: At time $t$, predict the final value for time $t+h$, $h > 0$\n  \n  <br>\n  \n  Backcasting\n: At time $t$, predict the final value for time $t-h$, $h < 0$\n\n  <br>\n  \n  Nowcasting\n: At time $t$, predict the final value for time $t$\n\n# Backfill projection in Epiverse\n\n## Nowcasting simple ratio Ex: NCHS mortality \n\n* In this example, we'll demonstrate the concept of nowcasting using [**NHCS mortality data**]{.primary}.\n(the number of weekly new deaths with confirmed or presumed COVID-19, per 100,000 population).\n* We will work with [**provisional**]{.primary} data (real-time reports) and compare them to **finalized** data (final reports).\n* The goal is to estimate or [**nowcast the mortality rate**]{.primary} for weeks when only provisional data is available.\n  \n## Fetch versioned data\n\nLet's fetch versioned mortality data from the API (`pub_covidcast`) for CA (`geo_values = \"ca\"`) and the signal of interest (`deaths_covid_incidence_num`) over early 2024.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fetch the versioned NCHS mortality data (weekly)\nmortality_archive <- pub_covidcast(\n  source = \"nchs-mortality\",\n  signals = \"deaths_covid_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"week\",\n  geo_values = \"ca\",  # California (CA)\n  time_values = epirange(202401, 202413),  \n  issues = \"*\"\n) |> \n  select(geo_value, time_value, version = issue, mortality = value) |> \n  as_epi_archive(compactify = TRUE)\n\n# Set the start and end days for the analysis \n# corresponding to the weeks entered in time_values\nstart_time = as.Date(\"2023-12-31\")\nend_time = as.Date(\"2024-03-24\")\n```\n:::\n\n\n\n## Latency in reporting - Minimum lag\n\n* A quick inspection reveals that mortality rates are systematically 7 days latent ([**fixed lag**]{.primary}).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmortality_revision_inspect = mortality_archive$DT |> mutate(version_time_diff = version - time_value)\n\n# Look at the first revision for each week\nmortality_revision_inspect |> group_by(time_value) |> slice(1) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n# Groups:   time_value [6]\n  geo_value time_value version    mortality version_time_diff\n  <chr>     <date>     <date>         <dbl> <drtn>           \n1 ca        2023-12-31 2024-01-07        48 7 days           \n2 ca        2024-01-07 2024-01-14        28 7 days           \n3 ca        2024-01-14 2024-01-21        47 7 days           \n4 ca        2024-01-21 2024-01-28        41 7 days           \n5 ca        2024-01-28 2024-02-04        31 7 days           \n6 ca        2024-02-04 2024-02-11        47 7 days           \n```\n\n\n:::\n:::\n\n\n\n* Use `revision_summary()` from `epiprocess` to generate basic statistics about the revision behavior for the dataset.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Latency in reporting - Finalized value attainment\n\n* [**Question:**]{.primary} When is the [**finalized value**]{.primary} first attained for each date? Would we have access to any in real-time?\n* How fast are the final values attained & what's the pattern for these times, if any?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value min_version diff    \n  <chr>     <date>     <date>      <drtn>  \n1 ca        2023-12-31 2024-10-13  287 days\n2 ca        2024-01-07 2024-10-27  294 days\n3 ca        2024-01-14 2024-06-09  147 days\n4 ca        2024-01-21 2024-08-11  203 days\n5 ca        2024-01-28 2024-07-07  161 days\n6 ca        2024-02-04 2024-11-24  294 days\n```\n\n\n:::\n:::\n\n\nAnd here's a numerical summary:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     84     147     189     196     259     294 \n```\n\n\n:::\n:::\n\n\n\n* [**Conclusion**]{.primary}: Tends to take a long time & varies. Even for this relatively small time period... Goes as low as 84 days or as high as 294 days. Yikes.\n* So if we were doing this in real-time, then we wouldn't have access to the finalized data.\n\n## Comparison of final vs. multiple revisions\nThis shows the finalized rates in comparison to [**multiple revisions**]{.primary} to see how the data changes over time:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/final-vs-revisions-plot-1.svg){fig-align='center' height=500px}\n:::\n:::\n\n\n\n## Comparison of final vs. one revision\n\nThe below figure compares the finalized rates (in black) to [**one revision**]{.primary} (in yellow) from March 3, 2024.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/one-revision-final-plot-1.svg){fig-align='center' height=400px}\n:::\n:::\n\n\nThe real-time data is biased downwards (systematically below the true value). That is, the signal tends to get scaled up with future revisions.\n\n\n## Calculate one ratio: Provisional vs. finalized data\n<!-- * Let's start simple with computing one ratio. -->\n\nSuppose that the day is March 10, 2024. Then, because the data is 7 days latent, we can compute the ratio between provisional and finalized data for [**March 3, 2024**]{.primary}.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas_of_date = as.Date(\"2024-03-10\"); fixed_lag = 7\n\n# Load the finalized mortality data for CA\nca_finalized <- mortality_latest |>\n  filter(time_value == (as_of_date - fixed_lag)) |>\n  dplyr::select(mortality)\n\n# Load the provisional mortality data for the same week\nmortality_old = epix_as_of(mortality_archive, as_of_date)\n\nca_provisional <- mortality_old |>\n  filter(time_value == (as_of_date - fixed_lag)) |>\n  dplyr::select(mortality)\n\n# Calculate ratio between provisional and finalized cases for the week of interest\nratio <- ca_provisional$mortality / ca_finalized$mortality\nratio\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3611111\n```\n\n\n:::\n:::\n\n\n\n[**Conclusion**]{.primary}: The real-time rate is well below the finalized for this time (26 vs 72 here).\n\n[**Question**]{.primary}: Can we generalize this over many days? \n\n## Calculating the ratio using multiple dates\nLet's move from calculating the ratio by using one day to multiple days with the goal to use it to nowcast for Feb. 18, which has a [**provisional value**]{.primary} of 23\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas_of_date = as.Date(\"2024-02-25\")\n\nprovisional <- epix_as_of(mortality_archive, as_of_date) |>\n  filter(time_value == as_of_date - 7) |>\n  pull(mortality)\nprovisional\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 23\n```\n\n\n:::\n:::\n\n\n<br>\nand a [**finalized value**]{.primary} of 104\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfinalized <- mortality_latest |>\n  filter(time_value == as_of_date - 7) |>\n  pull(mortality)\nfinalized\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 104\n```\n\n\n:::\n:::\n\n\n\n## Calculating the ratio using multiple dates\nFirst, let's download the real-time rates for CA, and compare them to their finalized version.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndates <- seq(start_time, (as_of_date - 7), by = \"day\")\nmortality_real_time <- function(date) {\n  epix_as_of(mortality_archive, date + 7) |>\n    filter(time_value == date)\n}\nmortality_real_time_df <- map_dfr(dates, mortality_real_time)\nhead(mortality_real_time_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 6 x 3 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-01-07\n\n# A tibble: 6 × 3\n  geo_value time_value mortality\n* <chr>     <date>         <dbl>\n1 ca        2023-12-31        48\n2 ca        2024-01-07        28\n3 ca        2024-01-14        47\n4 ca        2024-01-21        41\n5 ca        2024-01-28        31\n6 ca        2024-02-04        47\n```\n\n\n:::\n:::\n\n\n\n## Calculating the ratio using multiple dates\nNow, let's plot the real-time vs the finalized mortality rates:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/real-time-vs-finalized-1.svg){fig-align='center'}\n:::\n:::\n\n\n* [**Takeaways**]{.primary}: The real-time counts are biased [**well below**]{.primary} the finalized counts.\n* Systematic underreporting tends to lessen over time (the gap between the lines decreases).\n\n## Realistic limitation of nowcasting - Finalized data\n* Recall that real-time access to finalized data is limited as finalized values can take months to report (e.g., Jan. 7 is finalized 294 days later).\n* To nowcast accurately, we must rely on the [**best available approximation of finalized data**]{.primary} at the time of estimation (Feb. 25).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmortality_as_of_feb25 <- epix_as_of(mortality_archive, as_of_date)\nhead(mortality_as_of_feb25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 6 x 3 with metadata:\n* geo_type  = state\n* time_type = week\n* as_of     = 2024-02-25\n\n# A tibble: 6 × 3\n  geo_value time_value mortality\n* <chr>     <date>         <dbl>\n1 ca        2023-12-31       213\n2 ca        2024-01-07       183\n3 ca        2024-01-14       183\n4 ca        2024-01-21       146\n5 ca        2024-01-28       124\n6 ca        2024-02-04       120\n```\n\n\n:::\n:::\n\n\n\n## Ratio calculation & summary\n\nWe then use these \"finalized\" and real-time values to compute the mean ratio:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# exclude date we're nowcasting for\nmortality_real_time_df = mortality_real_time_df |> filter(time_value != \"2024-02-18\") \nmortality_as_of_feb25 = mortality_as_of_feb25 |> filter(time_value != \"2024-02-18\")\nratio_real_time_to_feb25 <- mortality_real_time_df$mortality / mortality_as_of_feb25$mortality\nsummary(ratio_real_time_to_feb25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1530  0.2317  0.2500  0.2565  0.2688  0.3917 \n```\n\n\n:::\n:::\n\n\nOn average, the real-time rates are ~25.7% of the finalized.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/boxplot-ratio-1.svg){fig-align='center'}\n:::\n:::\n\n\nTells us the distribution is right-skewed (mean > median) and so we should opt for the median.\n\n\n## Nowcasting on Feb. 25\n\n* Since the [**median ratio**]{.primary} between real-time and finalized values is [**0.250**]{.primary} (i.e., real-time values are typically 25% of the finalized), then the nowcast is\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Now we can nowcast properly:\nnowcast <- provisional *\n  1 / median(ratio_real_time_to_feb25)\nnowcast\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 92\n```\n\n\n:::\n:::\n\n\n\n* To get the accompanying 95% prediction interval, calculate the 2.5th and 97.5th percentiles:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npercentile_97.5 <- quantile(ratio_real_time_to_feb25, 0.975) |> unname()\n\n(lower_PI <- provisional * 1 / percentile_97.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 61.3268\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npercentile_2.5 <- quantile(ratio_real_time_to_feb25, 0.025) |> unname()\n(upper_PI <- provisional * 1 / percentile_2.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 140.3659\n```\n\n\n:::\n:::\n\n\n\n* So, the [**nowcast is 92**]{.primary} with 95% PI: [61, 140], which is much closer to the [**finalized value of 104**]{.primary} than the [**provisional value of 23**]{.primary}.\n\n## Summary of three main steps \nSo the main steps for this type of fixed lag nowcasting are...\n\n1. Obtain the [**provisional value**]{.primary} for the target.\n\n2. Estimate the ratio using the [**real-time**]{.primary} and [**\"finalized\"**]{.primary} data (for all previous dates that follow a consistent pattern in reporting). \n\n3. Profit.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Expand for the accompanying code\"}\n# Today\nas_of_date = as.Date(\"2024-02-25\")\n\n# 1. Obtain the provisional value\nprovisional <- epix_as_of(mortality_archive, as_of_date) |>\n  filter(time_value == as_of_date - 7) |>\n  pull(mortality)\nprovisional\n\n# 2. Estimate the ratio \nmortality_real_time_df <- map_dfr(dates, mortality_real_time) |> filter(time_value != \"2024-02-18\") # Real-time\nmortality_as_of_feb25 <- epix_as_of(mortality_archive, as_of_date) |> filter(time_value != \"2024-02-18\")  # \"Finalized\"\n\nratio_real_time_to_feb25 <- mortality_real_time_df$mortality / mortality_as_of_feb25$mortality\n\n# 3. Profit.\n(nowcast <- provisional * 1 / median(ratio_real_time_to_feb25))\n\n(upper_PI <- provisional * 1 / quantile(ratio_real_time_to_feb25, 0.025))\n(lower_PI <- provisional * 1 / quantile(ratio_real_time_to_feb25, 0.975))\n```\n:::\n\n\n\n## Nowcasting mortality for multiple dates\n\n* [**Define Nowcast Function**]{.primary}:\n  * [**Input**]{.primary}: Takes in the dates to nowcast and the fixed lag\n  * [**Output**]{.primary}: The nowcasted mortality rates based on the ratio of real-time to finalized data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nnowcast_function <- function(nowcast_date, fixed_lag) {\n  as_of_date = nowcast_date + fixed_lag\n  \n  # 1. Obtain the provisional value for the target.\n  provisional <- epix_as_of(mortality_archive, as_of_date) |>\n    filter(time_value == as_of_date - fixed_lag) |>\n    pull(mortality)\n  \n  #2. Estimate the ratio multiplier using\n  # real-time\n  dates_seq <- seq(start_time, (nowcast_date - fixed_lag), by = \"week\")\n  mortality_real_time <- map_dfr(dates_seq, mortality_real_time)\n  \n  # and \"finalized\" data\n  finalized <- epix_as_of(mortality_archive, as_of_date) |> filter(time_value >= start_time & time_value <= (nowcast_date - fixed_lag)) \n  \n  ratios <- mortality_real_time$mortality / finalized$mortality\n  \n  # Remove infinite or NaN ratios (i.e., keep only finite values)\n  median_ratio <- median(ratios[is.finite(ratios)])\n  \n  #3. Profit.\n  nowcast <- provisional * (1 / median_ratio)\n  upper_PI <- provisional * (1 / quantile(ratios[is.finite(ratios)], 0.025))\n  lower_PI <- provisional * (1 / quantile(ratios[is.finite(ratios)], 0.975))\n  \n  # Return a dataframe with the nowcast and date\n  tibble(\n    time_value = nowcast_date,\n    nowcast = nowcast,\n    lower_PI = lower_PI,\n    upper_PI = upper_PI \n  )\n}\n```\n:::\n\n\n\n## Map nowcast over multiple dates\n* We can use `map2()` to apply the function to a series of weeks (e.g., Jan. 28 to Mar. 24).\n* Returns a [**dataframe**]{.primary} with nowcasted results.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Apply Nowcast Function Over Multiple Dates\nnowcast_dates <- seq(as.Date(\"2024-01-28\"), as.Date(\"2024-03-24\"), by = \"week\")\nfixed_lag <- 7\nnowcast_results_df <- map2(nowcast_dates, fixed_lag, nowcast_function) |> list_rbind()\n```\n:::\n\n\n\nLet's smooth with a rolling trailing mean (window size 4) & see the results:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 4\n  time_value nowcast lower_PI upper_PI\n  <date>       <dbl>    <dbl>    <dbl>\n1 2024-01-28   116.      80.7     181.\n2 2024-02-04   145.     109.      234.\n3 2024-02-11   114.      83.1     187.\n4 2024-02-18   109.      77.6     175.\n5 2024-02-25   110.      79.0     178.\n6 2024-03-03    92.8     59.3     148.\n7 2024-03-10    90.3     57.3     144.\n8 2024-03-17    82.6     51.2     134.\n9 2024-03-24    65.4     38.3     108.\n```\n\n\n:::\n:::\n\n\n\n## Visualize nowcast, real-time, and finalized values\nFinally, we can compare these nowcast results to the real-time and finalized values:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcast-fun-plot-results-1.svg){fig-align='center'}\n:::\n:::\n\n\nThe real-time counts tend to be biased below the finalized counts. Nowcasted values tend to provide a much better approximation of the truth (at least for these dates).\n\n## Evaluation using MAE\n\n* Assume we have prediction $\\hat y_{t}$ for the provisional value at time $t$.\n\n* Then for $y_{t}$ over times $t = 1, \\dots, N$, then we may compute error metrics like mean absolute error (MAE).\n\n<!-- We'll see other error measures later on! For now, let's start with one that is simple and easy to interpret.-->\n\n* MAE measures the average absolute difference between the nowcast and finalized values. \n\n$$MAE = \\frac{1}{N} \\sum_{t=1}^N |y_{t}- \\hat y_{t}|$$\n\n* Note that it's scale-dependent, meaning it can vary depending on the units of the data (e.g., cases, deaths, etc.).\n\n## Evaluation using MAE\n\nLet's numerically evaluate our point nowcasts for the provisional values of a time series (e.g., COVID-19 mortality) using MAE.\n\n<!-- Accuracy of nowcast is assessed by how close provisional estimates are to the finalized values to gauge the model's performance. -->\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 1: Join the mortality data with nowcast data\nmae_data <- mortality_latest |> \n  filter(time_value %in% nowcast_dates) |>   \n  left_join(nowcast_results_df, by = \"time_value\") |> \n  left_join(map_dfr(nowcast_dates, mortality_real_time) |> rename(real_time = mortality), by = c(\"geo_value\", \"time_value\"))\n\n# Step 2: Calculate the absolute error between actual and nowcasted values\nmae_data <- mae_data |> \n  mutate(nc_abs_error = abs(mortality - nowcast),\n         rt_abs_error = abs(mortality - real_time))  \n\n# Step 3: Compute the MAE (mean of absolute errors)\nmae_value <- mae_data |> \n  summarise(nc_MAE = mean(nc_abs_error),\n            rt_MAE = mean(rt_abs_error))\nknitr::kable(mae_value)\n```\n\n::: {.cell-output-display}\n\n\n|   nc_MAE|   rt_MAE|\n|--------:|--------:|\n| 15.00924| 71.55556|\n\n\n:::\n:::\n\n\n\n## Evaluation using MAE\n\nFinally, we may visualize the distribution of errors across time:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/abs-error-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nWe can see that the absolute errors are almost always lower for nowcasting.\n\n# Nowcasting with auxilliary variables \n\n## Mathematical setup\n\n* **Nowcasting**: Predict a finalized value from a provisional value.\n\n* Suppose today is time $t$\n\n* Let $y_i$ denote a series of interest observed at times $i=1,\\ldots, t$.\n\n::: {.callout-important icon=\"false\"}\n## Our goal\n\n* Produce a [**point nowcast**]{.primary} for the finalized values of $y_t$.\n* Accompany with time-varying prediction intervals\n\n:::\n\n* We also have access to $p$ other time series \n$x_{ij},\\; i=1,\\ldots,t, \\; j = 1,\\ldots,p$\n\n* All may be subject to revisions.\n\n\n\n\n## Nowcasting: Moving from one signal to two\n\n* Recall that in nowcasting the goal is to predict a finalized value from a provisional value.\n* Now, we'll move from one signal to two, creating a simple linear model to nowcast.\n* Exogenous features (predictors) could include relevant signals, such as Google symptom search trends.\n* We will use these signals to nowcast hospital admissions related to influenza.\n\n\n## Data Sources: Google searches & hospital admissions\n\n* [**Google Search Trends**]{.primary}: Symptoms like cough, fever, and shortness of breath.\n  * [**s01**]{.primary}: Cough, Phlegm, Sputum, Upper respiratory tract infection  \n  * [**s02**]{.primary}: Nasal congestion, Post nasal drip, Sinusitis, Common cold\n\n* [**Hospital Admissions**]{.primary}: Data from the Department of Health & Human Services on confirmed influenza admissions.\n\n* Using these, we will [**nowcast**]{.primary} hospital admissions by using Google symptom search trends for GA from April to June 2023.\n\n* The first step is to fetch this data...\n\n## Data Sources: Google searches & hospital admissions\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fetch Google symptom data for s01 and s02\nx1 <- pub_covidcast(\n  source = \"google-symptoms\",\n  signals = \"s01_smoothed_search\", \n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = epirange(20230401, 20230701),\n  issues = \"*\"\n) |>\n  select(geo_value, time_value, version = issue, avg_search_vol_s01 = value) |>\n  as_epi_archive(compactify = FALSE)\n\nx2 <- pub_covidcast(\n  source = \"google-symptoms\",\n  signals = \"s02_smoothed_search\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = epirange(20230401, 20230701),\n  issues = \"*\"\n) |>\n  select(geo_value, time_value, version = issue, avg_search_vol_s02 = value) |>\n  as_epi_archive(compactify = FALSE)\n\n# Fetch hospital admissions data\ny1 <- pub_covidcast(\n  source = \"hhs\",\n  signals = \"confirmed_admissions_influenza_1d\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = epirange(20230401, 20230701),\n  issues = \"*\"\n) |>\n  select(geo_value, time_value, version = issue, admissions = value) |>\n  as_epi_archive(compactify = FALSE)\n```\n:::\n\n\n\n## Merging the archives\n\n* We'll merge the symptom search trends (`x1`, `x2`) with hospital admissions data (`y`) using `epix_merge()` from `epiprocess`.\n* This allows us to match data by time and geography, & fill any missing values with the most recent observation (LOCF).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Linear Model: A simple approach for nowcasting\n\n* Aside from ratios, one of the simplest approach to nowcasting is to use a [**linear regression model**]{.primary}.\n* We model the relationship between provisional (predictor) data and response data.\n* This model helps us make [**predictions**]{.primary} for the finalized data based on the current (provisional) signals.\n\n## Linear regression\n* [**Goal**]{.primary}: Estimate the coefficients $\\beta_0$ and $\\beta_1$ that describe the relationship between the predictor $x_i$ and the outcome $y_i$.\n* [**Linear Model**]{.primary}: The relationship is assumed to be:\n\n  $$y_i \\approx \\beta_0 + \\beta_1 x_i $$\n  \n  where\n  $\\beta_0$ is the intercept,\n  $\\beta_1$ is the slope.\n* **In R**: Use `lm(y ~ x)` to estimate the coefficients, where `y` is the outcome variable and `x` is the predictor.\n\n## Multiple linear regression\n* [**Goal**]{.primary}: Estimate coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$ that describe the relationship between multiple predictors $x_{i1}, x_{i2}, \\dots, x_{ip}$ and the outcome $y_i$.\n* [**Model**]{.primary}: The relationship is assumed to be:\n\n  $$y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}$$\n  \n  where:\n  $\\beta_0$ is the intercept,\n  $\\beta_1, \\dots, \\beta_p$ are the coefficients.\n* [**In R**]{.primary}: Use `lm(y ~ x1 + x2 + ... + xp)` to estimate the coefficients, where `y` is the outcome and `x1, x2, ..., xp` are the predictors.\n\n## Multiple linear regression model\n\n* A linear model is a good choice to describe the relationship between search trends and hospital admissions.\n* The model will include two predictors (s01 and s02).\n* We'll use these two search trend signals to predict hospital admissions (response).\n\n<!-- A linear regression model will be used to predict hospital admissions from search trends (s01 and s02). -->\n\n## Multiple linear regression model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the function for lm model fit and prediction\nlm_mod_pred <- function(data, gk, rtv, ...) {\n  \n  # Fit the linear model\n  model <- lm(admissions ~ avg_search_vol_s01 + avg_search_vol_s02, data = data)\n  \n  # Make predictions\n  predictions = predict(model,\n                        newdata = data |>\n                          # Use tidyr::fill() for LOCF if predictor data is incomplete \n                          fill(avg_search_vol_s01, .direction = \"down\") |> \n                          fill(avg_search_vol_s02, .direction = \"down\") |>\n                          filter(time_value == max(time_value)),\n                        interval = \"prediction\", level = 0.9\n  )\n\n  # Pull off true time value for comparison to target\n  real_time_val = data |> filter(time_value == max(time_value)) |> pull(admissions)\n\n  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val))\n}\n```\n:::\n\n\nNote that this code is intentionally simple; while it can be refined to handle cases like negatives or other boundary conditions, we aim to avoid unnecessary complexity.\n\n## Nowcasting with `epix_slide()`\n\n* We will use `epix_slide()` to create a sliding window of training data.\n* The model will be trained on a 14-day window before the target date, and predictions will be made for the target date.\n* The beauty of this function is that it is version-aware - the sliding computation at any given reference time [**t**]{.primary} is performed on data that would have been available as of [**t**]{.primary} automatically. \n\n## Nowcasting with `epix_slide()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the reference time points for nowcasting\ntargeted_nowcast_dates <- seq(as.Date(\"2023-04-15\"), as.Date(\"2023-06-15\"), by = \"1 week\")\nref_time_values = targeted_nowcast_dates + 2  # Adjust for the systematic 2-day latency in the response\n# Determine this from revision_summary(y1, print_inform = TRUE) \n\n# Perform nowcasting using epix_slide\nnowcast_res <- archive |>\n  group_by(geo_value) |>\n  epix_slide(\n    .f = lm_mod_pred,\n    .before = 14,  # 14-day training period\n    .versions = ref_time_values, \n    .new_col_name = \"res\"\n  ) |>\n  unnest() |> # Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns. \n  mutate(targeted_nowcast_date = targeted_nowcast_dates, time_value = actual_nowcast_date) |>\n  ungroup()\n\n# View results\nhead(nowcast_res, n=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 9\n  geo_value version      fit    lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>     <dbl>  <dbl> <dbl> <date>                      <dbl>\n1 ga        2023-04-17  4.64 -0.122  9.39 2023-04-15                      4\n2 ga        2023-04-24  7.36  1.56  13.2  2023-04-22                      4\n# ℹ 2 more variables: targeted_nowcast_date <date>, time_value <date>\n```\n\n\n:::\n:::\n\n\n\n## Compare with the actual admissions \n\nAfter making predictions, we compare them to the actual hospital admissions.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Left join with latest results \n# Latest snapshot of data (with the latest/finalized admissions)\nx_latest <- epix_as_of(archive, max(archive$DT$version)) |> select(-c(avg_search_vol_s01, avg_search_vol_s02))\n\nres <- nowcast_res |> left_join(x_latest, by = join_by(geo_value, time_value))\nhead(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n  geo_value version      fit    lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>     <dbl>  <dbl> <dbl> <date>                      <dbl>\n1 ga        2023-04-17  4.64 -0.122  9.39 2023-04-15                      4\n2 ga        2023-04-24  7.36  1.56  13.2  2023-04-22                      4\n3 ga        2023-05-01  6.06  1.57  10.5  2023-04-29                      5\n4 ga        2023-05-08  5.01  1.28   8.74 2023-05-06                      6\n5 ga        2023-05-15  8.14  5.69  10.6  2023-05-11                      8\n6 ga        2023-05-22  3.43 -2.35   9.21 2023-05-20                      4\n# ℹ 3 more variables: targeted_nowcast_date <date>, time_value <date>,\n#   admissions <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Visualizing the nowcast results\n\nWe can then visualize the nowcast results alongside the true values using `ggplot2`:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-multiple-lr-nowcast-res-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Evaluation using MAE\n\n* As before, we can evaluate our point nowcasts numerically using MAE.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the absolute error between actual and nowcasted values\nmae_data_admissions <- res |> \n  mutate(nc_abs_error = abs(admissions - fit),  # Nowcast vs Finalized admissions\n         rt_abs_error = abs(admissions - real_time_val))  # Real-Time vs Finalized admissions\n\n# Compute the MAE (mean of absolute errors)\nmae_value_admissions <- mae_data_admissions |> \n  summarise(nc_MAE = mean(nc_abs_error),\n            rt_MAE = mean(rt_abs_error))\nknitr::kable(mae_value_admissions)\n```\n\n::: {.cell-output-display}\n\n\n|   nc_MAE|   rt_MAE|\n|--------:|--------:|\n| 1.180965| 2.333333|\n\n\n:::\n:::\n\n\n\n* Based off of comparing these simple error measures, the nowcast MAE is clearly better.\n\n## Evaluation using MAE\n\nHowever, when we visualize the distribution of errors across time, it is not so cut-and-dry:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/abs-error-plot-hosp-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nThe main driver behind the real-time MAE being greater is the \"outlier-like\" May 25 AE.\n\nSo visualizing can provide an important perspective that is missed from a simple numerical summary of error.\n\n## Key Takeaways: Linear regression nowcasting example\n\n* [**Provisional Data as Predictors**]{.primary}: Using [Google symptom search trends]{.primary} to predict [influenza hospital admissions]{.primary}.\n* [**Simple Linear Model**]{.primary}: A linear regression model captures the relationship between symptom searches and hospital admissions.\n* [**Actionable Predictions**]{.primary}: Nowcasts provide [timely insights]{.primary} for hospital admissions, even before data is finalized.\n* [**Sliding Window Approach**]{.primary}: Predictions are based on [data up to the current time]{.primary}, ensuring no future information influences the nowcast.\n* [**Evaluation**]{.primary}: Predictions are compared with actual admissions using numerical and visual perspectives.\n\n# Case Study - Nowcasting Cases Using %CLI \n\n## Goal of this case study\n\n[**Goal**]{.primary}: Nowcast COVID-19 Cases for MA using the estimated percentage of COVID-related doctor's visits (%CLI), based on outpatient data from Optum.\n\n* %CLI is contained in the Epidata API.\n* Cases by specimen collection date are not. They are from the MA gov website.\n* Cases in the API (JHU) are aligned by report date, not specimen collection/test date.\n* Working with cases aligned by [**test date**]{.primary} allows us to avoid the more unpredictable delays introduced by the [**report date**]{.primary}.\n\n## Summary of main steps\n\nThe workflow is similar to the previous example where we nowcasted using two variables, only more involved. \nThe main steps are...\n\n1. [**Fetch Data**]{.primary}: Retrieve %CLI and COVID-19 case data (by specimen collection date) for MA.\n\n2. [**Merge Data**]{.primary}: Align %CLI and case data using `epix_merge`, filling missing values via last observation carried forward (LOCF).\n\n3. [**Model & Prediction**]{.primary}: Fit a linear model to predict cases based on %CLI, trained on a 30-day rolling window.\n\n4. [**Nowcast Execution**]{.primary}: Use `epix_slide` to nowcast the cases dynamically. \n\n5. [**Visualization**]{.primary}: Plot actual vs. nowcasted cases with confidence intervals to assess model accuracy.\n\nSo the first step is to fetch the data...\n\n## Construct an `epi_archive` from scratch\n\n[Here's](\"https://www.mass.gov/info-details/archive-of-covid-19-cases-2020-2021\") the archive of COVID-19 case excel files from the MA gov website, which we'll use to construct our own `epi_archive`.\n<br>\n<br>\nBrief summary of this data:\n\n* [**First release**]{.primary}: Raw .xlsx data was first released early January 2021.\n\n* [**Change in reporting**]{.primary}: Starting [**July 1, 2021**]{.primary}, the dashboard shifted from [**7 days/week**]{.primary} to [**5 days/week**]{.primary} (Monday-Friday).\n\n* [**Friday, Saturday, and Sunday**]{.primary} data is included in the [**Monday**]{.primary} dashboard.\n\n* When [**Monday**]{.primary} is a holiday, the [**Friday through Monday**]{.primary} data is posted on [**Tuesday**]{.primary}.\n\n\n## Construct an `epi_archive` from scratch\n\n* [**Purpose**]{.primary}: To create an `epi_archive` object for storing versioned time series data.\n* [**Required Columns**]{.primary}:\n  * `geo_value`: Geographic data (e.g., region).\n  * `time_value`: Time-related data (e.g., date, time).\n  * `version`: Tracks when the data was available (enables version-aware forecasting).\n* [**Constructor**]{.primary}:\n  * `new_epi_archive()`: For manual construction of `epi_archive` (assumes validation of inputs).\n* [**Recommended Method**]{.primary}:\n  * `as_epi_archive()`: Simplifies the creation process, ensuring proper formatting and validation. We'll use this one when we download some data from the MA gov website!\n\n\n## Main steps to construct the `epi_archive`\n\n1. [**Load necessary Libraries**]{.primary}: Such as `tidyverse`, `readxl`, `epiprocess`.\n2. [**Process Each Date's Data**]{.primary}: \n   * A function we'll make (`process_covid_data`) downloads and processes daily COVID-19 data from the MA gov Excel files on their website.\n   * The data is cleaned and formatted with columns: `geo_value`, `time_value`, `version`, and values.\n3. [**Handle Missing Data**]{.primary}: Checks if a date's data is available (handle 404 errors).\n4. [**Create `epi_archive`**]{.primary}: \n   * Combine processed data into a tibble.\n   * Convert the tibble to an `epi_archive` object using `as_epi_archive()`.\n\n\n## Fetch Data - Code for one date\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)\nlibrary(tibble)\nlibrary(epiprocess)\n\n# Function to download and process each Excel file for a given date\nprocess_covid_data <- function(Date) {\n  # Generate the URL for the given date\n  url <- paste0(\"https://www.mass.gov/doc/covid-19-raw-data-\", tolower(gsub(\"-0\", \"-\", format(Date, \"%B-%d-%Y\"))), \"/download\") \n  # Applies gsub(\"-0\", \"-\", ...) to replace any occurrence of -0 (such as in \"April-01\") with just - (resulting in \"April-1\").\n  \n  # Check if the URL exists (handle the 404 error by skipping that date)\n  response <- GET(url)\n  \n  if (status_code(response) != 200) {\n    return(NULL)  # Skip if URL doesn't exist (404)\n  }\n  \n  # Define the destination file path for the Excel file\n  file_path <- tempfile(fileext = \".xlsx\")\n  \n  # Download the Excel file\n  GET(url, write_disk(file_path, overwrite = TRUE))\n  \n  # Read the relevant sheet from the Excel file\n  data <- read_excel(file_path, sheet = \"CasesByDate (Test Date)\")\n  \n  # Process the data: rename columns and convert Date\n  data <- data |>\n    rename(\n      Date = `Date`,\n      Positive_Total = `Positive Total`,\n      Positive_New = `Positive New`,\n      Case_Average_7day = `7-day confirmed case average`\n    ) |>\n    mutate(Date = as.Date(Date))  # Convert to Date class\n  \n  # Create a tibble with the required columns for the epi_archive\n  tib <- tibble(\n    geo_value = \"ma\",  # Massachusetts (geo_value)\n    time_value = data$Date,  # Date from the data\n    version = Date,  # The extracted version date\n    case_rate_7d_av = data$Case_Average_7day  # 7-day average case value\n  )\n  \n  return(tib)\n}\n```\n:::\n\n\n\n## Fetch Data - Code breakdown \n\n* This purpose of this function is to download and process each Excel file as of a date.\n* [**URL Creation**]{.primary}: Dynamically generates the URL based on the date, removing leading zeros in day values (e.g., \"April-01\" → \"April-1\").\n* [**Check URL**]{.primary}: Sends a request (`GET(url)`) and skips the date if the URL returns a non-200 status (e.g., 404 error).\n* [**Download File**]{.primary}: Saves the Excel file to a temporary path using `tempfile()` and `GET()`.\n* [**Read Data**]{.primary}: Loads the relevant sheet (\"CasesByDate\") from the Excel file using `read_excel()`.\n* [**Tibble Creation**]{.primary}: Constructs a tibble with `geo_value`, `time_value`, `version`, and `case_rate_7d_av` to later compile into an `epi_archive` (you can think of an `epi_archive` as being a comprised of many `epi_df`s).\n\n\n## Fetch Data - Process eange of dates\n* Note that `process_covid_data()` works on one date at a time.\n* So now, we need a function that iterates over a date range and applies `process_covid_data()` to each date & combines the resulting tibbles into an `epi_archive`.\n* We call this function `process_data_for_date_range()`...\n\n## Fetch Data - Process range of dates\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Function to process data for a range of dates\nprocess_data_for_date_range <- function(start_date, end_date) {\n  # Generate a sequence of dates between start_date and end_date\n  date_sequence <- seq(as.Date(start_date), as.Date(end_date), by = \"day\")\n  \n  # Process data for each date and combine results\n  covid_data_list <- lapply(date_sequence, function(Date) {\n    process_covid_data(Date)  # Skip over dates with no data (NULLs will be ignored)\n  })\n  \n  # Combine all non-null individual tibbles into one data frame\n  combined_data <- bind_rows(covid_data_list[!sapply(covid_data_list, is.null)])\n  \n  # Convert the combined data into an epi_archive object\n  if (nrow(combined_data) > 0) {\n    epi_archive_data <- combined_data |>\n      as_epi_archive(compactify = FALSE)\n    \n    return(epi_archive_data)\n  } else {\n    message(\"No valid data available for the given date range.\")\n    return(NULL)\n  }\n}\n```\n:::\n\n\n\n## Fetch Data - Code breakdown\nHere's a summary of what `process_data_for_date_range()` does:\n1. [**Generates Date Range**]{.primary}: Creates a sequence of dates between `start_date` and `end_date`.\n\n2. [**Processes Data**]{.primary}: Applies the `process_covid_data` function to each date in the range (skip over dates with no data).\n\n3. [**Combines Results**]{.primary}: Combines all valid (non-NULL) tibbles into one single data frame.\n\n4. [**Creates `epi_archive`**]{.primary}: Converts the combined data into an `epi_archive` object.\n\n## Fetch Data - Run the function & inspect archive\n\n* Now, let's run the function & inspect the resulting `epi_archive` of 7-day avg. COVID-19 case counts:\n* Expect building the archive to some time (enough for a cup of coffee or to meditate on life).\n\n<!-- To wonder why you chose Expect building the archive to take a nontrivial amount of time (enough for a cup of coffee or to wonder why you chose coding in the first place). -->\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Example usage: process data between Jan. 10, 2021, and Dec. 1, 2021\n# y <- process_data_for_date_range(\"2021-01-10\", \"2021-12-01\")  # Raw .xlsx data is first released on Jan. 4, 2021\n# y\n```\n:::\n\n\n\n* Alternatively, you may run the following to load `y` that was previously saved as an RDS file: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- readRDS(\"_data/ma_case_archive.rds\")\n```\n:::\n\n\n\n\n## Fetch Data - % Outpatient doctors visits for CLI\n\n* Now, from the Epidata API, let's download the [estimated percentage of outpatient doctor visits](\"https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/doctor-visits.html\") primarily for COVID-related symptoms, based on health system data.\n* Comes pre-smoothed in time using a Gaussian linear smoother\n* This will be the predictor when we nowcast COVID-19 cases in MA.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 1: Fetch Versioned Data \nx <- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ma\", # Just for MA to keep it simple (& to go with the case data by test date for that state)\n  time_values = epirange(20210301, 20211231),\n  issues = epirange(20210301, 20211231)\n) |>\n  select(geo_value, time_value,\n         version = issue,\n         percent_cli = value\n  ) |>\n  as_epi_archive(compactify = FALSE)\n```\n:::\n\n\n\n## Use `epix_merge()` to merge the two archives\nNow we'll use `epix_merge()` to combine the two `epi_archive`s that share the same `geo_value` & `time_value`.\n\n<!-- LOCF is used to ensure missing data is handled by filling forward. -->\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narchive <- epix_merge(\n  x, y,\n  sync = \"locf\",\n  compactify = FALSE\n)\narchive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-01-29 / 2021-12-13\nℹ First/last version with update: 2021-01-10 / 2021-12-17\nℹ Versions end: 2021-12-17\nℹ A preview of the table (139190 rows x 5 columns):\nKey: <geo_value, time_value, version>\n        geo_value time_value    version percent_cli case_rate_7d_av\n           <char>     <Date>     <Date>       <num>           <num>\n     1:        ma 2020-01-29 2021-01-10          NA              NA\n     2:        ma 2020-01-29 2021-01-11          NA              NA\n     3:        ma 2020-01-29 2021-01-12          NA              NA\n     4:        ma 2020-01-29 2021-01-13          NA              NA\n     5:        ma 2020-01-29 2021-01-14          NA              NA\n    ---                                                            \n139186:        ma 2021-12-11 2021-12-16    2.306966              NA\n139187:        ma 2021-12-11 2021-12-17    2.281141              NA\n139188:        ma 2021-12-12 2021-12-16    2.333759              NA\n139189:        ma 2021-12-12 2021-12-17    2.369756              NA\n139190:        ma 2021-12-13 2021-12-17    2.256551              NA\n```\n\n\n:::\n:::\n\n\n\n## Fitting and predicting with linear model\n\n* Define `lm_mod_pred()`: A function that fits a linear model to forecast cases based on the `percent_cli` predictor.\n* Use `predict()` with a 90% prediction interval.\n* Save the actual cases to compare to the nowcasts later.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_mod_pred <- function(data, ...) {\n  # Linear model\n  model <- lm(case_rate_7d_av ~ percent_cli, data = data)\n\n  # Make predictions\n  predictions = predict(model,\n                        newdata = data |>\n                          fill(percent_cli, .direction = \"down\") |> \n                          filter(time_value == max(time_value)),\n                        interval = \"prediction\", level = 0.9)\n  \n  # Pull off real-time value for later comparison to the nowcast value\n  real_time_val = data |> filter(time_value == max(time_value)) |> pull(case_rate_7d_av)\n  \n  # Could clip predictions and bounds at 0\n  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val)) \n}\n```\n:::\n\n\n\n## Nowcasting with `epix_slide()`\n* [**Specify targets**]{.primary}: Define the target dates for nowcasting (e.g., 1st of each month) & adjust training data to include the lag for the latent case data.\n* [**Sliding window**]{.primary}: Use `epix_slide()` to apply the linear model across a sliding window of data for each region.\n* [**Training-test split**]{.primary}: Use the last 30 days of data to train and predict cases for each target nowcast date.\n\n## Nowcasting with `epix_slide()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the reference time points (to give the training/test split)\ntargeted_nowcast_dates <- seq(as.Date(\"2021-04-01\"), as.Date(\"2021-11-01\"), by = \"1 month\") \nref_time_values = targeted_nowcast_dates + 1 # + 1 because the case data is 1 day latent. \n# Determine this from revision_summary(y)\n\n# Use epix_slide to perform the nowcasting with a training-test split\nnowcast_res <- archive |>\n  group_by(geo_value) |>\n  epix_slide(\n    .f = lm_mod_pred,  # Pass the function defined above\n    .before = 30,   # Training period of 30 days\n    .versions = ref_time_values, # Determines the day where training data goes up to (not inclusive)\n    .new_col_name = \"res\"\n  ) |>\n  unnest() |>\n  mutate(targeted_nowcast_date = targeted_nowcast_dates,\n         time_value = actual_nowcast_date)\n\n# Take a peek at the results\nhead(nowcast_res, n = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n# Groups:   geo_value [1]\n  geo_value version      fit   lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>     <dbl> <dbl> <dbl> <date>                      <dbl>\n1 ma        2021-04-02 2114. 1975. 2254. 2021-04-01                  1556.\n# ℹ 2 more variables: targeted_nowcast_date <date>, time_value <date>\n```\n\n\n:::\n:::\n\n\n\n## Visualizing nowcasts vs. actual values\nMerge the nowcast results with the latest data for more direct comparison:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx_latest <- epix_as_of(archive, max(archive$DT$version)) |>\n  select(-percent_cli) \n\nres <- nowcast_res |> left_join(x_latest, by = join_by(geo_value, time_value))\n\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 10\n# Groups:   geo_value [1]\n  geo_value version       fit    lwr   upr actual_nowcast_date real_time_val\n  <chr>     <date>      <dbl>  <dbl> <dbl> <date>                      <dbl>\n1 ma        2021-04-02 2114.  1975.  2254. 2021-04-01                  1556.\n2 ma        2021-05-02 1083.   851.  1316. 2021-05-01                   869.\n3 ma        2021-06-02  353.   164.   541. 2021-06-01                   117.\n4 ma        2021-07-02   57.1   11.3  103. 2021-07-01                    59 \n5 ma        2021-08-02  513.   284.   742. 2021-08-01                   572.\n6 ma        2021-09-02 1207.   888.  1527. 2021-09-01                  1099.\n7 ma        2021-10-02 1575.  1357.  1793. 2021-09-30                  1069 \n8 ma        2021-11-02 1299.  1257.  1340. 2021-11-01                   891.\n# ℹ 3 more variables: targeted_nowcast_date <date>, time_value <date>,\n#   case_rate_7d_av <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Visualizing nowcasts vs. actual values\n\nNow, plot the predictions & real-time values on top of latest COVID-19 cases using `ggplot2`:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-simple-lr-nowcast-res-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Evaluation using MAE\n\n* Finally, we numerically evaluate our nowcasts using MAE.\n\n* Shows that the nowcast errors are lower than those of the real-time estimates.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the absolute error between actual and nowcasted COVID-19 cases\nmae_data_cases <- res |> \n  mutate(nc_abs_error = abs(case_rate_7d_av - fit),  # Nowcast vs Finalized cases (7-day average)\n         rt_abs_error = abs(case_rate_7d_av - real_time_val))  # Real-Time vs Finalized cases\n\n# Compute the MAE (mean of absolute errors)\nmae_value_cases <- mae_data_cases |> \n  summarise(nc_MAE = mean(nc_abs_error),\n            rt_MAE = mean(rt_abs_error))\nknitr::kable(mae_value_cases)\n```\n\n::: {.cell-output-display}\n\n\n|geo_value |   nc_MAE|   rt_MAE|\n|:---------|--------:|--------:|\n|ma        | 152.5013| 228.5357|\n\n\n:::\n:::\n\n\n\n\n## Evaluation using MAE\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/abs-error-plot-cases-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nThe elevated errors at both ends highlight periods where the discrepancies between the real-time and nowcast estimates are most pronounced.\n\n## Takeaways\n\n[**Goal**]{.primary}: Predict COVID-19 cases using %CLI, overcoming delays in report data.\n\nMain Steps:\n\n1. [**Fetch Data**]{.primary}: Collect case and %CLI data.\n\n2. [**Merge Data**]{.primary}: Align datasets with `epix_merge()` and fill missing values.\n\n3. [**Model**]{.primary}: Fit a linear model to predict cases.\n\n4. [**Nowcast**]{.primary}: Apply dynamic forecasting with `epix_slide()`.\n\n5. [**Evaluate**]{.primary}: Calculate error measures and numerically and visually assess the results.\n\nOverall, nowcasting, based on the linear model, provided a closer approximation of true cases compared to the real-time values.\n\n\n## Bonus\n\n\n## Aside on nowcasting\n\n* To some Epis, \"nowcasting\" can be equated with \"estimate the time-varying instantaneous reproduction number, $R_t$\"\n\n* Ex. using the number of reported COVID-19 cases in British Columbia between Jan. 2020 and Apr. 15, 2023. \n\n<!-- This data is the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. The values are.up-to-date as of August 2023. -->\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcasting-1.svg){fig-align='center' height=400px}\n:::\n:::\n\n\n\n* Group built [`{rtestim}`](https://dajmcdon.github.io/rtestim) doing for this nonparametrically.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}