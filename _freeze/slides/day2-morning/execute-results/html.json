{
  "hash": "46bc6f2dfcd67feb4ae5e2414fb1e197",
  "result": {
    "markdown": "---\ntalk-title: \"Forecasting and Time-Series Models\"\ntalk-short-title: \"{{< meta talk-title >}}\"\ntalk-subtitle: \"\"\nauthor: \"\"\nother-authors: \"\"\nrepo-address: \"cmu-delphi/insightnet-workshop-2024\"\ntalk-date: \"Venue -- dd Somemonth yyyy\"\nformat: revealjs\nexecute:\n  cache: false\n---\n\n\n<!-- Set any of the above to \"\" to omit them -->\n\n<!-- Or adjust the formatting in _titleslide.qmd -->\n---\n---\n\n\\DeclareMathOperator*{\\minimize}{minimize}\n\n\n\n\n\n\n\n::: flex\n::: w-20\n\n:::\n::: w-80\n## {{< meta talk-title >}} {background-image=\"gfx/cover-art-1.svg\" background-position=\"bottom\"}\n\n### {{< meta talk-subtitle >}}\n\n<br>\n\n#### {{< meta author >}} \n{{< meta other-authors >}}\n\n{{< meta talk-date >}}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Outline\n\n1. Linear Regression for Time Series Data\n\n1. Evaluation Methods\n\n1. ARX Models\n\n1. Overfitting and Regularization\n\n1. Prediction Intervals\n\n1. Forecasting with Versioned Data\n\n1. Modeling Multiple Time Series\n\n\n# Linear Regression for Time Series Data\n\n## Basics of linear regression \n\n* Assume we observe a predictor $x_i$ and an outcome $y_i$ for $i = 1, \\dots, n$.\n\n* Linear regression seeks coefficients $\\beta_0$ and $\\beta_1$ such that\n\n$$y_i \\approx \\beta_0 + \\beta_1 x_i$$\n\nis a good approximation for every $i = 1, \\dots, n$.\n\n* In R, the coefficients are found by running `lm(y ~ x)`, where `y` is the vector \nof responses and `x` the vector of predictors.\n\n\n## Multiple linear regression \n\n* Given $p$ different predictors, we seek $(p+1)$ coefficients such that\n\n$$y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}$$\nis a good approximation for every $i = 1, \\dots, n$.\n\n\n## Linear regression with lagged predictor\n\n* In time series, outcomes and predictors are usually indexed by time $t$. \n\n* [Goal]{.primary}: predicting future $y$, given present $x$. \n\n* [Model]{.primary}: linear regression with lagged predictor\n\n$$\\hat y_t = \\hat \\beta + \\hat \\beta_0 x_{t-k}$$\n\ni.e. regress the outcome $y$ at time $t$ on the predictor $x$ at time $t-k$.\n\n* [Equivalent]{.primary} way to write the model: \n\n$$\\hat y_{t+k} = \\hat \\beta + \\hat \\beta_0 x_t$$\n\n\n## Example: predicting COVID deaths  \n\n* During the pandemic, interest in predicting COVID deaths 7, 14, 21, 28 days ahead.\n\n* Can we reasonably [predict COVID deaths 28 days ahead]{.primary} by just using cases today?\n\n* If we let\n\n$$y_{t+28} = \\text{deaths at time } t+28 \\quad\\quad x_{t} = \\text{cases at time } t$$\n  is the following a good model?\n\n  $$\\hat y_{t+28} = \\hat\\beta_0 + \\hat\\beta_1 x_{t}$$\n\n\n## Example: COVID cases and deaths in California \n\n* Let's focus on California.\n\n* Cases seem highly correlated with deaths several weeks later.\n\n::: flex\n::: w-50\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-ca-cases-deaths-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n::: w-50\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(ca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-11-06 08:50:44.00687\n\n# A tibble: 6 × 4\n# Groups:   geo_value [1]\n  geo_value time_value cases deaths\n* <chr>     <date>     <dbl>  <dbl>\n1 ca        2020-04-01  3.17 0.0734\n2 ca        2020-04-02  3.48 0.0835\n3 ca        2020-04-03  3.44 0.0894\n4 ca        2020-04-04  3.05 0.0778\n5 ca        2020-04-05  3.28 0.0876\n6 ca        2020-04-06  3.37 0.0848\n```\n:::\n:::\n\n:::\n:::\n\n\n## Checking correlation\n\n* Let’s split the data into a training and a test set (before/after 2021-04-01).\n\n* On training set: [large correlation]{.primary}\nbetween cases and deaths 28 days ahead (> 0.95).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/correlation-cases-deaths-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* Let's use (base) R to prepare the data and fit \n\n$$\\hat y_{t+28} = \\hat\\beta + \\hat\\beta_0 x_{t}$$\n\n\n## Preparing the data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Add column with cases lagged by k\nca$lagged_cases <- dplyr::lag(ca$cases, n = k)\n\n# Split into train and test (before/after t0_date)\nt0_date <- as.Date('2021-04-01')\ntrain <- ca %>% filter(time_value <= t0_date)\ntest <- ca %>% filter(time_value > t0_date)\n```\n:::\n\n\n* Check if `deaths` is approximately linear in `lagged_cases`:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-lag-cases-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Fitting lagged linear regression in R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg_lagged = lm(deaths ~ lagged_cases, data = train)\ncoef(reg_lagged)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept) lagged_cases \n   0.1171839    0.0112714 \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-linear-fit-1.svg){fig-align='center'}\n:::\n:::\n\n\n# Evaluation\n\n## Error metrics\n\n* Assume we have predictions $\\hat y_{new, t}$ for the unseen observations \n$y_{new,t}$ over times $t = 1, \\dots, N$.\n\n* Four commonly used error metrics are:\n\n  * mean squared error (MSE)\n\n  * mean absolute error (MAE)\n\n  * mean absolute percentage error (MAPE)\n\n  * mean absolute scaled error (MASE)\n\n## Error metrics: MSE and MAE\n\n$$MSE = \\frac{1}{N} \\sum_{t=1}^N (y_{new, t}- \\hat y_{new, t})^2$$\n$$MAE = \\frac{1}{N} \\sum_{t=1}^N |y_{new, t}- \\hat y_{new, t}|$$\n\n* MAE gives less importance to extreme errors than MSE.\n\n* [Drawback]{.primary}: both metrics are scale-dependent, so they are not universally \ninterpretable.\n(For example, if $y$ captures height, MSE and MAE will vary depending on whether we measure in feet or meters.)\n\n## Error metrics: MAPE\n\n* Fixing scale-dependence:\n\n$$MAPE = 100 \\times \\frac{1}{N} \\sum_{t=1}^N \n\\left|\\frac{y_{new, t}- \\hat y_{new, t}}{y_{new, t}}\\right|$$\n\n* [Drawbacks]{.primary}:\n\n  * Erratic behavior when $y_{new, t}$ is close to zero\n\n  * It assumes the unit of measurement has a meaningful zero (e.g. using \nFahrenheit or Celsius to measure temperature will lead to different MAPE)\n\n## Comparing MAE and MAPE\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nThere are situations when MAPE is problematic!\n:::\n\n::: flex\n::: w-70\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/mae-mape-example-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.w-30 .align-end}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n           MAE     MAPE\nyhat1 2.873328 43.14008\nyhat2 5.382247 36.08279\n```\n:::\n:::\n\n\n:::\n:::\n\n\n## Error metrics: MASE\n\n$$MASE = 100 \\times \\frac{\\frac{1}{N} \\sum_{t=1}^N \n|y_{new, t}- \\hat y_{new, t}|}\n{\\frac{1}{N-1} \\sum_{t=2}^N \n|y_{new, t}- y_{new, t-1}|}$$\n\n* [Advantages]{.primary}:\n\n  * is universally interpretable (not scale dependent)\n\n  * avoids the zero-pitfall\n\n* MASE in words: we normalize the error of our forecasts by that of a naive method \nwhich always predicts the last observation.\n\n\n## Comparing MAE, MAPE and MASE\n\n::: flex\n::: w-65\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/mae-mape-mase-example-1.svg){fig-align='left'}\n:::\n:::\n\n:::\n\n::: w-35\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n           MAE     MAPE      MASE\nyhat1 2.873328 43.14008  66.10004\nyhat2 5.382247 36.08279 123.81696\n```\n:::\n:::\n\n\n:::\n:::\n\n## Defining the error metrics in R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMSE <- function(truth, prediction) {\n  mean((truth - prediction)^2)}\n\nMAE <- function(truth, prediction) {\n  mean(abs(truth - prediction))}\n\nMAPE <- function(truth, prediction) {\n  100 * mean(abs(truth - prediction) / truth)}\n\nMASE <- function(truth, prediction) {\n  100 * MAE(truth, prediction) / mean(abs(diff(truth)))}\n```\n:::\n\n\n## Estimating the prediction error\n\n* Given an error metric, we want to estimate the prediction error under that metric. \n\n* This can be accomplished in different ways, using the\n\n  * Training error\n\n  * Split-sample error\n\n  * Time series cross-validation error (using all past data or a trailing window)\n\n\n## Training error\n\n* The easiest but [worst]{.primary} approach to estimate the prediction error is \nto use the training error, i.e. the average error on the training set that was \nused to fit the model.\n\n* The training error is\n\n  * generally too optimistic as an estimate of prediction error\n\n  * [more optimistic the more complex the model!]{.primary}^[More on this when we talk about overfitting.]\n\n\n## Training error\n#### Linear regression of COVID deaths on lagged cases\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting the predictions for the training set\npred_train <- predict(reg_lagged)\n```\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-train-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n               MAE     MASE\ntraining 0.0740177 380.9996\n```\n:::\n:::\n\n\n\n\n## Split-sample error \n\nTo compute the split-sample error  \n\n  1. Split data into training (up to time $t_0$), and test set (after $t_0$)\n\n  1. Fit the model to the training data only\n\n  1. Make predictions for the test set\n\n  1. Compute the selected error metric on the test set only\n\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nSplit-sample estimates of prediction error don't mimic a situation where we \nwould refit the model in the future. \nThey are [pessimistic]{.primary} if the relation between outcome and predictors \nchanges over time.\n:::\n\n## Split-sample error \n\nAssume we want to make $h$-step ahead predictions, i.e. at time $t$ we want to \nmake a forecast for $t+h$. Then, the split-sample MSE is\n\n$$\\text{SplitMSE} = \\frac{1}{n-h-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t_0} - y_{t+h})^2$$\n\nwhere\t$\\hat y_{t+h|t_0}$ indicates a prediction for $y$ at time $t+h$ that was made \nwith a model that was fit on data up to time $t_0$.\n\n\n\n## Split-sample error\n#### Linear regression of COVID deaths on lagged cases\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting the h-step ahead predictions for the test set\nh <- k\ntest_h <- test[-(1:h-1), ] # drop first h-1 rows to avoid data leakage\npred_test <- predict(reg_lagged, newdata = test_h)\n```\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-test-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                   MAE      MASE\ntraining     0.0740177  380.9996\nsplit-sample 0.3116854 2914.4575\n```\n:::\n:::\n\n\n::: {.notes}\nNote that we are overestimating the peak due to the changed relationship \nbetween cases - deaths over time.\n\nTalk about data leakage.\n:::\n\n## Time-series cross-validation (CV) {.smaller}\n#### $h$-step ahead predictions\n\n* If we refit in the future once new data are available, a more \nappropriate way to estimate the prediction error is time-series cross-validation.\n\n* To get $h$-step ahead predictions, we:\n\n  * Fit the model using data up to time $t$\n\n  * Make a prediction for $t+h$ \n\n  * Record the prediction error\n\n* The cross-validation MSE is then\n\n$$CVMSE = \\frac{1}{n-h-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t} - y_{t+h})^2$$\n\nwhere\t$\\hat y_{t+h|t}$ indicates a prediction for $y$ at time $t+h$ that was made \nwith data available up to time $t$.\n\n## Time-series cross-validation (CV) \n#### Linear regression of COVID deaths on lagged cases\n\nGetting the predictions requires slightly more code:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- nrow(ca)                               #length of time series\nh <- k                                      #number of days ahead for which prediction is wanted\npred_all_past <- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to all past data and make h-step ahead prediction\n  reg_all_past = lm(deaths ~ lagged_cases, data = ca, subset = (1:n) <= t) \n  pred_all_past[t+h] = predict(reg_all_past, newdata = data.frame(ca[t+h, ]))\n}\n```\n:::\n\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nWith the current model, we can only predict $k$ days ahead (where $k$ = number of days by which predictor is lagged)!\n:::\n\n\n## Time-series cross-validation (CV)\n#### Linear regression of COVID deaths on lagged cases\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-cv-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                     MAE      MASE\ntraining       0.0740177  380.9996\nsplit-sample   0.3116854 2914.4575\ntime series CV 0.2374931 2212.5992\n```\n:::\n:::\n\n\n::: {.notes}\nSome improvement wrt split-sample, but still overestimating peak. \n:::\n\n## Regression on a trailing window \n\n* So far, to get $h$-step ahead predictions for time $t+h$, we have fitted the \nmodel on all data available up to time $t$. We can instead use a trailing \nwindow, i.e. fit the model on a window of data of length $w$, starting at $t-w$ and ending at $t$.\n\n* [Advantage]{.primary}: if the predictors-outcome relation changes over time,\ntraining the forecaster on a window of recent data can better capture the recent \nrelation which might be more relevant to predict the outcome in the near future.\n\n* Window length [$w$]{.primary} considerations: \n\n  * if $w$ is too [big]{.primary}, the model [can't adapt]{.primary} to the \n  recent predictors-outcome relation \n\n  * if $w$ is too [small]{.primary}, the fitted model may be [too volatile]{.primary} \n  (trained on too little data)\n\n## Trailing window\n#### Linear regression of COVID deaths on lagged cases\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Getting the predictions through CV with trailing window\nw <- 120                                    #trailing window size\nh <- k                                      #number of days ahead for which prediction is wanted\npred_trailing <- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to a trailing window of size w and make h-step ahead prediction\n  reg_trailing = lm(deaths ~ lagged_cases, data = ca, \n                    subset = (1:n) <= t & (1:n) > (t-w)) \n  pred_trailing[t+h] = predict(reg_trailing, newdata = data.frame(ca[t+h, ]))\n}\n```\n:::\n\n\n\n## Time-series CV: all past vs trailing window\n#### Linear regression of COVID deaths on lagged cases\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-cv-predictions-trailing-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                                 MAE      MASE\ntraining                  0.07401770  380.9996\nsplit-sample              0.31168536 2914.4575\ntime series CV            0.23749306 2212.5992\ntime series CV + trailing 0.09932651  925.3734\n```\n:::\n:::\n\n\n::: {.notes}\nA lot of improvement: trailing window allows to adapt to the change in relationship \nbetween cases and deaths over time.\n:::\n\n# ARX models\n\n## Autoregressive exogenous input (ARX) model\n\n* [Idea]{.primary}: predicting the outcome via a linear combination of its lags \nand a set of exogenous (i.e. external) input variables\n\n* Example:\n\n$$\\hat y_{t+h} = \\hat\\phi + \\sum_{i=0}^p \\hat\\phi_i y_{t-i} + \\sum_{j=0}^q \\hat\\beta_j x_{t-j}$$\n\n* [Notice]{.primary}: we don't need to include all contiguous lags, and we could fit e.g.\n\n$$\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}$$\n\n\n## ARX model for COVID deaths\n\n* Let's add lagged deaths as a predictor to our previous forecaster: \n\n$$\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Prepare data: add column with deaths lagged by 28\nca$lagged_deaths <- dplyr::lag(ca$deaths, n = k)\n```\n:::\n\n\n* How does it compare to the previous model in terms of time-series CV?\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Time-Series CV: all past and trailing (ARX model)\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/arx-plot-cv-predictions-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                                 MAE      MASE\ntime series CV            0.16204381 1509.6779\ntime series CV + trailing 0.07872895  733.4767\n```\n:::\n:::\n\n\n::: {.notes}\nErrors under both metrics are smaller than with previous model.\n:::\n\n## Predictions for different $h$\n\n* So far we only focused on COVID death predictions 28 days ahead.\n\n* We will now compare the first model \n\n$$\\hat y_{t+h} = \\hat\\beta + \\hat\\beta_0 x_t$$\n\nto the second model\n\n$$\\hat y_{t+h} = \\hat\\phi + \\hat\\phi_0 y_t + \\hat\\beta_0 x_t$$\n\nfor horizons $h = 7, 14, 21, 28$.\n\n* We will only make forecasts on the $1^{st}$ day of each month, and use a trailing window with $w = 120$.\n\n\n## Predictions for different $h$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nh_vals <- c(7, 14, 21, 28)  #horizons \npred_m1 = pred_m2 <- data.frame(matrix(NA, nrow = 0, ncol = 3))  #initialize df for predictions\ncolnames(pred_m1) = colnames(pred_m2) = c(\"forecast_date\", \"target_date\", \"prediction\")\nw <- 120    #trailing window size\n\nca_lags <- ca %>% select(!c(lagged_cases, lagged_deaths))\n\n# Create lagged predictors \nfor (i in seq_along(h_vals)) {\n  ca_lags[[paste0(\"lagged_deaths_\", h_vals[i])]] <- dplyr::lag(ca_lags$deaths, n = h_vals[i])\n  ca_lags[[paste0(\"lagged_cases_\", h_vals[i])]] <- dplyr::lag(ca_lags$cases, n = h_vals[i])\n}\n\n# Only forecast on 1st day of the months\nforecast_time <- which(ca_lags$time_value >= t0_date & \n                         ca_lags$time_value < ca_lags$time_value[n-max(h_vals)] &\n                         day(ca_lags$time_value) == 1)\n\nfor (t in forecast_time) {\n  for (i in seq_along(h_vals)) {\n    h = h_vals[i]\n    # formulas including h-lagged variables\n    m1_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h))\n    m2_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h, \" + lagged_deaths_\", h))\n    # fit to trailing window of data\n    m1_fit = lm(m1_formula, data = ca_lags, subset = (1:n) <= t & (1:n) > (t-w)) \n    m2_fit = lm(m2_formula, data = ca_lags, subset = (1:n) <= t & (1:n) > (t-w)) \n    # make h-step ahead predictions\n    pred_m1 = rbind(pred_m1, \n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m1_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    pred_m2 = rbind(pred_m2, \n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m2_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    }\n}\n```\n:::\n\n\n## Predictions for different $h$ (Model 1)\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-m1-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n              MAE    MASE\nModel 1 0.1049742 304.007\n```\n:::\n:::\n\n\n## Predictions for different $h$ (Model 2)\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-m2-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n               MAE     MASE\nModel 2 0.04463132 129.2531\n```\n:::\n:::\n\n\n## ARX with more predictors\n\n* The ARX model with only two predictors seems to forecast quite well for different $h$.\n\n* We will try to improve it by adding some more lags. We will fit and compare\n\n$$\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7}$$\n\nand\n\n$$\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}$$\n\n## Predictions using ARX with 2 lags\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-m3-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n              MAE     MASE\nModel 3 0.0495936 143.6239\n```\n:::\n:::\n\n\n## Predictions using ARX with 3 lags\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-m4-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n               MAE     MASE\nModel 4 0.05836984 169.0401\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n# Overfitting and Regularization\n\n## Too many predictors\n\n* What if we try to incorporate past information extensively by fitting a model \nwith a very large number of predictors?\n\n  * The estimated coefficients will be chosen to mimic the observed data very \n  closely on the training set, leading to [small training error]{.primary}\n\n  * The predictive performance on the test set might be very poor, \n  producing [large split-sample and CV error]{.primary}\n\n::: {.callout-important icon=\"false\"}\n## Issue\nOverfitting!\n:::\n\n## ARX model for COVID deaths with many predictors\n\n* When predicting COVID deaths 28 days ahead, we can try to use more past \ninformation by fitting a model that includes the past two months of COVID deaths \nand cases as predictors\n\n$$\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-1} + \\dots + \n\\hat\\phi_{59} y_{t-59} + \n\\hat\\beta_0 x_{t} + \\dots + \\hat\\beta_{t-59} x_{t-59}$$\n\n## Preparing the data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- ca$deaths  #outcome\nlags <- 28:87   #lags used for predictors (deaths and cases)\nh <- 28\n\n# Build predictor matrix with 60 columns\nX <- data.frame(matrix(NA, nrow = length(y), ncol = 2*length(lags)))\ncolnames(X) <- paste('X', 1:ncol(X), sep = '')\n\nfor (j in 1:length(lags)) {\n  # first 60 columns contain deaths lagged by 28, 29, ..., 87\n  X[, j] = dplyr::lag(ca$deaths, lags[j])\n  # last 60 columns contain cases lagged by 28, 29, ..., 87\n  X[, length(lags) + j] = dplyr::lag(ca$cases, lags[j])\n}\n```\n:::\n\n\n## Fitting the ARX model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Train/test split\ny_train <- y[1:t0]\nX_train <- X[1:t0, ]\ny_test <- y[(t0+h):length(y)]\nX_test <- X[(t0+h):length(y), ]\n\n# Fitting the ARX model\nreg = lm(y_train ~ ., data = X_train)\ncoef(reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)            X1            X2            X3            X4 \n 0.0775116437 -0.5593818462  0.7979659233 -0.4175920623 -0.2780809101 \n           X5            X6            X7            X8            X9 \n 0.3031742752 -0.0809244359  0.2548033065 -0.4860940499  0.1322930625 \n          X10           X11           X12           X13           X14 \n-0.2352041391 -0.1327834155  0.2155747658 -0.3380379255  0.4239820677 \n          X15           X16           X17           X18           X19 \n-0.2012041267 -0.3367851572 -0.2889890062  0.5328712849  0.5388650654 \n          X20           X21           X22           X23           X24 \n-0.3065835983  0.0724595436 -0.0168042757 -0.1171985635  0.2046513639 \n          X25           X26           X27           X28           X29 \n 0.1810480128  0.1213875691  0.0230516587  0.0196208441  0.0397085778 \n          X30           X31           X32           X33           X34 \n-0.3884271784  0.2088690345  0.1248242133  0.0706165553 -0.4882035875 \n          X35           X36           X37           X38           X39 \n 0.3609708771 -0.3169047917  0.4216666798  0.1891753615 -0.1106475626 \n          X40           X41           X42           X43           X44 \n 0.1498605000 -0.0692090064  0.1336287081 -0.1875462008 -0.2449003857 \n          X45           X46           X47           X48           X49 \n-0.0001337325 -0.5738823399  0.0695056705 -0.2460256934  1.0173509442 \n          X50           X51           X52           X53           X54 \n-0.1853591480 -0.5428279059  0.2678983608 -0.6935743948  0.3829408389 \n          X55           X56           X57           X58           X59 \n 0.1088530454  0.9466159031 -0.5618240450 -0.4660113206  0.6102916420 \n          X60           X61           X62           X63           X64 \n-0.2859449807  0.0283237204 -0.0099051792 -0.0070208086  0.0021192306 \n          X65           X66           X67           X68           X69 \n-0.0047341417 -0.0111532015  0.0038542335  0.0184565802 -0.0060684485 \n          X70           X71           X72           X73           X74 \n-0.0005801373  0.0048246180 -0.0061516656 -0.0066597399  0.0039021597 \n          X75           X76           X77           X78           X79 \n 0.0126296042 -0.0080708988 -0.0027091539  0.0052517573 -0.0052000323 \n          X80           X81           X82           X83           X84 \n 0.0029961750  0.0013593227  0.0083628716 -0.0063778828 -0.0018882435 \n          X85           X86           X87           X88           X89 \n-0.0097221295  0.0003314155 -0.0013911110  0.0066935430  0.0107961484 \n          X90           X91           X92           X93           X94 \n-0.0052473765 -0.0057177036  0.0023462634 -0.0112827594  0.0008517257 \n          X95           X96           X97           X98           X99 \n-0.0004388072  0.0231342674 -0.0056794349 -0.0046693142 -0.0061536587 \n         X100          X101          X102          X103          X104 \n-0.0094880392  0.0071605921  0.0021423255  0.0108738290 -0.0015420116 \n         X105          X106          X107          X108          X109 \n 0.0015155025  0.0022482275 -0.0148197121  0.0129113709  0.0009150566 \n         X110          X111          X112          X113          X114 \n 0.0021338029 -0.0019029077 -0.0040171812 -0.0025674957 -0.0069761237 \n         X115          X116          X117          X118          X119 \n 0.0226899068 -0.0022271856 -0.0060651747  0.0071536700 -0.0016426930 \n         X120 \n-0.0127949778 \n```\n:::\n:::\n\n\n## Predictions on training and test set \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/overfit-plot-train-test-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                    MAE      MASE\ntraining     0.04230235  189.9364\nsplit-sample 0.41684935 3883.5685\n```\n:::\n:::\n\n\n::: {.callout-important icon=\"false\"}\n## Note\n\nSome predictions are negative, which doesn't make sense for count data, so let's truncate them at 0.\n:::\n\n## Truncated predictions on training and test set \n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/overfit-plot-train-test-trunc-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                             MAE    MASE\nsplit-sample truncated 0.3978198 3706.28\n```\n:::\n:::\n\n\n## Regularization\n\n* If we want to consider a large number of predictors, \nhow can we avoid overfitting?\n\n* [Idea]{.primary}: introduce a regularization parameter $\\lambda$ that [shrinks or sets]{.primary} some \nof the estimated coefficients to zero, i.e. some predictors are estimated to \nhave limited or no predictive power\n\n* Most common regularization methods\n\n  * [Ridge]{.primary}: shrinks coefficients to zero\n  \n  * [Lasso]{.primary}: sets some coefficients to zero\n\n\n\n## ARX + ridge/lasso for COVID deaths\n\nLet's consider a model with lagged cases and deaths: 7 lags for each, spaced by one week.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nh <- 28\nlags <- h + 7*(0:6)   #lags used for predictors (deaths and cases)\n\n# Build predictor matrix \nX <- data.frame(matrix(NA, nrow = length(y), ncol = 2*length(lags)))\ncolnames(X) <- paste('X', 1:ncol(X), sep = '')\n\nfor (j in 1:length(lags)) {\n  # lagged deaths \n  X[, j] = dplyr::lag(ca$deaths, lags[j])\n  # lagged cases\n  X[, length(lags) + j] = dplyr::lag(ca$cases, lags[j])\n}\n\n# Train/test split\ny_train <- y[1:t0]\nX_train <- X[1:t0, ]\ny_test <- y[(t0+h):length(y)]\nX_test <- X[(t0+h):length(y), ]\n```\n:::\n\n\n\n## Fit ARX + ridge/lasso for COVID deaths\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(glmnet) # Implements ridge and lasso\n\n# We'll need to omit NA values explicitly, as otherwise glmnet will complain\nna_obs <- 1:max(lags)\nX_train <- X_train[-na_obs, ]\ny_train <- y_train[-na_obs]\n\n# Ridge regression: set alpha = 0, lambda sequence will be chosen automatically\nridge <- glmnet(X_train, y_train, alpha = 0)\nbeta_ridge <- coef(ridge)       # matrix of estimated coefficients \nlambda_ridge <- ridge$lambda    # sequence of lambdas used to fit ridge \n\n# Lasso regression: set alpha = 1, lambda sequence will be chosen automatically\nlasso <- glmnet(X_train, y_train, alpha = 1)\nbeta_lasso <- coef(lasso)       # matrix of estimated coefficients \nlambda_lasso <- lasso$lambda    # sequence of lambdas used to fit lasso \n\ndim(beta_lasso)      # One row per coefficient, one column per lambda value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15 92\n```\n:::\n:::\n\n\n\n## Predictions on test set and best $\\lambda$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Predict values for second half of the time series\nyhat_ridge <- predict(ridge, newx = as.matrix(X_test))\nyhat_lasso <- predict(lasso, newx = as.matrix(X_test))\n\n# Compute MAE \nmae_ridge <- colMeans(abs(yhat_ridge - y_test))\nmae_lasso <- colMeans(abs(yhat_lasso - y_test))\n\n# Select index of lambda vector which gives lowest MAE\nmin_ridge <- which.min(mae_ridge)\nmin_lasso <- which.min(mae_lasso)\npaste('Best MAE ridge:', round(min(mae_ridge), 3),\n      '; Best MAE lasso:', round(min(mae_lasso), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Best MAE ridge: 0.292 ; Best MAE lasso: 0.295\"\n```\n:::\n\n```{.r .cell-code}\n# Get predictions for train and test sets\npred_train_ridge <- predict(ridge, newx = as.matrix(X_train))[, min_ridge] \npred_test_ridge <- yhat_ridge[, min_ridge]\npred_train_lasso <- predict(lasso, newx = as.matrix(X_train))[, min_lasso] \npred_test_lasso <- yhat_lasso[, min_lasso]\n```\n:::\n\n\n## Estimated coefficients: shrinkage vs sparsity\n\n::: flex\n::: w-50\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                    ridge       lasso\n(Intercept)  0.2978514823 0.124915807\nX1           0.0433892112 0.069300307\nX2           0.0292256563 0.000000000\nX3           0.0179647838 0.000000000\nX4           0.0083918680 0.000000000\nX5          -0.0015177021 0.000000000\nX6          -0.0125786976 0.000000000\nX7          -0.0191532557 0.000000000\nX8           0.0010586265 0.008320956\nX9           0.0009417383 0.000000000\nX10          0.0008208805 0.001690258\nX11          0.0006535137 0.000000000\nX12          0.0004488789 0.000000000\nX13          0.0002816751 0.000000000\nX14          0.0001839354 0.000000000\n```\n:::\n:::\n\n\n:::\n\n::: w-50\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-ridge-lasso-coeff-1.svg){fig-align='center' height=600px}\n:::\n:::\n\n\n:::\n:::\n\n## Predictions: ARX + ridge/lasso (train and test set)\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/shrinkage-sparsity-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                         MAE      MASE\nridge training     0.1975073  924.4584\nridge split-sample 0.2923452 2723.6281\nlasso training     0.0790951  370.2149\nlasso split-sample 0.2945295 2743.9784\n```\n:::\n:::\n\n\n## Time-series CV for ARX + ridge/lasso (trailing)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nh <- 28  # number of days ahead \nw <- 120 # window length\n\n# Initialize matrices for predictions (one column per lambda value)\nyhat_ridge <- rep(NA, length = n) \nyhat_lasso <- rep(NA, length = n) \n\nfor (t in t0:(n-h)) {\n  # Choose best lambda\n  cv_inds = max(lags) < 1:n & 1:n <= t-w\n  ridge_cv = cv.glmnet(as.matrix(X[cv_inds, ]), y[cv_inds], alpha = 0)\n  lasso_cv = cv.glmnet(as.matrix(X[cv_inds, ]), y[cv_inds], alpha = 1)\n  best_lambda_ridge = ridge_cv$lambda.min\n  best_lambda_lasso = lasso_cv$lambda.min\n  # Indices of data within window\n  inds = t-w < 1:n & 1:n <= t\n  # Fit ARX + ridge/lasso\n  ridge_trail = glmnet(X[inds, ], y[inds], alpha = 0, lambda = best_lambda_ridge)\n  lasso_trail = glmnet(X[inds, ], y[inds], alpha = 1, lambda = best_lambda_lasso)\n  # Predict\n  yhat_ridge[t+h] = predict(ridge_trail, newx = as.matrix(X[(t+h), ]))\n  yhat_lasso[t+h] = predict(lasso_trail, newx = as.matrix(X[(t+h), ]))\n}\n```\n:::\n\n\n\n## Predictions: time-series CV for ARX + ridge/lasso (trailing)\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-regularization-cv-1.svg){fig-align='left'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                          MAE     MASE\nridge CV + trailing 0.1068111  995.103\nlasso CV + trailing 0.1328789 1237.964\n```\n:::\n:::\n\n\n# Prediction Intervals\n\n## Point predictions vs intervals \n\n* So far, we have only considered [point predictions]{.primary}, i.e. \nwe have fitted models \nto provide our [best guess on the outcome]{.primary} at time $t+h$. \n\n::: {.callout-important icon=\"false\"}\n## \nWhat if we want to provide a [measure of uncertainty]{.primary} around the point \nprediction or a [likely range of values]{.primary} for the outcome at time $t+h$?\n\n:::\n\n* For each target time $t+h$, we can construct [prediction intervals]{.primary}, i.e. provide \nranges of values that are expected to cover the true outcome value a fixed \nfraction of times.\n\n## Prediction intervals for `lm` fits\n\n* To get prediction intervals for the models we previously fitted, \nwe only need to tweak our call to `predict` by adding as an input: \n\n  `interval = \"prediction\", level = p`\n\n  where $p \\in (0, 1)$ is the desired coverage.\n\n* The output from `predict` will then be a matrix with \n\n  * first column a [point estimate]{.primary}\n  \n  * second column the [lower limit]{.primary} of the interval\n  \n  * third column the [upper limit]{.primary} of the interval\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Prediction intervals for ARX (CV, trailing window)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Initialize matrices to store predictions \n# 3 columns: point estimate, lower limit, and upper limit\npred_trailing <- matrix(NA, nrow = n, ncol = 3)\ncolnames(pred_trailing) <- c('prediction', 'lower', 'upper')\n\nfor (t in t0:(n-h)) {\n  # Fit ARX and predict\n  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, \n                    subset = (1:n) <= t & (1:n) > (t-w)) \n  pred_trailing[t+h, ] = predict(arx_trailing, newdata = data.frame(ca[t+h, ]),\n                                 interval = \"prediction\", level = 0.95)\n}\n```\n:::\n\n\n## Prediction intervals for ARX (CV, trailing window)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot arx-intervals-cv-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                 MAE     MASE\nlm.trailing 0.104397 972.6125\n```\n:::\n:::\n\n\n## Quantile regression\n\n* So far we only considered different ways to apply linear regression.\n\n* Quantile regression is a different estimation method, and it directly targets conditional \nquantiles of the outcome over time.\n\n::: {.callout-note}\n## Definition\nConditional quantile = value below which a given percentage (e.g. 25%, 50%, \n75%) of observations fall, given specific values of the predictor variables. \n:::\n\n* [Advantage]{.primary}: it provides a more complete picture of the outcome distribution.\n\n## ARX model for COVID deaths via quantile regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#install.packages(\"quantreg\")\nlibrary(quantreg)  #library to perform quantile regression\n\n# Set quantiles of interest: we will focus on 2.5%, 50% (i.e. median), and 97.5% quantiles\nquantiles <- c(0.025, 0.5, 0.975)  \n\n# Fit quantile regression to training set\nq_reg <- rq(deaths ~ lagged_deaths + lagged_cases, data = train, tau = quantiles)\n\n# Estimated coefficients\ncoef(q_reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               tau= 0.025 tau= 0.500 tau= 0.975\n(Intercept)   0.009351562 0.06896010 0.12257658\nlagged_deaths 0.229011485 0.19821254 0.28469573\nlagged_cases  0.007439881 0.01022547 0.01265167\n```\n:::\n\n```{.r .cell-code}\n# Sort estimated coefficients \ncoefs_sorted <- t(apply(coef(q_reg), 1, sort))\ncolnames(coefs_sorted) <- colnames(coef(q_reg))\ncoefs_sorted\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               tau= 0.025 tau= 0.500 tau= 0.975\n(Intercept)   0.009351562 0.06896010 0.12257658\nlagged_deaths 0.198212543 0.22901149 0.28469573\nlagged_cases  0.007439881 0.01022547 0.01265167\n```\n:::\n\n```{.r .cell-code}\nq_reg$coefficients <- coefs_sorted\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Predictions via quantile regression (CV, trailing)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Initialize matrix to store predictions \n# 3 columns: lower limit, median, and upper limit\npred_trailing <- matrix(NA, nrow = n, ncol = 3)\ncolnames(pred_trailing) <- c('lower', 'median', 'upper')\n\nfor (t in t0:(n-h)) {\n  # Fit quantile regression\n  rq_trailing = rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles,\n                   data = ca, subset = (1:n) <= t & (1:n) > (t-w)) \n  # Sort estimated coefficients \n  coefs_sorted <- t(apply(coef(rq_trailing), 1, sort))\n  rq_trailing$coefficients <- coefs_sorted\n  # Predict\n  pred_trailing[t+h, ] = predict(rq_trailing, newdata = data.frame(ca[t+h, ]))\n}\n```\n:::\n\n\n## Predictions via quantile regression (CV, trailing)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/q-reg-plot-cv-predictions-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                  MAE     MASE\nrq.trailing 0.1244401 1159.343\n```\n:::\n:::\n\n\n## Actual Coverage\n\n* We would expect the ARX model fitted via `lm` and via `rq` to cover the truth\nabout 95\\% of the times. Is this actually true in practice?\n\n* The actual coverage of each predictive interval is lower:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n         lm.trailing rq.trailing\nCoverage   0.8294118   0.8117647\n```\n:::\n:::\n\n\n\n## Evaluation\n\n* Prediction intervals are “good” if they \n\n  * cover the truth most of the time\n  \n  * are not too wide\n  \n* Error metric that captures both desiderata: [Weighted Interval Score (WIS)]{.primary}\n\n* $F$ = forecast composed of predicted quantiles $q_{\\tau}$ for the set \nof quantile levels $\\tau$. The WIS for target variable $Y$ is represented as \n([McDonald et al., 2021](https://www.pnas.org/doi/full/10.1073/pnas.2111453118)):\n\n$$WIS(F, Y) = 2\\sum_{\\tau} \\phi_{\\tau} (Y - q_{\\tau})$$\n\nwhere $\\phi_{\\tau}(x) = \\tau |x|$ for $x \\geq 0$ and \n$\\phi_{\\tau}(x) = (1-\\tau) |x|$ for $x < 0$.\n\n## Computing the WIS \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nWIS <- function(truth, estimates, quantiles) {\n  2 * sum(pmax(\n    quantiles * (truth - estimates),\n    (1 - quantiles) * (estimates - truth),\n    na.rm = TRUE\n  ))\n}\n```\n:::\n\n\n::: {.callout-important icon=\"false\"}\n## Note\nWIS tends to [prioritize sharpness]{.primary} (how wide the interval is) relative to \ncoverage (if the interval contains the truth).\n:::\n\n## WIS for ARX fitted via `lm` and `rq`\n\n* The lowest mean WIS is attained by quantile regression. \n\n* Notice: this method has coverage below 95\\% but is still preferred under WIS \nbecause its intervals are narrower than for linear regression.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n  Mean WIS lm Mean WIS rq\n1   0.1335326   0.1056215\n```\n:::\n:::\n\n\n\n# Forecasting with Versioned Data\n\n## Versioned data\n\n* In our forecasting examples, we have assumed the data are never revised \n(or have simply ignored revisions, and used data `as_of` today)\n\n::: {.callout-important icon=\"false\"}\n## \nHow can we train forecasters when dealing with versioned data?\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_archive\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-04-01 / 2023-03-09\nℹ First/last version with update: 2020-04-02 / 2023-03-10\nℹ Versions end: 2023-03-10\nℹ A preview of the table (148820 rows x 5 columns):\n        geo_value time_value    version case_rate death_rate\n     1:        ak 2020-04-01 2020-04-02  1.797489  0.0000000\n     2:        ak 2020-04-01 2020-05-07  1.777061  0.0000000\n     3:        ak 2020-04-01 2020-10-28  1.106147  0.0000000\n     4:        ak 2020-04-01 2020-10-29  1.797489  0.0000000\n     5:        ak 2020-04-01 2020-10-30  1.797489  0.0000000\n    ---                                                     \n148816:        wy 2023-03-05 2023-03-06  0.000000  0.0000000\n148817:        wy 2023-03-06 2023-03-07  0.000000  0.0000000\n148818:        wy 2023-03-07 2023-03-08 38.809743  0.3434491\n148819:        wy 2023-03-08 2023-03-09  0.000000  0.0000000\n148820:        wy 2023-03-09 2023-03-10  0.000000  0.0000000\n```\n:::\n:::\n\n\n## Version-aware forecasting\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# initialize dataframe for predictions\n# 5 columns: forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_trailing <- data.frame(matrix(NA, ncol = 5, nrow = 0))\ncolnames(pred_trailing) <- c(\"forecast_date\", \"target_date\", 'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw <- 120         #trailing window size\nh <- 28          #number of days ahead\n\n# dates when predictions are made (set to be 1 month apart)\nfc_time_values <- seq(from = t0_date, to = as.Date(\"2023-02-01\"), by = \"1 month\")\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data <- epix_as_of(ca_archive, max_version = as.Date(fc_date))\n  # create lagged predictors\n  data$lagged_deaths <- dplyr::lag(data$deaths, h) \n  data$lagged_cases <- dplyr::lag(data$cases, h)\n  # perform quantile regression\n  rq_trailing <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, \n                    # only consider window of data\n                    data = data %>% filter(time_value > (max(time_value) - w))) \n  # sort estimated coefficients \n  coefs_sorted <- t(apply(coef(rq_trailing), 1, sort))\n  colnames(coefs_sorted) <- c('tau..0.025', 'tau..0.500', 'tau..0.975')\n  rq_trailing$coefficients <- coefs_sorted\n  # construct data.frame with the right predictors for the target date\n  predictors <- data.frame(lagged_deaths = tail(data$deaths, 1), \n                           lagged_cases = tail(data$cases, 1))\n  # make predictions for target date and add them to matrix of predictions\n  pred_trailing <- rbind(pred_trailing, \n                         data.frame('forecast_date' = max(data$time_value),\n                                    'target_date' = max(data$time_value) + h, \n                                    predict(rq_trailing, newdata = predictors)))\n}\n```\n:::\n\n\n\n## Version-aware predictions (CV, trailing)\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-versioned-cv-trailing-1.svg){fig-align='center'}\n:::\n:::\n\n\n# Modeling Multiple Time Series\n\n## Using geo information\n\n* Assume we observe data over time from [multiple locations]{.primary}\n(e.g. states or counties).\n\n* We could\n\n  * Estimate coefficients [separately]{.primary} for each location (as we have done so far).\n  \n  * Fit one model using all locations together at each time point ([geo-pooling]{.primary}). \nEstimated coefficients will not be location specific.\n\n  * Estimate coefficients separately for each location, but include predictors capturing \naverages across locations ([partial geo-pooling]{.primary}).\n\n\n\n## Geo-pooling (trailing window)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nusa_archive <- data_archive$DT %>% \n  as_epi_archive()\n\n# initialize dataframe for predictions\n# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_trailing <- data.frame(matrix(NA, ncol = 6, nrow = 0))\ncolnames(pred_trailing) <- c('geo_value', 'forecast_date', 'target_date',\n                             'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw <- 120         #trailing window size\nh <- 28          #number of days ahead\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data <- epix_as_of(usa_archive, max_version = as.Date(fc_date))\n  \n  # create lagged predictors for each state \n  data <- data %>%\n    arrange(geo_value, time_value) %>%  \n    group_by(geo_value) %>%\n    mutate(lagged_deaths = dplyr::lag(deaths, h),\n           lagged_cases = dplyr::lag(cases, h)) %>%\n    ungroup()\n  \n  # perform quantile regression\n  rq_trailing <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, \n                    # only consider window of data\n                    data = data %>% filter(time_value > (max(time_value) - w))) \n  # sort estimated coefficients \n  coefs_sorted <- t(apply(coef(rq_trailing), 1, sort))\n  colnames(coefs_sorted) <- c('tau..0.025', 'tau..0.500', 'tau..0.975')\n  rq_trailing$coefficients <- coefs_sorted\n  \n  # construct dataframe with the right predictors for the target date\n  new_lagged_deaths <- data %>% \n    filter(time_value == max(time_value)) %>%\n    select(geo_value, deaths)\n  \n  new_lagged_cases <- data %>% \n    filter(time_value == max(time_value)) %>%\n    select(geo_value, cases)\n  \n  predictors <- new_lagged_deaths %>%\n    inner_join(new_lagged_cases, join_by(geo_value)) %>%\n    rename(lagged_deaths = deaths,\n           lagged_cases = cases)\n  \n  # make predictions for target date and add them to matrix of predictions\n  pred_trailing <- rbind(pred_trailing, \n                         data.frame(\n                           'geo_value' = predictors$geo_value,\n                           'forecast_date' = max(data$time_value),\n                           'target_date' = max(data$time_value) + h, \n                           predict(rq_trailing, newdata = predictors)))\n}\n\n# geo-pooled predictions for California\npred_ca <- pred_trailing %>%\n  filter(geo_value == 'ca') %>%\n  rename(median = `tau..0.500`,\n         lower = `tau..0.025`,\n         upper = `tau..0.975`) %>%\n  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%\n  arrange(target_date)\n```\n:::\n\n\n## Geo-pooled predictions for California\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-geo-pooling-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Partial geo-pooling (trailing window)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# initialize dataframe for predictions\n# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_trailing <- data.frame(matrix(NA, ncol = 6, nrow = 0))\ncolnames(pred_trailing) <- c('geo_value', 'forecast_date', 'target_date',\n                             'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw <- 120         #trailing window size\nh <- 28          #number of days ahead\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data <- epix_as_of(usa_archive, max_version = as.Date(fc_date))\n  \n  # create lagged predictors \n  data <- data %>%\n    arrange(geo_value, time_value) %>%  \n    group_by(geo_value) %>%\n    mutate(lagged_deaths = dplyr::lag(deaths, h),\n           lagged_cases = dplyr::lag(cases, h)) %>%\n    ungroup() %>%\n    group_by(time_value) %>%\n    mutate(avg_lagged_deaths = mean(lagged_deaths, na.rm = T),\n           avg_lagged_cases = mean(lagged_cases, na.rm = T)) %>%\n    ungroup() \n  \n  # perform quantile regression\n  rq_trailing <- rq(deaths ~ lagged_deaths + lagged_cases + avg_lagged_deaths +\n                      avg_lagged_cases, tau = quantiles, \n                    data = (data %>% filter(geo_value == 'ca'))) \n  \n  # sort estimated coefficients \n  coefs_sorted <- t(apply(coef(rq_trailing), 1, sort))\n  colnames(coefs_sorted) <- c('tau..0.025', 'tau..0.500', 'tau..0.975')\n  rq_trailing$coefficients <- coefs_sorted\n  \n  # construct data.frame with the right predictors for the target date\n  new_lagged_deaths <- data %>% \n    filter(time_value == max(time_value)) %>%\n    select(geo_value, deaths) %>%\n    mutate(avg_lagged_deaths = mean(deaths, na.rm = T)) %>%\n    filter(geo_value == 'ca')\n  \n  new_lagged_cases <- data %>% \n    filter(time_value == max(time_value)) %>%\n    select(geo_value, cases) %>%\n    mutate(avg_lagged_cases = mean(cases, na.rm = T)) %>%\n    filter(geo_value == 'ca')\n  \n  predictors <- new_lagged_deaths %>%\n    inner_join(new_lagged_cases, join_by(geo_value)) %>%\n    rename(lagged_deaths = deaths,\n           lagged_cases = cases)\n  \n  # make predictions for target date and add them to matrix of predictions\n  pred_trailing <- rbind(pred_trailing, \n                         data.frame(\n                           'geo_value' = predictors$geo_value,\n                           'forecast_date' = max(data$time_value),\n                           'target_date' = max(data$time_value) + h, \n                           predict(rq_trailing, newdata = predictors)))\n}\n\n# partially geo-pooled predictions for California\npred_ca <- pred_trailing %>%\n  filter(geo_value == 'ca') %>%\n  rename(median = `tau..0.500`,\n         lower = `tau..0.025`,\n         upper = `tau..0.975`) %>%\n  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%\n  arrange(target_date)\n```\n:::\n\n\n## Partially geo-pooled predictions for California\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2-morning_files/figure-revealjs/plot-partial-geo-pooling-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Final slide {.smaller}\n\n### Thanks:\n\n\n\n\n\n- The whole [CMU Delphi Team](https://delphi.cmu.edu/about/team/) (across many institutions)\n- Optum/UnitedHealthcare, Change Healthcare.\n- Google, Facebook, Amazon Web Services.\n- Quidel, SafeGraph, Qualtrics.\n- Centers for Disease Control and Prevention.\n- Council of State and Territorial Epidemiologists\n\n\n::: {layout-row=1 fig-align=\"center\"}\n![](gfx/delphi.jpg){height=\"100px\"}\n![](gfx/berkeley.jpg){height=\"100px\"}\n![](gfx/cmu.jpg){height=\"100px\"}\n![](gfx/ubc.jpg){width=\"250px\"}\n![](gfx/stanford.jpg){width=\"250px\"}\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}