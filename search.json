[
  {
    "objectID": "slides/day2-morning.html#section",
    "href": "slides/day2-morning.html#section",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Forecasting and Time-Series Models",
    "text": "Forecasting and Time-Series Models\n\n\n\n\nVenue – dd Somemonth yyyy"
  },
  {
    "objectID": "slides/day2-morning.html#outline",
    "href": "slides/day2-morning.html#outline",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Outline",
    "text": "Outline\n\nLinear Regression for Time Series Data\nEvaluation Methods\nARX Models\nOverfitting and Regularization\nPrediction Intervals\nForecasting with Versioned Data\nModeling Multiple Time Series"
  },
  {
    "objectID": "slides/day2-morning.html#basics-of-linear-regression",
    "href": "slides/day2-morning.html#basics-of-linear-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Basics of linear regression",
    "text": "Basics of linear regression\n\nAssume we observe a predictor \\(x_i\\) and an outcome \\(y_i\\) for \\(i = 1, \\dots, n\\).\nLinear regression seeks coefficients \\(\\beta_0\\) and \\(\\beta_1\\) such that\n\n\\[y_i \\approx \\beta_0 + \\beta_1 x_i\\]\nis a good approximation for every \\(i = 1, \\dots, n\\).\n\nIn R, the coefficients are found by running lm(y ~ x), where y is the vector of responses and x the vector of predictors."
  },
  {
    "objectID": "slides/day2-morning.html#multiple-linear-regression",
    "href": "slides/day2-morning.html#multiple-linear-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nGiven \\(p\\) different predictors, we seek \\((p+1)\\) coefficients such that\n\n\\[y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\\] is a good approximation for every \\(i = 1, \\dots, n\\)."
  },
  {
    "objectID": "slides/day2-morning.html#linear-regression-with-lagged-predictor",
    "href": "slides/day2-morning.html#linear-regression-with-lagged-predictor",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Linear regression with lagged predictor",
    "text": "Linear regression with lagged predictor\n\nIn time series, outcomes and predictors are usually indexed by time \\(t\\).\nGoal: predicting future \\(y\\), given present \\(x\\).\nModel: linear regression with lagged predictor\n\n\\[\\hat y_t = \\hat \\beta + \\hat \\beta_0 x_{t-k}\\]\ni.e. regress the outcome \\(y\\) at time \\(t\\) on the predictor \\(x\\) at time \\(t-k\\).\n\nEquivalent way to write the model:\n\n\\[\\hat y_{t+k} = \\hat \\beta + \\hat \\beta_0 x_t\\]"
  },
  {
    "objectID": "slides/day2-morning.html#example-predicting-covid-deaths",
    "href": "slides/day2-morning.html#example-predicting-covid-deaths",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example: predicting COVID deaths",
    "text": "Example: predicting COVID deaths\n\nDuring the pandemic, interest in predicting COVID deaths 7, 14, 21, 28 days ahead.\nCan we reasonably predict COVID deaths 28 days ahead by just using cases today?\nIf we let\n\n\\[y_{t+28} = \\text{deaths at time } t+28 \\quad\\quad x_{t} = \\text{cases at time } t\\] is the following a good model?\n\\[\\hat y_{t+28} = \\hat\\beta_0 + \\hat\\beta_1 x_{t}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#example-covid-cases-and-deaths-in-california",
    "href": "slides/day2-morning.html#example-covid-cases-and-deaths-in-california",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example: COVID cases and deaths in California",
    "text": "Example: COVID cases and deaths in California\n\nLet’s focus on California.\nCases seem highly correlated with deaths several weeks later.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhead(ca)\n\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-11-06 08:50:44.00687\n\n# A tibble: 6 × 4\n# Groups:   geo_value [1]\n  geo_value time_value cases deaths\n* &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 ca        2020-04-01  3.17 0.0734\n2 ca        2020-04-02  3.48 0.0835\n3 ca        2020-04-03  3.44 0.0894\n4 ca        2020-04-04  3.05 0.0778\n5 ca        2020-04-05  3.28 0.0876\n6 ca        2020-04-06  3.37 0.0848"
  },
  {
    "objectID": "slides/day2-morning.html#checking-correlation",
    "href": "slides/day2-morning.html#checking-correlation",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Checking correlation",
    "text": "Checking correlation\n\nLet’s split the data into a training and a test set (before/after 2021-04-01).\nOn training set: large correlation between cases and deaths 28 days ahead (&gt; 0.95).\n\n\n\nLet’s use (base) R to prepare the data and fit\n\n\\[\\hat y_{t+28} = \\hat\\beta + \\hat\\beta_0 x_{t}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#preparing-the-data",
    "href": "slides/day2-morning.html#preparing-the-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Preparing the data",
    "text": "Preparing the data\n\n# Add column with cases lagged by k\nca$lagged_cases &lt;- dplyr::lag(ca$cases, n = k)\n\n# Split into train and test (before/after t0_date)\nt0_date &lt;- as.Date('2021-04-01')\ntrain &lt;- ca %&gt;% filter(time_value &lt;= t0_date)\ntest &lt;- ca %&gt;% filter(time_value &gt; t0_date)\n\n\nCheck if deaths is approximately linear in lagged_cases:"
  },
  {
    "objectID": "slides/day2-morning.html#fitting-lagged-linear-regression-in-r",
    "href": "slides/day2-morning.html#fitting-lagged-linear-regression-in-r",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fitting lagged linear regression in R",
    "text": "Fitting lagged linear regression in R\n\nreg_lagged = lm(deaths ~ lagged_cases, data = train)\ncoef(reg_lagged)\n\n (Intercept) lagged_cases \n   0.1171839    0.0112714"
  },
  {
    "objectID": "slides/day2-morning.html#error-metrics",
    "href": "slides/day2-morning.html#error-metrics",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Error metrics",
    "text": "Error metrics\n\nAssume we have predictions \\(\\hat y_{new, t}\\) for the unseen observations \\(y_{new,t}\\) over times \\(t = 1, \\dots, N\\).\nFour commonly used error metrics are:\n\nmean squared error (MSE)\nmean absolute error (MAE)\nmean absolute percentage error (MAPE)\nmean absolute scaled error (MASE)"
  },
  {
    "objectID": "slides/day2-morning.html#error-metrics-mse-and-mae",
    "href": "slides/day2-morning.html#error-metrics-mse-and-mae",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Error metrics: MSE and MAE",
    "text": "Error metrics: MSE and MAE\n\\[MSE = \\frac{1}{N} \\sum_{t=1}^N (y_{new, t}- \\hat y_{new, t})^2\\] \\[MAE = \\frac{1}{N} \\sum_{t=1}^N |y_{new, t}- \\hat y_{new, t}|\\]\n\nMAE gives less importance to extreme errors than MSE.\nDrawback: both metrics are scale-dependent, so they are not universally interpretable. (For example, if \\(y\\) captures height, MSE and MAE will vary depending on whether we measure in feet or meters.)"
  },
  {
    "objectID": "slides/day2-morning.html#error-metrics-mape",
    "href": "slides/day2-morning.html#error-metrics-mape",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Error metrics: MAPE",
    "text": "Error metrics: MAPE\n\nFixing scale-dependence:\n\n\\[MAPE = 100 \\times \\frac{1}{N} \\sum_{t=1}^N\n\\left|\\frac{y_{new, t}- \\hat y_{new, t}}{y_{new, t}}\\right|\\]\n\nDrawbacks:\n\nErratic behavior when \\(y_{new, t}\\) is close to zero\nIt assumes the unit of measurement has a meaningful zero (e.g. using Fahrenheit or Celsius to measure temperature will lead to different MAPE)"
  },
  {
    "objectID": "slides/day2-morning.html#comparing-mae-and-mape",
    "href": "slides/day2-morning.html#comparing-mae-and-mape",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Comparing MAE and MAPE",
    "text": "Comparing MAE and MAPE\n\n\n\nNote\n\n\nThere are situations when MAPE is problematic!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n           MAE     MAPE\nyhat1 2.873328 43.14008\nyhat2 5.382247 36.08279"
  },
  {
    "objectID": "slides/day2-morning.html#error-metrics-mase",
    "href": "slides/day2-morning.html#error-metrics-mase",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Error metrics: MASE",
    "text": "Error metrics: MASE\n\\[MASE = 100 \\times \\frac{\\frac{1}{N} \\sum_{t=1}^N\n|y_{new, t}- \\hat y_{new, t}|}\n{\\frac{1}{N-1} \\sum_{t=2}^N\n|y_{new, t}- y_{new, t-1}|}\\]\n\nAdvantages:\n\nis universally interpretable (not scale dependent)\navoids the zero-pitfall\n\nMASE in words: we normalize the error of our forecasts by that of a naive method which always predicts the last observation."
  },
  {
    "objectID": "slides/day2-morning.html#comparing-mae-mape-and-mase",
    "href": "slides/day2-morning.html#comparing-mae-mape-and-mase",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Comparing MAE, MAPE and MASE",
    "text": "Comparing MAE, MAPE and MASE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n           MAE     MAPE      MASE\nyhat1 2.873328 43.14008  66.10004\nyhat2 5.382247 36.08279 123.81696"
  },
  {
    "objectID": "slides/day2-morning.html#defining-the-error-metrics-in-r",
    "href": "slides/day2-morning.html#defining-the-error-metrics-in-r",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Defining the error metrics in R",
    "text": "Defining the error metrics in R\n\nMSE &lt;- function(truth, prediction) {\n  mean((truth - prediction)^2)}\n\nMAE &lt;- function(truth, prediction) {\n  mean(abs(truth - prediction))}\n\nMAPE &lt;- function(truth, prediction) {\n  100 * mean(abs(truth - prediction) / truth)}\n\nMASE &lt;- function(truth, prediction) {\n  100 * MAE(truth, prediction) / mean(abs(diff(truth)))}"
  },
  {
    "objectID": "slides/day2-morning.html#estimating-the-prediction-error",
    "href": "slides/day2-morning.html#estimating-the-prediction-error",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Estimating the prediction error",
    "text": "Estimating the prediction error\n\nGiven an error metric, we want to estimate the prediction error under that metric.\nThis can be accomplished in different ways, using the\n\nTraining error\nSplit-sample error\nTime series cross-validation error (using all past data or a trailing window)"
  },
  {
    "objectID": "slides/day2-morning.html#training-error",
    "href": "slides/day2-morning.html#training-error",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Training error",
    "text": "Training error\n\nThe easiest but worst approach to estimate the prediction error is to use the training error, i.e. the average error on the training set that was used to fit the model.\nThe training error is\n\ngenerally too optimistic as an estimate of prediction error\nmore optimistic the more complex the model!1\n\n\nMore on this when we talk about overfitting."
  },
  {
    "objectID": "slides/day2-morning.html#training-error-1",
    "href": "slides/day2-morning.html#training-error-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Training error",
    "text": "Training error\nLinear regression of COVID deaths on lagged cases\n\n# Getting the predictions for the training set\npred_train &lt;- predict(reg_lagged)\n\n\n\n\n               MAE     MASE\ntraining 0.0740177 380.9996"
  },
  {
    "objectID": "slides/day2-morning.html#split-sample-error",
    "href": "slides/day2-morning.html#split-sample-error",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Split-sample error",
    "text": "Split-sample error\nTo compute the split-sample error\n\nSplit data into training (up to time \\(t_0\\)), and test set (after \\(t_0\\))\nFit the model to the training data only\nMake predictions for the test set\nCompute the selected error metric on the test set only\n\n\n\n\nNote\n\n\nSplit-sample estimates of prediction error don’t mimic a situation where we would refit the model in the future. They are pessimistic if the relation between outcome and predictors changes over time."
  },
  {
    "objectID": "slides/day2-morning.html#split-sample-error-1",
    "href": "slides/day2-morning.html#split-sample-error-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Split-sample error",
    "text": "Split-sample error\nAssume we want to make \\(h\\)-step ahead predictions, i.e. at time \\(t\\) we want to make a forecast for \\(t+h\\). Then, the split-sample MSE is\n\\[\\text{SplitMSE} = \\frac{1}{n-h-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t_0} - y_{t+h})^2\\]\nwhere \\(\\hat y_{t+h|t_0}\\) indicates a prediction for \\(y\\) at time \\(t+h\\) that was made with a model that was fit on data up to time \\(t_0\\)."
  },
  {
    "objectID": "slides/day2-morning.html#split-sample-error-2",
    "href": "slides/day2-morning.html#split-sample-error-2",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Split-sample error",
    "text": "Split-sample error\nLinear regression of COVID deaths on lagged cases\n\n# Getting the h-step ahead predictions for the test set\nh &lt;- k\ntest_h &lt;- test[-(1:h-1), ] # drop first h-1 rows to avoid data leakage\npred_test &lt;- predict(reg_lagged, newdata = test_h)\n\n\n\n\n                   MAE      MASE\ntraining     0.0740177  380.9996\nsplit-sample 0.3116854 2914.4575\n\n\n\nNote that we are overestimating the peak due to the changed relationship between cases - deaths over time.\nTalk about data leakage."
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cross-validation-cv",
    "href": "slides/day2-morning.html#time-series-cross-validation-cv",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series cross-validation (CV)",
    "text": "Time-series cross-validation (CV)\n\\(h\\)-step ahead predictions\n\nIf we refit in the future once new data are available, a more appropriate way to estimate the prediction error is time-series cross-validation.\nTo get \\(h\\)-step ahead predictions, we:\n\nFit the model using data up to time \\(t\\)\nMake a prediction for \\(t+h\\)\nRecord the prediction error\n\nThe cross-validation MSE is then\n\n\\[CVMSE = \\frac{1}{n-h-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t} - y_{t+h})^2\\]\nwhere \\(\\hat y_{t+h|t}\\) indicates a prediction for \\(y\\) at time \\(t+h\\) that was made with data available up to time \\(t\\)."
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cross-validation-cv-1",
    "href": "slides/day2-morning.html#time-series-cross-validation-cv-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series cross-validation (CV)",
    "text": "Time-series cross-validation (CV)\nLinear regression of COVID deaths on lagged cases\nGetting the predictions requires slightly more code:\n\nn &lt;- nrow(ca)                               #length of time series\nh &lt;- k                                      #number of days ahead for which prediction is wanted\npred_all_past &lt;- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to all past data and make h-step ahead prediction\n  reg_all_past = lm(deaths ~ lagged_cases, data = ca, subset = (1:n) &lt;= t) \n  pred_all_past[t+h] = predict(reg_all_past, newdata = data.frame(ca[t+h, ]))\n}\n\n\n\n\nNote\n\n\nWith the current model, we can only predict \\(k\\) days ahead (where \\(k\\) = number of days by which predictor is lagged)!"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cross-validation-cv-2",
    "href": "slides/day2-morning.html#time-series-cross-validation-cv-2",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series cross-validation (CV)",
    "text": "Time-series cross-validation (CV)\nLinear regression of COVID deaths on lagged cases\n\n\n\n                     MAE      MASE\ntraining       0.0740177  380.9996\nsplit-sample   0.3116854 2914.4575\ntime series CV 0.2374931 2212.5992\n\n\n\nSome improvement wrt split-sample, but still overestimating peak."
  },
  {
    "objectID": "slides/day2-morning.html#regression-on-a-trailing-window",
    "href": "slides/day2-morning.html#regression-on-a-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Regression on a trailing window",
    "text": "Regression on a trailing window\n\nSo far, to get \\(h\\)-step ahead predictions for time \\(t+h\\), we have fitted the model on all data available up to time \\(t\\). We can instead use a trailing window, i.e. fit the model on a window of data of length \\(w\\), starting at \\(t-w\\) and ending at \\(t\\).\nAdvantage: if the predictors-outcome relation changes over time, training the forecaster on a window of recent data can better capture the recent relation which might be more relevant to predict the outcome in the near future.\nWindow length \\(w\\) considerations:\n\nif \\(w\\) is too big, the model can’t adapt to the recent predictors-outcome relation\nif \\(w\\) is too small, the fitted model may be too volatile (trained on too little data)"
  },
  {
    "objectID": "slides/day2-morning.html#trailing-window",
    "href": "slides/day2-morning.html#trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Trailing window",
    "text": "Trailing window\nLinear regression of COVID deaths on lagged cases\n\n# Getting the predictions through CV with trailing window\nw &lt;- 120                                    #trailing window size\nh &lt;- k                                      #number of days ahead for which prediction is wanted\npred_trailing &lt;- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to a trailing window of size w and make h-step ahead prediction\n  reg_trailing = lm(deaths ~ lagged_cases, data = ca, \n                    subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n  pred_trailing[t+h] = predict(reg_trailing, newdata = data.frame(ca[t+h, ]))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cv-all-past-vs-trailing-window",
    "href": "slides/day2-morning.html#time-series-cv-all-past-vs-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series CV: all past vs trailing window",
    "text": "Time-series CV: all past vs trailing window\nLinear regression of COVID deaths on lagged cases\n\n\n\n                                 MAE      MASE\ntraining                  0.07401770  380.9996\nsplit-sample              0.31168536 2914.4575\ntime series CV            0.23749306 2212.5992\ntime series CV + trailing 0.09932651  925.3734\n\n\n\nA lot of improvement: trailing window allows to adapt to the change in relationship between cases and deaths over time."
  },
  {
    "objectID": "slides/day2-morning.html#autoregressive-exogenous-input-arx-model",
    "href": "slides/day2-morning.html#autoregressive-exogenous-input-arx-model",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Autoregressive exogenous input (ARX) model",
    "text": "Autoregressive exogenous input (ARX) model\n\nIdea: predicting the outcome via a linear combination of its lags and a set of exogenous (i.e. external) input variables\nExample:\n\n\\[\\hat y_{t+h} = \\hat\\phi + \\sum_{i=0}^p \\hat\\phi_i y_{t-i} + \\sum_{j=0}^q \\hat\\beta_j x_{t-j}\\]\n\nNotice: we don’t need to include all contiguous lags, and we could fit e.g.\n\n\\[\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#arx-model-for-covid-deaths",
    "href": "slides/day2-morning.html#arx-model-for-covid-deaths",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX model for COVID deaths",
    "text": "ARX model for COVID deaths\n\nLet’s add lagged deaths as a predictor to our previous forecaster:\n\n\\[\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}\\]\n\n# Prepare data: add column with deaths lagged by 28\nca$lagged_deaths &lt;- dplyr::lag(ca$deaths, n = k)\n\n\nHow does it compare to the previous model in terms of time-series CV?"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cv-all-past-and-trailing-arx-model",
    "href": "slides/day2-morning.html#time-series-cv-all-past-and-trailing-arx-model",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-Series CV: all past and trailing (ARX model)",
    "text": "Time-Series CV: all past and trailing (ARX model)\n\n\n\n                                 MAE      MASE\ntime series CV            0.16204381 1509.6779\ntime series CV + trailing 0.07872895  733.4767\n\n\n\nErrors under both metrics are smaller than with previous model."
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h",
    "href": "slides/day2-morning.html#predictions-for-different-h",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions for different \\(h\\)",
    "text": "Predictions for different \\(h\\)\n\nSo far we only focused on COVID death predictions 28 days ahead.\nWe will now compare the first model\n\n\\[\\hat y_{t+h} = \\hat\\beta + \\hat\\beta_0 x_t\\]\nto the second model\n\\[\\hat y_{t+h} = \\hat\\phi + \\hat\\phi_0 y_t + \\hat\\beta_0 x_t\\]\nfor horizons \\(h = 7, 14, 21, 28\\).\n\nWe will only make forecasts on the \\(1^{st}\\) day of each month, and use a trailing window with \\(w = 120\\)."
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h-1",
    "href": "slides/day2-morning.html#predictions-for-different-h-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions for different \\(h\\)",
    "text": "Predictions for different \\(h\\)\n\nh_vals &lt;- c(7, 14, 21, 28)  #horizons \npred_m1 = pred_m2 &lt;- data.frame(matrix(NA, nrow = 0, ncol = 3))  #initialize df for predictions\ncolnames(pred_m1) = colnames(pred_m2) = c(\"forecast_date\", \"target_date\", \"prediction\")\nw &lt;- 120    #trailing window size\n\nca_lags &lt;- ca %&gt;% select(!c(lagged_cases, lagged_deaths))\n\n# Create lagged predictors \nfor (i in seq_along(h_vals)) {\n  ca_lags[[paste0(\"lagged_deaths_\", h_vals[i])]] &lt;- dplyr::lag(ca_lags$deaths, n = h_vals[i])\n  ca_lags[[paste0(\"lagged_cases_\", h_vals[i])]] &lt;- dplyr::lag(ca_lags$cases, n = h_vals[i])\n}\n\n# Only forecast on 1st day of the months\nforecast_time &lt;- which(ca_lags$time_value &gt;= t0_date & \n                         ca_lags$time_value &lt; ca_lags$time_value[n-max(h_vals)] &\n                         day(ca_lags$time_value) == 1)\n\nfor (t in forecast_time) {\n  for (i in seq_along(h_vals)) {\n    h = h_vals[i]\n    # formulas including h-lagged variables\n    m1_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h))\n    m2_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h, \" + lagged_deaths_\", h))\n    # fit to trailing window of data\n    m1_fit = lm(m1_formula, data = ca_lags, subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n    m2_fit = lm(m2_formula, data = ca_lags, subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n    # make h-step ahead predictions\n    pred_m1 = rbind(pred_m1, \n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m1_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    pred_m2 = rbind(pred_m2, \n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m2_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    }\n}"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h-model-1",
    "href": "slides/day2-morning.html#predictions-for-different-h-model-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions for different \\(h\\) (Model 1)",
    "text": "Predictions for different \\(h\\) (Model 1)\n\n\n\n              MAE    MASE\nModel 1 0.1049742 304.007"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h-model-2",
    "href": "slides/day2-morning.html#predictions-for-different-h-model-2",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions for different \\(h\\) (Model 2)",
    "text": "Predictions for different \\(h\\) (Model 2)\n\n\n\n               MAE     MASE\nModel 2 0.04463132 129.2531"
  },
  {
    "objectID": "slides/day2-morning.html#arx-with-more-predictors",
    "href": "slides/day2-morning.html#arx-with-more-predictors",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX with more predictors",
    "text": "ARX with more predictors\n\nThe ARX model with only two predictors seems to forecast quite well for different \\(h\\).\nWe will try to improve it by adding some more lags. We will fit and compare\n\n\\[\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7}\\]\nand\n\\[\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-using-arx-with-2-lags",
    "href": "slides/day2-morning.html#predictions-using-arx-with-2-lags",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions using ARX with 2 lags",
    "text": "Predictions using ARX with 2 lags\n\n\n\n              MAE     MASE\nModel 3 0.0495936 143.6239"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-using-arx-with-3-lags",
    "href": "slides/day2-morning.html#predictions-using-arx-with-3-lags",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions using ARX with 3 lags",
    "text": "Predictions using ARX with 3 lags\n\n\n\n               MAE     MASE\nModel 4 0.05836984 169.0401"
  },
  {
    "objectID": "slides/day2-morning.html#too-many-predictors",
    "href": "slides/day2-morning.html#too-many-predictors",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Too many predictors",
    "text": "Too many predictors\n\nWhat if we try to incorporate past information extensively by fitting a model with a very large number of predictors?\n\nThe estimated coefficients will be chosen to mimic the observed data very closely on the training set, leading to small training error\nThe predictive performance on the test set might be very poor, producing large split-sample and CV error\n\n\n\n\n\nIssue\n\n\nOverfitting!"
  },
  {
    "objectID": "slides/day2-morning.html#arx-model-for-covid-deaths-with-many-predictors",
    "href": "slides/day2-morning.html#arx-model-for-covid-deaths-with-many-predictors",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX model for COVID deaths with many predictors",
    "text": "ARX model for COVID deaths with many predictors\n\nWhen predicting COVID deaths 28 days ahead, we can try to use more past information by fitting a model that includes the past two months of COVID deaths and cases as predictors\n\n\\[\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-1} + \\dots +\n\\hat\\phi_{59} y_{t-59} +\n\\hat\\beta_0 x_{t} + \\dots + \\hat\\beta_{t-59} x_{t-59}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#preparing-the-data-1",
    "href": "slides/day2-morning.html#preparing-the-data-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Preparing the data",
    "text": "Preparing the data\n\ny &lt;- ca$deaths  #outcome\nlags &lt;- 28:87   #lags used for predictors (deaths and cases)\nh &lt;- 28\n\n# Build predictor matrix with 60 columns\nX &lt;- data.frame(matrix(NA, nrow = length(y), ncol = 2*length(lags)))\ncolnames(X) &lt;- paste('X', 1:ncol(X), sep = '')\n\nfor (j in 1:length(lags)) {\n  # first 60 columns contain deaths lagged by 28, 29, ..., 87\n  X[, j] = dplyr::lag(ca$deaths, lags[j])\n  # last 60 columns contain cases lagged by 28, 29, ..., 87\n  X[, length(lags) + j] = dplyr::lag(ca$cases, lags[j])\n}"
  },
  {
    "objectID": "slides/day2-morning.html#fitting-the-arx-model",
    "href": "slides/day2-morning.html#fitting-the-arx-model",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fitting the ARX model",
    "text": "Fitting the ARX model\n\n# Train/test split\ny_train &lt;- y[1:t0]\nX_train &lt;- X[1:t0, ]\ny_test &lt;- y[(t0+h):length(y)]\nX_test &lt;- X[(t0+h):length(y), ]\n\n# Fitting the ARX model\nreg = lm(y_train ~ ., data = X_train)\ncoef(reg)\n\n  (Intercept)            X1            X2            X3            X4 \n 0.0775116437 -0.5593818462  0.7979659233 -0.4175920623 -0.2780809101 \n           X5            X6            X7            X8            X9 \n 0.3031742752 -0.0809244359  0.2548033065 -0.4860940499  0.1322930625 \n          X10           X11           X12           X13           X14 \n-0.2352041391 -0.1327834155  0.2155747658 -0.3380379255  0.4239820677 \n          X15           X16           X17           X18           X19 \n-0.2012041267 -0.3367851572 -0.2889890062  0.5328712849  0.5388650654 \n          X20           X21           X22           X23           X24 \n-0.3065835983  0.0724595436 -0.0168042757 -0.1171985635  0.2046513639 \n          X25           X26           X27           X28           X29 \n 0.1810480128  0.1213875691  0.0230516587  0.0196208441  0.0397085778 \n          X30           X31           X32           X33           X34 \n-0.3884271784  0.2088690345  0.1248242133  0.0706165553 -0.4882035875 \n          X35           X36           X37           X38           X39 \n 0.3609708771 -0.3169047917  0.4216666798  0.1891753615 -0.1106475626 \n          X40           X41           X42           X43           X44 \n 0.1498605000 -0.0692090064  0.1336287081 -0.1875462008 -0.2449003857 \n          X45           X46           X47           X48           X49 \n-0.0001337325 -0.5738823399  0.0695056705 -0.2460256934  1.0173509442 \n          X50           X51           X52           X53           X54 \n-0.1853591480 -0.5428279059  0.2678983608 -0.6935743948  0.3829408389 \n          X55           X56           X57           X58           X59 \n 0.1088530454  0.9466159031 -0.5618240450 -0.4660113206  0.6102916420 \n          X60           X61           X62           X63           X64 \n-0.2859449807  0.0283237204 -0.0099051792 -0.0070208086  0.0021192306 \n          X65           X66           X67           X68           X69 \n-0.0047341417 -0.0111532015  0.0038542335  0.0184565802 -0.0060684485 \n          X70           X71           X72           X73           X74 \n-0.0005801373  0.0048246180 -0.0061516656 -0.0066597399  0.0039021597 \n          X75           X76           X77           X78           X79 \n 0.0126296042 -0.0080708988 -0.0027091539  0.0052517573 -0.0052000323 \n          X80           X81           X82           X83           X84 \n 0.0029961750  0.0013593227  0.0083628716 -0.0063778828 -0.0018882435 \n          X85           X86           X87           X88           X89 \n-0.0097221295  0.0003314155 -0.0013911110  0.0066935430  0.0107961484 \n          X90           X91           X92           X93           X94 \n-0.0052473765 -0.0057177036  0.0023462634 -0.0112827594  0.0008517257 \n          X95           X96           X97           X98           X99 \n-0.0004388072  0.0231342674 -0.0056794349 -0.0046693142 -0.0061536587 \n         X100          X101          X102          X103          X104 \n-0.0094880392  0.0071605921  0.0021423255  0.0108738290 -0.0015420116 \n         X105          X106          X107          X108          X109 \n 0.0015155025  0.0022482275 -0.0148197121  0.0129113709  0.0009150566 \n         X110          X111          X112          X113          X114 \n 0.0021338029 -0.0019029077 -0.0040171812 -0.0025674957 -0.0069761237 \n         X115          X116          X117          X118          X119 \n 0.0226899068 -0.0022271856 -0.0060651747  0.0071536700 -0.0016426930 \n         X120 \n-0.0127949778"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-on-training-and-test-set",
    "href": "slides/day2-morning.html#predictions-on-training-and-test-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions on training and test set",
    "text": "Predictions on training and test set\n\n\n\n                    MAE      MASE\ntraining     0.04230235  189.9364\nsplit-sample 0.41684935 3883.5685\n\n\n\n\n\nNote\n\n\nSome predictions are negative, which doesn’t make sense for count data, so let’s truncate them at 0."
  },
  {
    "objectID": "slides/day2-morning.html#truncated-predictions-on-training-and-test-set",
    "href": "slides/day2-morning.html#truncated-predictions-on-training-and-test-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Truncated predictions on training and test set",
    "text": "Truncated predictions on training and test set\n\n\n\n                             MAE    MASE\nsplit-sample truncated 0.3978198 3706.28"
  },
  {
    "objectID": "slides/day2-morning.html#regularization",
    "href": "slides/day2-morning.html#regularization",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Regularization",
    "text": "Regularization\n\nIf we want to consider a large number of predictors, how can we avoid overfitting?\nIdea: introduce a regularization parameter \\(\\lambda\\) that shrinks or sets some of the estimated coefficients to zero, i.e. some predictors are estimated to have limited or no predictive power\nMost common regularization methods\n\nRidge: shrinks coefficients to zero\nLasso: sets some coefficients to zero"
  },
  {
    "objectID": "slides/day2-morning.html#arx-ridgelasso-for-covid-deaths",
    "href": "slides/day2-morning.html#arx-ridgelasso-for-covid-deaths",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX + ridge/lasso for COVID deaths",
    "text": "ARX + ridge/lasso for COVID deaths\nLet’s consider a model with lagged cases and deaths: 7 lags for each, spaced by one week.\n\nh &lt;- 28\nlags &lt;- h + 7*(0:6)   #lags used for predictors (deaths and cases)\n\n# Build predictor matrix \nX &lt;- data.frame(matrix(NA, nrow = length(y), ncol = 2*length(lags)))\ncolnames(X) &lt;- paste('X', 1:ncol(X), sep = '')\n\nfor (j in 1:length(lags)) {\n  # lagged deaths \n  X[, j] = dplyr::lag(ca$deaths, lags[j])\n  # lagged cases\n  X[, length(lags) + j] = dplyr::lag(ca$cases, lags[j])\n}\n\n# Train/test split\ny_train &lt;- y[1:t0]\nX_train &lt;- X[1:t0, ]\ny_test &lt;- y[(t0+h):length(y)]\nX_test &lt;- X[(t0+h):length(y), ]"
  },
  {
    "objectID": "slides/day2-morning.html#fit-arx-ridgelasso-for-covid-deaths",
    "href": "slides/day2-morning.html#fit-arx-ridgelasso-for-covid-deaths",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fit ARX + ridge/lasso for COVID deaths",
    "text": "Fit ARX + ridge/lasso for COVID deaths\n\nlibrary(glmnet) # Implements ridge and lasso\n\n# We'll need to omit NA values explicitly, as otherwise glmnet will complain\nna_obs &lt;- 1:max(lags)\nX_train &lt;- X_train[-na_obs, ]\ny_train &lt;- y_train[-na_obs]\n\n# Ridge regression: set alpha = 0, lambda sequence will be chosen automatically\nridge &lt;- glmnet(X_train, y_train, alpha = 0)\nbeta_ridge &lt;- coef(ridge)       # matrix of estimated coefficients \nlambda_ridge &lt;- ridge$lambda    # sequence of lambdas used to fit ridge \n\n# Lasso regression: set alpha = 1, lambda sequence will be chosen automatically\nlasso &lt;- glmnet(X_train, y_train, alpha = 1)\nbeta_lasso &lt;- coef(lasso)       # matrix of estimated coefficients \nlambda_lasso &lt;- lasso$lambda    # sequence of lambdas used to fit lasso \n\ndim(beta_lasso)      # One row per coefficient, one column per lambda value\n\n[1] 15 92"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-on-test-set-and-best-lambda",
    "href": "slides/day2-morning.html#predictions-on-test-set-and-best-lambda",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions on test set and best \\(\\lambda\\)",
    "text": "Predictions on test set and best \\(\\lambda\\)\n\n# Predict values for second half of the time series\nyhat_ridge &lt;- predict(ridge, newx = as.matrix(X_test))\nyhat_lasso &lt;- predict(lasso, newx = as.matrix(X_test))\n\n# Compute MAE \nmae_ridge &lt;- colMeans(abs(yhat_ridge - y_test))\nmae_lasso &lt;- colMeans(abs(yhat_lasso - y_test))\n\n# Select index of lambda vector which gives lowest MAE\nmin_ridge &lt;- which.min(mae_ridge)\nmin_lasso &lt;- which.min(mae_lasso)\npaste('Best MAE ridge:', round(min(mae_ridge), 3),\n      '; Best MAE lasso:', round(min(mae_lasso), 3))\n\n[1] \"Best MAE ridge: 0.292 ; Best MAE lasso: 0.295\"\n\n# Get predictions for train and test sets\npred_train_ridge &lt;- predict(ridge, newx = as.matrix(X_train))[, min_ridge] \npred_test_ridge &lt;- yhat_ridge[, min_ridge]\npred_train_lasso &lt;- predict(lasso, newx = as.matrix(X_train))[, min_lasso] \npred_test_lasso &lt;- yhat_lasso[, min_lasso]"
  },
  {
    "objectID": "slides/day2-morning.html#estimated-coefficients-shrinkage-vs-sparsity",
    "href": "slides/day2-morning.html#estimated-coefficients-shrinkage-vs-sparsity",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Estimated coefficients: shrinkage vs sparsity",
    "text": "Estimated coefficients: shrinkage vs sparsity\n\n\n\n\n                    ridge       lasso\n(Intercept)  0.2978514823 0.124915807\nX1           0.0433892112 0.069300307\nX2           0.0292256563 0.000000000\nX3           0.0179647838 0.000000000\nX4           0.0083918680 0.000000000\nX5          -0.0015177021 0.000000000\nX6          -0.0125786976 0.000000000\nX7          -0.0191532557 0.000000000\nX8           0.0010586265 0.008320956\nX9           0.0009417383 0.000000000\nX10          0.0008208805 0.001690258\nX11          0.0006535137 0.000000000\nX12          0.0004488789 0.000000000\nX13          0.0002816751 0.000000000\nX14          0.0001839354 0.000000000"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-arx-ridgelasso-train-and-test-set",
    "href": "slides/day2-morning.html#predictions-arx-ridgelasso-train-and-test-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions: ARX + ridge/lasso (train and test set)",
    "text": "Predictions: ARX + ridge/lasso (train and test set)\n\n\n\n                         MAE      MASE\nridge training     0.1975073  924.4584\nridge split-sample 0.2923452 2723.6281\nlasso training     0.0790951  370.2149\nlasso split-sample 0.2945295 2743.9784"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cv-for-arx-ridgelasso-trailing",
    "href": "slides/day2-morning.html#time-series-cv-for-arx-ridgelasso-trailing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series CV for ARX + ridge/lasso (trailing)",
    "text": "Time-series CV for ARX + ridge/lasso (trailing)\n\nh &lt;- 28  # number of days ahead \nw &lt;- 120 # window length\n\n# Initialize matrices for predictions (one column per lambda value)\nyhat_ridge &lt;- rep(NA, length = n) \nyhat_lasso &lt;- rep(NA, length = n) \n\nfor (t in t0:(n-h)) {\n  # Choose best lambda\n  cv_inds = max(lags) &lt; 1:n & 1:n &lt;= t-w\n  ridge_cv = cv.glmnet(as.matrix(X[cv_inds, ]), y[cv_inds], alpha = 0)\n  lasso_cv = cv.glmnet(as.matrix(X[cv_inds, ]), y[cv_inds], alpha = 1)\n  best_lambda_ridge = ridge_cv$lambda.min\n  best_lambda_lasso = lasso_cv$lambda.min\n  # Indices of data within window\n  inds = t-w &lt; 1:n & 1:n &lt;= t\n  # Fit ARX + ridge/lasso\n  ridge_trail = glmnet(X[inds, ], y[inds], alpha = 0, lambda = best_lambda_ridge)\n  lasso_trail = glmnet(X[inds, ], y[inds], alpha = 1, lambda = best_lambda_lasso)\n  # Predict\n  yhat_ridge[t+h] = predict(ridge_trail, newx = as.matrix(X[(t+h), ]))\n  yhat_lasso[t+h] = predict(lasso_trail, newx = as.matrix(X[(t+h), ]))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-time-series-cv-for-arx-ridgelasso-trailing",
    "href": "slides/day2-morning.html#predictions-time-series-cv-for-arx-ridgelasso-trailing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions: time-series CV for ARX + ridge/lasso (trailing)",
    "text": "Predictions: time-series CV for ARX + ridge/lasso (trailing)\n\n\n\n                          MAE     MASE\nridge CV + trailing 0.1068111  995.103\nlasso CV + trailing 0.1328789 1237.964"
  },
  {
    "objectID": "slides/day2-morning.html#point-predictions-vs-intervals",
    "href": "slides/day2-morning.html#point-predictions-vs-intervals",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Point predictions vs intervals",
    "text": "Point predictions vs intervals\n\nSo far, we have only considered point predictions, i.e.  we have fitted models to provide our best guess on the outcome at time \\(t+h\\).\n\n\n\n\nImportant\n\n\nWhat if we want to provide a measure of uncertainty around the point prediction or a likely range of values for the outcome at time \\(t+h\\)?\n\n\n\n\nFor each target time \\(t+h\\), we can construct prediction intervals, i.e. provide ranges of values that are expected to cover the true outcome value a fixed fraction of times."
  },
  {
    "objectID": "slides/day2-morning.html#prediction-intervals-for-lm-fits",
    "href": "slides/day2-morning.html#prediction-intervals-for-lm-fits",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Prediction intervals for lm fits",
    "text": "Prediction intervals for lm fits\n\nTo get prediction intervals for the models we previously fitted, we only need to tweak our call to predict by adding as an input:\ninterval = \"prediction\", level = p\nwhere \\(p \\in (0, 1)\\) is the desired coverage.\nThe output from predict will then be a matrix with\n\nfirst column a point estimate\nsecond column the lower limit of the interval\nthird column the upper limit of the interval"
  },
  {
    "objectID": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window",
    "href": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Prediction intervals for ARX (CV, trailing window)",
    "text": "Prediction intervals for ARX (CV, trailing window)\n\n# Initialize matrices to store predictions \n# 3 columns: point estimate, lower limit, and upper limit\npred_trailing &lt;- matrix(NA, nrow = n, ncol = 3)\ncolnames(pred_trailing) &lt;- c('prediction', 'lower', 'upper')\n\nfor (t in t0:(n-h)) {\n  # Fit ARX and predict\n  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, \n                    subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n  pred_trailing[t+h, ] = predict(arx_trailing, newdata = data.frame(ca[t+h, ]),\n                                 interval = \"prediction\", level = 0.95)\n}"
  },
  {
    "objectID": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window-1",
    "href": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Prediction intervals for ARX (CV, trailing window)",
    "text": "Prediction intervals for ARX (CV, trailing window)\n\n\n\n                 MAE     MASE\nlm.trailing 0.104397 972.6125"
  },
  {
    "objectID": "slides/day2-morning.html#quantile-regression",
    "href": "slides/day2-morning.html#quantile-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Quantile regression",
    "text": "Quantile regression\n\nSo far we only considered different ways to apply linear regression.\nQuantile regression is a different estimation method, and it directly targets conditional quantiles of the outcome over time.\n\n\n\n\n\n\n\nDefinition\n\n\nConditional quantile = value below which a given percentage (e.g. 25%, 50%, 75%) of observations fall, given specific values of the predictor variables.\n\n\n\n\nAdvantage: it provides a more complete picture of the outcome distribution."
  },
  {
    "objectID": "slides/day2-morning.html#arx-model-for-covid-deaths-via-quantile-regression",
    "href": "slides/day2-morning.html#arx-model-for-covid-deaths-via-quantile-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX model for COVID deaths via quantile regression",
    "text": "ARX model for COVID deaths via quantile regression\n\n#install.packages(\"quantreg\")\nlibrary(quantreg)  #library to perform quantile regression\n\n# Set quantiles of interest: we will focus on 2.5%, 50% (i.e. median), and 97.5% quantiles\nquantiles &lt;- c(0.025, 0.5, 0.975)  \n\n# Fit quantile regression to training set\nq_reg &lt;- rq(deaths ~ lagged_deaths + lagged_cases, data = train, tau = quantiles)\n\n# Estimated coefficients\ncoef(q_reg)\n\n               tau= 0.025 tau= 0.500 tau= 0.975\n(Intercept)   0.009351562 0.06896010 0.12257658\nlagged_deaths 0.229011485 0.19821254 0.28469573\nlagged_cases  0.007439881 0.01022547 0.01265167\n\n# Sort estimated coefficients \ncoefs_sorted &lt;- t(apply(coef(q_reg), 1, sort))\ncolnames(coefs_sorted) &lt;- colnames(coef(q_reg))\ncoefs_sorted\n\n               tau= 0.025 tau= 0.500 tau= 0.975\n(Intercept)   0.009351562 0.06896010 0.12257658\nlagged_deaths 0.198212543 0.22901149 0.28469573\nlagged_cases  0.007439881 0.01022547 0.01265167\n\nq_reg$coefficients &lt;- coefs_sorted"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-via-quantile-regression-cv-trailing",
    "href": "slides/day2-morning.html#predictions-via-quantile-regression-cv-trailing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions via quantile regression (CV, trailing)",
    "text": "Predictions via quantile regression (CV, trailing)\n\n# Initialize matrix to store predictions \n# 3 columns: lower limit, median, and upper limit\npred_trailing &lt;- matrix(NA, nrow = n, ncol = 3)\ncolnames(pred_trailing) &lt;- c('lower', 'median', 'upper')\n\nfor (t in t0:(n-h)) {\n  # Fit quantile regression\n  rq_trailing = rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles,\n                   data = ca, subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n  # Sort estimated coefficients \n  coefs_sorted &lt;- t(apply(coef(rq_trailing), 1, sort))\n  rq_trailing$coefficients &lt;- coefs_sorted\n  # Predict\n  pred_trailing[t+h, ] = predict(rq_trailing, newdata = data.frame(ca[t+h, ]))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-via-quantile-regression-cv-trailing-1",
    "href": "slides/day2-morning.html#predictions-via-quantile-regression-cv-trailing-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions via quantile regression (CV, trailing)",
    "text": "Predictions via quantile regression (CV, trailing)\n\n\n\n                  MAE     MASE\nrq.trailing 0.1244401 1159.343"
  },
  {
    "objectID": "slides/day2-morning.html#actual-coverage",
    "href": "slides/day2-morning.html#actual-coverage",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Actual Coverage",
    "text": "Actual Coverage\n\nWe would expect the ARX model fitted via lm and via rq to cover the truth about 95% of the times. Is this actually true in practice?\nThe actual coverage of each predictive interval is lower:\n\n\n\n         lm.trailing rq.trailing\nCoverage   0.8294118   0.8117647"
  },
  {
    "objectID": "slides/day2-morning.html#evaluation-1",
    "href": "slides/day2-morning.html#evaluation-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Evaluation",
    "text": "Evaluation\n\nPrediction intervals are “good” if they\n\ncover the truth most of the time\nare not too wide\n\nError metric that captures both desiderata: Weighted Interval Score (WIS)\n\\(F\\) = forecast composed of predicted quantiles \\(q_{\\tau}\\) for the set of quantile levels \\(\\tau\\). The WIS for target variable \\(Y\\) is represented as (McDonald et al., 2021):\n\n\\[WIS(F, Y) = 2\\sum_{\\tau} \\phi_{\\tau} (Y - q_{\\tau})\\]\nwhere \\(\\phi_{\\tau}(x) = \\tau |x|\\) for \\(x \\geq 0\\) and \\(\\phi_{\\tau}(x) = (1-\\tau) |x|\\) for \\(x &lt; 0\\)."
  },
  {
    "objectID": "slides/day2-morning.html#computing-the-wis",
    "href": "slides/day2-morning.html#computing-the-wis",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Computing the WIS",
    "text": "Computing the WIS\n\nWIS &lt;- function(truth, estimates, quantiles) {\n  2 * sum(pmax(\n    quantiles * (truth - estimates),\n    (1 - quantiles) * (estimates - truth),\n    na.rm = TRUE\n  ))\n}\n\n\n\n\nNote\n\n\nWIS tends to prioritize sharpness (how wide the interval is) relative to coverage (if the interval contains the truth)."
  },
  {
    "objectID": "slides/day2-morning.html#wis-for-arx-fitted-via-lm-and-rq",
    "href": "slides/day2-morning.html#wis-for-arx-fitted-via-lm-and-rq",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "WIS for ARX fitted via lm and rq",
    "text": "WIS for ARX fitted via lm and rq\n\nThe lowest mean WIS is attained by quantile regression.\nNotice: this method has coverage below 95% but is still preferred under WIS because its intervals are narrower than for linear regression.\n\n\n\n  Mean WIS lm Mean WIS rq\n1   0.1335326   0.1056215"
  },
  {
    "objectID": "slides/day2-morning.html#versioned-data",
    "href": "slides/day2-morning.html#versioned-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Versioned data",
    "text": "Versioned data\n\nIn our forecasting examples, we have assumed the data are never revised (or have simply ignored revisions, and used data as_of today)\n\n\n\n\nImportant\n\n\nHow can we train forecasters when dealing with versioned data?\n\n\n\n\ndata_archive\n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-04-01 / 2023-03-09\nℹ First/last version with update: 2020-04-02 / 2023-03-10\nℹ Versions end: 2023-03-10\nℹ A preview of the table (148820 rows x 5 columns):\n        geo_value time_value    version case_rate death_rate\n     1:        ak 2020-04-01 2020-04-02  1.797489  0.0000000\n     2:        ak 2020-04-01 2020-05-07  1.777061  0.0000000\n     3:        ak 2020-04-01 2020-10-28  1.106147  0.0000000\n     4:        ak 2020-04-01 2020-10-29  1.797489  0.0000000\n     5:        ak 2020-04-01 2020-10-30  1.797489  0.0000000\n    ---                                                     \n148816:        wy 2023-03-05 2023-03-06  0.000000  0.0000000\n148817:        wy 2023-03-06 2023-03-07  0.000000  0.0000000\n148818:        wy 2023-03-07 2023-03-08 38.809743  0.3434491\n148819:        wy 2023-03-08 2023-03-09  0.000000  0.0000000\n148820:        wy 2023-03-09 2023-03-10  0.000000  0.0000000"
  },
  {
    "objectID": "slides/day2-morning.html#version-aware-forecasting",
    "href": "slides/day2-morning.html#version-aware-forecasting",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Version-aware forecasting",
    "text": "Version-aware forecasting\n\n# initialize dataframe for predictions\n# 5 columns: forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_trailing &lt;- data.frame(matrix(NA, ncol = 5, nrow = 0))\ncolnames(pred_trailing) &lt;- c(\"forecast_date\", \"target_date\", 'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw &lt;- 120         #trailing window size\nh &lt;- 28          #number of days ahead\n\n# dates when predictions are made (set to be 1 month apart)\nfc_time_values &lt;- seq(from = t0_date, to = as.Date(\"2023-02-01\"), by = \"1 month\")\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data &lt;- epix_as_of(ca_archive, max_version = as.Date(fc_date))\n  # create lagged predictors\n  data$lagged_deaths &lt;- dplyr::lag(data$deaths, h) \n  data$lagged_cases &lt;- dplyr::lag(data$cases, h)\n  # perform quantile regression\n  rq_trailing &lt;- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, \n                    # only consider window of data\n                    data = data %&gt;% filter(time_value &gt; (max(time_value) - w))) \n  # sort estimated coefficients \n  coefs_sorted &lt;- t(apply(coef(rq_trailing), 1, sort))\n  colnames(coefs_sorted) &lt;- c('tau..0.025', 'tau..0.500', 'tau..0.975')\n  rq_trailing$coefficients &lt;- coefs_sorted\n  # construct data.frame with the right predictors for the target date\n  predictors &lt;- data.frame(lagged_deaths = tail(data$deaths, 1), \n                           lagged_cases = tail(data$cases, 1))\n  # make predictions for target date and add them to matrix of predictions\n  pred_trailing &lt;- rbind(pred_trailing, \n                         data.frame('forecast_date' = max(data$time_value),\n                                    'target_date' = max(data$time_value) + h, \n                                    predict(rq_trailing, newdata = predictors)))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#version-aware-predictions-cv-trailing",
    "href": "slides/day2-morning.html#version-aware-predictions-cv-trailing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Version-aware predictions (CV, trailing)",
    "text": "Version-aware predictions (CV, trailing)"
  },
  {
    "objectID": "slides/day2-morning.html#using-geo-information",
    "href": "slides/day2-morning.html#using-geo-information",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using geo information",
    "text": "Using geo information\n\nAssume we observe data over time from multiple locations (e.g. states or counties).\nWe could\n\nEstimate coefficients separately for each location (as we have done so far).\nFit one model using all locations together at each time point (geo-pooling). Estimated coefficients will not be location specific.\nEstimate coefficients separately for each location, but include predictors capturing averages across locations (partial geo-pooling)."
  },
  {
    "objectID": "slides/day2-morning.html#geo-pooling-trailing-window",
    "href": "slides/day2-morning.html#geo-pooling-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Geo-pooling (trailing window)",
    "text": "Geo-pooling (trailing window)\n\nusa_archive &lt;- data_archive$DT %&gt;% \n  as_epi_archive()\n\n# initialize dataframe for predictions\n# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_trailing &lt;- data.frame(matrix(NA, ncol = 6, nrow = 0))\ncolnames(pred_trailing) &lt;- c('geo_value', 'forecast_date', 'target_date',\n                             'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw &lt;- 120         #trailing window size\nh &lt;- 28          #number of days ahead\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data &lt;- epix_as_of(usa_archive, max_version = as.Date(fc_date))\n  \n  # create lagged predictors for each state \n  data &lt;- data %&gt;%\n    arrange(geo_value, time_value) %&gt;%  \n    group_by(geo_value) %&gt;%\n    mutate(lagged_deaths = dplyr::lag(deaths, h),\n           lagged_cases = dplyr::lag(cases, h)) %&gt;%\n    ungroup()\n  \n  # perform quantile regression\n  rq_trailing &lt;- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, \n                    # only consider window of data\n                    data = data %&gt;% filter(time_value &gt; (max(time_value) - w))) \n  # sort estimated coefficients \n  coefs_sorted &lt;- t(apply(coef(rq_trailing), 1, sort))\n  colnames(coefs_sorted) &lt;- c('tau..0.025', 'tau..0.500', 'tau..0.975')\n  rq_trailing$coefficients &lt;- coefs_sorted\n  \n  # construct dataframe with the right predictors for the target date\n  new_lagged_deaths &lt;- data %&gt;% \n    filter(time_value == max(time_value)) %&gt;%\n    select(geo_value, deaths)\n  \n  new_lagged_cases &lt;- data %&gt;% \n    filter(time_value == max(time_value)) %&gt;%\n    select(geo_value, cases)\n  \n  predictors &lt;- new_lagged_deaths %&gt;%\n    inner_join(new_lagged_cases, join_by(geo_value)) %&gt;%\n    rename(lagged_deaths = deaths,\n           lagged_cases = cases)\n  \n  # make predictions for target date and add them to matrix of predictions\n  pred_trailing &lt;- rbind(pred_trailing, \n                         data.frame(\n                           'geo_value' = predictors$geo_value,\n                           'forecast_date' = max(data$time_value),\n                           'target_date' = max(data$time_value) + h, \n                           predict(rq_trailing, newdata = predictors)))\n}\n\n# geo-pooled predictions for California\npred_ca &lt;- pred_trailing %&gt;%\n  filter(geo_value == 'ca') %&gt;%\n  rename(median = `tau..0.500`,\n         lower = `tau..0.025`,\n         upper = `tau..0.975`) %&gt;%\n  full_join(ca %&gt;% select(time_value, deaths), join_by(target_date == time_value)) %&gt;%\n  arrange(target_date)"
  },
  {
    "objectID": "slides/day2-morning.html#geo-pooled-predictions-for-california",
    "href": "slides/day2-morning.html#geo-pooled-predictions-for-california",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Geo-pooled predictions for California",
    "text": "Geo-pooled predictions for California"
  },
  {
    "objectID": "slides/day2-morning.html#partial-geo-pooling-trailing-window",
    "href": "slides/day2-morning.html#partial-geo-pooling-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Partial geo-pooling (trailing window)",
    "text": "Partial geo-pooling (trailing window)\n\n# initialize dataframe for predictions\n# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_trailing &lt;- data.frame(matrix(NA, ncol = 6, nrow = 0))\ncolnames(pred_trailing) &lt;- c('geo_value', 'forecast_date', 'target_date',\n                             'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw &lt;- 120         #trailing window size\nh &lt;- 28          #number of days ahead\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data &lt;- epix_as_of(usa_archive, max_version = as.Date(fc_date))\n  \n  # create lagged predictors \n  data &lt;- data %&gt;%\n    arrange(geo_value, time_value) %&gt;%  \n    group_by(geo_value) %&gt;%\n    mutate(lagged_deaths = dplyr::lag(deaths, h),\n           lagged_cases = dplyr::lag(cases, h)) %&gt;%\n    ungroup() %&gt;%\n    group_by(time_value) %&gt;%\n    mutate(avg_lagged_deaths = mean(lagged_deaths, na.rm = T),\n           avg_lagged_cases = mean(lagged_cases, na.rm = T)) %&gt;%\n    ungroup() \n  \n  # perform quantile regression\n  rq_trailing &lt;- rq(deaths ~ lagged_deaths + lagged_cases + avg_lagged_deaths +\n                      avg_lagged_cases, tau = quantiles, \n                    data = (data %&gt;% filter(geo_value == 'ca'))) \n  \n  # sort estimated coefficients \n  coefs_sorted &lt;- t(apply(coef(rq_trailing), 1, sort))\n  colnames(coefs_sorted) &lt;- c('tau..0.025', 'tau..0.500', 'tau..0.975')\n  rq_trailing$coefficients &lt;- coefs_sorted\n  \n  # construct data.frame with the right predictors for the target date\n  new_lagged_deaths &lt;- data %&gt;% \n    filter(time_value == max(time_value)) %&gt;%\n    select(geo_value, deaths) %&gt;%\n    mutate(avg_lagged_deaths = mean(deaths, na.rm = T)) %&gt;%\n    filter(geo_value == 'ca')\n  \n  new_lagged_cases &lt;- data %&gt;% \n    filter(time_value == max(time_value)) %&gt;%\n    select(geo_value, cases) %&gt;%\n    mutate(avg_lagged_cases = mean(cases, na.rm = T)) %&gt;%\n    filter(geo_value == 'ca')\n  \n  predictors &lt;- new_lagged_deaths %&gt;%\n    inner_join(new_lagged_cases, join_by(geo_value)) %&gt;%\n    rename(lagged_deaths = deaths,\n           lagged_cases = cases)\n  \n  # make predictions for target date and add them to matrix of predictions\n  pred_trailing &lt;- rbind(pred_trailing, \n                         data.frame(\n                           'geo_value' = predictors$geo_value,\n                           'forecast_date' = max(data$time_value),\n                           'target_date' = max(data$time_value) + h, \n                           predict(rq_trailing, newdata = predictors)))\n}\n\n# partially geo-pooled predictions for California\npred_ca &lt;- pred_trailing %&gt;%\n  filter(geo_value == 'ca') %&gt;%\n  rename(median = `tau..0.500`,\n         lower = `tau..0.025`,\n         upper = `tau..0.975`) %&gt;%\n  full_join(ca %&gt;% select(time_value, deaths), join_by(target_date == time_value)) %&gt;%\n  arrange(target_date)"
  },
  {
    "objectID": "slides/day2-morning.html#partially-geo-pooled-predictions-for-california",
    "href": "slides/day2-morning.html#partially-geo-pooled-predictions-for-california",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Partially geo-pooled predictions for California",
    "text": "Partially geo-pooled predictions for California"
  },
  {
    "objectID": "slides/day2-morning.html#final-slide",
    "href": "slides/day2-morning.html#final-slide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Final slide",
    "text": "Final slide\nThanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nForecasting and Time-Series Models — cmu-delphi/insightnet-workshop-2024"
  },
  {
    "objectID": "slides/day1-afternoon.html#section",
    "href": "slides/day1-afternoon.html#section",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Explore, clean & transform data",
    "text": "Explore, clean & transform data"
  },
  {
    "objectID": "slides/day1-afternoon.html#outline",
    "href": "slides/day1-afternoon.html#outline",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Outline",
    "text": "Outline\n\nEssentials of dplyr and tidyr\nEpiverse software ecosystem\nPanel and versioned data in the epiverse\nBasic Nowcasting using epiprocess\nMotivating case study"
  },
  {
    "objectID": "slides/day1-afternoon.html#down-with-spreadsheets-for-data-manipulation",
    "href": "slides/day1-afternoon.html#down-with-spreadsheets-for-data-manipulation",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Down with Spreadsheets for Data Manipulation",
    "text": "Down with Spreadsheets for Data Manipulation\n\nSpreadsheets make it difficult to rerun analyses consistently.\nUsing R (and dplyr) allows for:\n\nReproducibility\nEase of modification\n\nRecommendation: Avoid manual edits; instead, use code for transformations."
  },
  {
    "objectID": "slides/day1-afternoon.html#introduction-to-dplyr",
    "href": "slides/day1-afternoon.html#introduction-to-dplyr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Introduction to dplyr",
    "text": "Introduction to dplyr\n\ndplyr is a powerful package in R for data manipulation.\nIt is part of the tidyverse, which includes a collection of packages designed to work together.\nWe focus on basic operations like selecting and filtering data.\nMake sure to load the necessary libraries before using dplyr."
  },
  {
    "objectID": "slides/day1-afternoon.html#meet-the-palmers",
    "href": "slides/day1-afternoon.html#meet-the-palmers",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Meet the Palmers",
    "text": "Meet the Palmers\n\nIllustration from the palmerpenguins website"
  },
  {
    "objectID": "slides/day1-afternoon.html#working-with-the-palmerpenguins-dataset",
    "href": "slides/day1-afternoon.html#working-with-the-palmerpenguins-dataset",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Working with the palmerpenguins Dataset",
    "text": "Working with the palmerpenguins Dataset\n\nThe palmerpenguins dataset is included in the palmerpenguins package.\nLoad both the tidyverse and palmerpenguins libraries to access and explore the dataset.\nThe dataset includes measurements of penguins such as species, bill length, flipper length, and body mass.\n\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#ways-to-inspect-the-dataset",
    "href": "slides/day1-afternoon.html#ways-to-inspect-the-dataset",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Ways to Inspect the Dataset",
    "text": "Ways to Inspect the Dataset\n\nUse head() to view the first 6 row of the data (tail() to view the last 6 rows)\n\n\nhead(penguins)  # First 6 rows\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n#tail(penguins)  # Last 6 rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#ways-to-inspect-the-dataset-1",
    "href": "slides/day1-afternoon.html#ways-to-inspect-the-dataset-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Ways to Inspect the Dataset",
    "text": "Ways to Inspect the Dataset\n\nglimpse() to get a compact overview of the dataset.\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-tibbles",
    "href": "slides/day1-afternoon.html#creating-tibbles",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Creating Tibbles",
    "text": "Creating Tibbles\n\nTibbles: Modern data frames with enhanced features.\nRows represent observations (or cases).\nColumns represent variables (or features).\nYou can create tibbles manually using the tibble() function.\n\n\ntibble(x = letters, y = 1:26)\n\n# A tibble: 26 × 2\n   x         y\n   &lt;chr&gt; &lt;int&gt;\n 1 a         1\n 2 b         2\n 3 c         3\n 4 d         4\n 5 e         5\n 6 f         6\n 7 g         7\n 8 h         8\n 9 i         9\n10 j        10\n# ℹ 16 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#selecting-columns-with-select",
    "href": "slides/day1-afternoon.html#selecting-columns-with-select",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Selecting Columns with select()",
    "text": "Selecting Columns with select()\n\nThe select() function is used to pick specific columns from your dataset.\n\n\nselect(penguins, species, body_mass_g)  # Select the 'species' and 'body_mass_g' columns\n\n# A tibble: 344 × 2\n   species body_mass_g\n   &lt;fct&gt;         &lt;int&gt;\n 1 Adelie         3750\n 2 Adelie         3800\n 3 Adelie         3250\n 4 Adelie           NA\n 5 Adelie         3450\n 6 Adelie         3650\n 7 Adelie         3625\n 8 Adelie         4675\n 9 Adelie         3475\n10 Adelie         4250\n# ℹ 334 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#selecting-columns-with-select-1",
    "href": "slides/day1-afternoon.html#selecting-columns-with-select-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Selecting Columns with select()",
    "text": "Selecting Columns with select()\n\nYou can exclude columns by prefixing the column names with a minus sign -.\n\n\nselect(penguins, -species)  # Exclude the 'species' column from the dataset\n\n# A tibble: 344 × 7\n   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Torge…           39.1          18.7               181        3750 male   2007\n 2 Torge…           39.5          17.4               186        3800 fema…  2007\n 3 Torge…           40.3          18                 195        3250 fema…  2007\n 4 Torge…           NA            NA                  NA          NA &lt;NA&gt;   2007\n 5 Torge…           36.7          19.3               193        3450 fema…  2007\n 6 Torge…           39.3          20.6               190        3650 male   2007\n 7 Torge…           38.9          17.8               181        3625 fema…  2007\n 8 Torge…           39.2          19.6               195        4675 male   2007\n 9 Torge…           34.1          18.1               193        3475 &lt;NA&gt;   2007\n10 Torge…           42            20.2               190        4250 &lt;NA&gt;   2007\n# ℹ 334 more rows\n\n\n\nSo, this is useful when you want to keep only certain columns or remove unnecessary ones."
  },
  {
    "objectID": "slides/day1-afternoon.html#extracting-columns-with-pull",
    "href": "slides/day1-afternoon.html#extracting-columns-with-pull",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Extracting Columns with pull()",
    "text": "Extracting Columns with pull()\n\npull(): Extract a column as a vector.\nLet’s try this with the species column…\n\n\npull(penguins, species)\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo"
  },
  {
    "objectID": "slides/day1-afternoon.html#filtering-rows-with-filter",
    "href": "slides/day1-afternoon.html#filtering-rows-with-filter",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Filtering Rows with filter()",
    "text": "Filtering Rows with filter()\n\nThe filter() function allows you to select rows that meet specific conditions.\nConditions can involve column values, such as selecting only “Gentoo” penguins or filtering based on measurements like flipper length.\nThis enables you to narrow down your dataset to focus on relevant data.\n\n\nfilter(penguins, species == \"Gentoo\", flipper_length_mm &lt; 208)  # Filter Gentoo penguins with flipper length &lt; 208mm\n\n# A tibble: 2 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo  Biscoe           45.1          14.5               207        5050\n2 Gentoo  Biscoe           48.4          14.4               203        4625\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#combining-select-and-filter-functions",
    "href": "slides/day1-afternoon.html#combining-select-and-filter-functions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Combining select() and filter() Functions",
    "text": "Combining select() and filter() Functions\n\nYou can combine select() and filter() functions to refine the dataset further.\nUse select() to choose columns and filter() to narrow rows based on conditions.\nThis helps in extracting the exact data needed for analysis.\n\n\nselect(filter(penguins, species == \"Gentoo\", flipper_length_mm &lt; 208), species, flipper_length_mm)\n\n# A tibble: 2 × 2\n  species flipper_length_mm\n  &lt;fct&gt;               &lt;int&gt;\n1 Gentoo                207\n2 Gentoo                203"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-the-pipe-operator",
    "href": "slides/day1-afternoon.html#using-the-pipe-operator",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using the Pipe Operator %>%",
    "text": "Using the Pipe Operator %&gt;%\n\nThe pipe operator (%&gt;%) makes code more readable by chaining multiple operations together.\nThe output of one function is automatically passed to the next function.\nThis allows you to perform multiple steps (e.g., select() followed by filter()) in a clear and concise manner.\n\n\n# This code reads more like poetry!\npenguins %&gt;% \n  select(species, flipper_length_mm) %&gt;%\n  filter(species == \"Gentoo\", flipper_length_mm &lt; 208)\n\n# A tibble: 2 × 2\n  species flipper_length_mm\n  &lt;fct&gt;               &lt;int&gt;\n1 Gentoo                207\n2 Gentoo                203"
  },
  {
    "objectID": "slides/day1-afternoon.html#key-practices-in-dplyr",
    "href": "slides/day1-afternoon.html#key-practices-in-dplyr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Key Practices in dplyr",
    "text": "Key Practices in dplyr\n\nUse tibbles for easier data handling.\nUse select() and filter() for data manipulation.\nUse pull() to extract columns as vectors.\nUse head(), tail(), and glimpse() for quick data inspection.\nChain functions with %&gt;% for cleaner code."
  },
  {
    "objectID": "slides/day1-afternoon.html#grouping-data-with-group_by",
    "href": "slides/day1-afternoon.html#grouping-data-with-group_by",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Grouping Data with group_by()",
    "text": "Grouping Data with group_by()\n\nUse group_by() to group data by one or more columns.\nAllows performing operations on specific groups of data.\n\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  filter(body_mass_g == min(body_mass_g, na.rm = TRUE))\n\n# A tibble: 4 × 8\n# Groups:   species [3]\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Biscoe           36.5          16.6               181        2850\n2 Adelie    Biscoe           36.4          17.1               184        2850\n3 Gentoo    Biscoe           42.7          13.7               208        3950\n4 Chinstrap Dream            46.9          16.6               192        2700\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#penguin-bill-length-and-depth",
    "href": "slides/day1-afternoon.html#penguin-bill-length-and-depth",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Penguin bill length and depth",
    "text": "Penguin bill length and depth\n\n\nIllustration from the palmerpenguins website"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-new-columns-with-mutate",
    "href": "slides/day1-afternoon.html#creating-new-columns-with-mutate",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Creating New Columns with mutate()",
    "text": "Creating New Columns with mutate()\n\nmutate() is used to create new columns.\nPerform calculations using existing columns and assign to new columns.\n\n\npenguins %&gt;%\n  mutate(bill_size_mm2 = bill_depth_mm * bill_length_mm) %&gt;% \n  select(-c(flipper_length_mm, body_mass_g, sex)) # too many cols to print\n\n# A tibble: 344 × 6\n   species island    bill_length_mm bill_depth_mm  year bill_size_mm2\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7  2007          731.\n 2 Adelie  Torgersen           39.5          17.4  2007          687.\n 3 Adelie  Torgersen           40.3          18    2007          725.\n 4 Adelie  Torgersen           NA            NA    2007           NA \n 5 Adelie  Torgersen           36.7          19.3  2007          708.\n 6 Adelie  Torgersen           39.3          20.6  2007          810.\n 7 Adelie  Torgersen           38.9          17.8  2007          692.\n 8 Adelie  Torgersen           39.2          19.6  2007          768.\n 9 Adelie  Torgersen           34.1          18.1  2007          617.\n10 Adelie  Torgersen           42            20.2  2007          848.\n# ℹ 334 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-new-columns-with-mutate-1",
    "href": "slides/day1-afternoon.html#creating-new-columns-with-mutate-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Creating New Columns with mutate()",
    "text": "Creating New Columns with mutate()\n\nmutate() can create multiple new columns in one step.\nLogical comparisons (e.g., sex == \"male\") can be used within mutate().\n\n\npenguins %&gt;%\n  mutate(bill_size_mm2 = bill_depth_mm * bill_length_mm, \n         TF = sex == \"male\") %&gt;% \n  select(-c(flipper_length_mm, body_mass_g, year)) \n\n# A tibble: 344 × 7\n   species island    bill_length_mm bill_depth_mm sex    bill_size_mm2 TF   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;lgl&gt;\n 1 Adelie  Torgersen           39.1          18.7 male            731. TRUE \n 2 Adelie  Torgersen           39.5          17.4 female          687. FALSE\n 3 Adelie  Torgersen           40.3          18   female          725. FALSE\n 4 Adelie  Torgersen           NA            NA   &lt;NA&gt;             NA  NA   \n 5 Adelie  Torgersen           36.7          19.3 female          708. FALSE\n 6 Adelie  Torgersen           39.3          20.6 male            810. TRUE \n 7 Adelie  Torgersen           38.9          17.8 female          692. FALSE\n 8 Adelie  Torgersen           39.2          19.6 male            768. TRUE \n 9 Adelie  Torgersen           34.1          18.1 &lt;NA&gt;            617. NA   \n10 Adelie  Torgersen           42            20.2 &lt;NA&gt;            848. NA   \n# ℹ 334 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#combining-group_by-and-mutate",
    "href": "slides/day1-afternoon.html#combining-group_by-and-mutate",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Combining group_by() and mutate()",
    "text": "Combining group_by() and mutate()\n\nFirst, group data using group_by().\nThen, use drop_na() from tidyr to exclude rows with missing values.\nFinally, mutate to perform calculations for each group.\n\n\npenguins %&gt;%\n  drop_na() %&gt;% # Remove all non-complete rows\n  group_by(species) %&gt;%\n  mutate(body_mass_median = median(body_mass_g)) %&gt;% \n  select(-c(flipper_length_mm, body_mass_g, sex)) %&gt;% \n  head()\n\n# A tibble: 6 × 6\n# Groups:   species [1]\n  species island    bill_length_mm bill_depth_mm  year body_mass_median\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;            &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7  2007             3700\n2 Adelie  Torgersen           39.5          17.4  2007             3700\n3 Adelie  Torgersen           40.3          18    2007             3700\n4 Adelie  Torgersen           36.7          19.3  2007             3700\n5 Adelie  Torgersen           39.3          20.6  2007             3700\n6 Adelie  Torgersen           38.9          17.8  2007             3700"
  },
  {
    "objectID": "slides/day1-afternoon.html#conditional-calculations-in-mutate-with-if_else",
    "href": "slides/day1-afternoon.html#conditional-calculations-in-mutate-with-if_else",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Conditional Calculations in mutate() with if_else()",
    "text": "Conditional Calculations in mutate() with if_else()\n\nif_else() allows conditional logic within mutate().\nPerform different operations depending on conditions, like “big” or “small.”\n\n\nt &lt;- 800\npenguins %&gt;%\n  mutate(bill_size_mm2 = bill_depth_mm * bill_length_mm, \n         TF = sex == \"male\",\n         bill_size_binary = if_else(bill_size_mm2 &gt; t, \"big\", \"small\")) %&gt;% \n  select(-c(flipper_length_mm, body_mass_g, sex)) %&gt;% # too many cols to print\n  head()\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm  year bill_size_mm2 TF   \n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt; &lt;lgl&gt;\n1 Adelie  Torgersen           39.1          18.7  2007          731. TRUE \n2 Adelie  Torgersen           39.5          17.4  2007          687. FALSE\n3 Adelie  Torgersen           40.3          18    2007          725. FALSE\n4 Adelie  Torgersen           NA            NA    2007           NA  NA   \n5 Adelie  Torgersen           36.7          19.3  2007          708. FALSE\n6 Adelie  Torgersen           39.3          20.6  2007          810. TRUE \n# ℹ 1 more variable: bill_size_binary &lt;chr&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#summarizing-data-with-summarise",
    "href": "slides/day1-afternoon.html#summarizing-data-with-summarise",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Summarizing Data with summarise()",
    "text": "Summarizing Data with summarise()\n\nsummarise() reduces data to summary statistics (e.g., mean, median).\nTypically used after group_by() to summarize each group.\n\n\npenguins %&gt;%\n  drop_na() %&gt;%\n  group_by(species) %&gt;%\n  summarise(body_mass_median = median(body_mass_g))\n\n# A tibble: 3 × 2\n  species   body_mass_median\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie                3700\n2 Chinstrap             3700\n3 Gentoo                5050"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-summarise-with-multiple-calculations",
    "href": "slides/day1-afternoon.html#using-summarise-with-multiple-calculations",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using summarise() with Multiple Calculations",
    "text": "Using summarise() with Multiple Calculations\n\nUse summarise() to calculate multiple summary statistics at once.\nInclude multiple columns and functions in a single call.\n\n\npenguins %&gt;%\n  drop_na() %&gt;%\n  group_by(species) %&gt;%\n  summarise(body_mass_median = median(body_mass_g), \n            bill_depth_median = median(bill_depth_mm),\n            flipper_length_median = median(flipper_length_mm),\n            bill_length_mm = median(bill_length_mm))\n\n# A tibble: 3 × 5\n  species   body_mass_median bill_depth_median flipper_length_median\n  &lt;fct&gt;                &lt;dbl&gt;             &lt;dbl&gt;                 &lt;dbl&gt;\n1 Adelie                3700              18.4                   190\n2 Chinstrap             3700              18.4                   196\n3 Gentoo                5050              15                     216\n# ℹ 1 more variable: bill_length_mm &lt;dbl&gt;\n\n\n\nYikes! This can get long. Is there a way to condense this?"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-across-to-apply-functions-to-multiple-columns-in-one-swoop",
    "href": "slides/day1-afternoon.html#using-across-to-apply-functions-to-multiple-columns-in-one-swoop",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using across() to Apply Functions to Multiple Columns in one Swoop",
    "text": "Using across() to Apply Functions to Multiple Columns in one Swoop\n\nacross() applies a function (e.g., median) to multiple columns.\nIt’s especially useful for summarizing multiple numeric columns in one step.\n\n\npenguins %&gt;%\n  drop_na() %&gt;% \n  group_by(species) %&gt;%\n  summarise(across(where(is.numeric), median))\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.4               190        3700  2008\n2 Chinstrap           49.6          18.4               196        3700  2008\n3 Gentoo              47.4          15                 216        5050  2008"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-count-to-aggregate-data",
    "href": "slides/day1-afternoon.html#using-count-to-aggregate-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using count() to Aggregate Data",
    "text": "Using count() to Aggregate Data\n\ncount() is a shortcut for grouping and summarizing the data:\n\nFor example, if we want to count the number of penguins by species, then\n\npenguins_count &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(count = n())\n\nis equivalent to\n\npenguins_count &lt;- penguins %&gt;%\n  count(species)\n\npenguins_count # Let's see what the counts are.\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124"
  },
  {
    "objectID": "slides/day1-afternoon.html#tidy-data",
    "href": "slides/day1-afternoon.html#tidy-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Tidy Data",
    "text": "Tidy Data\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.” — Leo Tolstoy\n\n\nTidy datasets are like happy families: consistent, standardized, and easy to work with.\n\nMessy datasets are like unhappy families: each one messy in its own unique way.\nIn this section:\nWe’ll define what makes data tidy and how to transform between the tidy and messy formats."
  },
  {
    "objectID": "slides/day1-afternoon.html#what-is-tidy-data",
    "href": "slides/day1-afternoon.html#what-is-tidy-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What is Tidy Data?",
    "text": "What is Tidy Data?\n\nTidy data follows a consistent structure: each row represents one observation, and each column represents one variable.\n\n\n\n# Simple example of messy data (wide format)\npenguins_wide &lt;- tibble(\n  island = c(\"Biscoe\", \"Biscoe\", \"Dream\"),\n  year = c(2007, 2008, 2007),\n  Adelie = c(10, 18, 20),\n  Gentoo = c(34, 46, 0)\n)\npenguins_wide\n\n# A tibble: 3 × 4\n  island  year Adelie Gentoo\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0"
  },
  {
    "objectID": "slides/day1-afternoon.html#tidying-messy-data-with-pivot_longer",
    "href": "slides/day1-afternoon.html#tidying-messy-data-with-pivot_longer",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Tidying Messy Data with pivot_longer()",
    "text": "Tidying Messy Data with pivot_longer()\n\nTo turn messy data into tidy data, we often use the tidyr package in the tidyverse.\nUse pivot_longer() to convert data from wide format (multiple columns for the same variable) to long format (one column per variable).\nThis makes it easier to perform group-based calculations or create visualizations.\n\n\n\n# A tibble: 6 × 4\n  island  year species count\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Biscoe  2007 Adelie     10\n2 Biscoe  2007 Gentoo     34\n3 Biscoe  2008 Adelie     18\n4 Biscoe  2008 Gentoo     46\n5 Dream   2007 Adelie     20\n6 Dream   2007 Gentoo      0"
  },
  {
    "objectID": "slides/day1-afternoon.html#making-data-wider-with-pivot_wider",
    "href": "slides/day1-afternoon.html#making-data-wider-with-pivot_wider",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Making Data Wider with pivot_wider()",
    "text": "Making Data Wider with pivot_wider()\n\nSometimes, you need to convert data from long format to wide format using pivot_wider().\nThis can be useful when you want to separate variables into individual columns.\nLet’s try converting penguins_tidy back to penguins_wide!\n\n\n# Pivoting long data back to wide format\npenguins_wide_back &lt;- penguins_tidy %&gt;%\n  pivot_wider(names_from = species, values_from = count)\n\npenguins_wide_back\n\n# A tibble: 3 × 4\n  island  year Adelie Gentoo\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0"
  },
  {
    "objectID": "slides/day1-afternoon.html#complete-and-fill-to-handle-missing-data",
    "href": "slides/day1-afternoon.html#complete-and-fill-to-handle-missing-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "complete() and fill() to Handle Missing Data",
    "text": "complete() and fill() to Handle Missing Data\n\n\ncomplete(): Adds missing rows for combinations of specified variables.\nfill(): Fills missing values in columns, typically from previous or next available values (default is LOCF).\n\n\n\n# First, use complete() to add missing year (2008 for Dream)\npenguins_complete &lt;- penguins_wide %&gt;%\n  complete(island, year)\npenguins_complete\n\n# A tibble: 4 × 4\n  island  year Adelie Gentoo\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0\n4 Dream   2008     NA     NA\n\n\n\n# Then, use fill() to fill the missing penguin counts\npenguins_complete %&gt;%\n  fill(Adelie, Gentoo)\n\n# A tibble: 4 × 4\n  island  year Adelie Gentoo\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0\n4 Dream   2008     20      0"
  },
  {
    "objectID": "slides/day1-afternoon.html#introduction-to-joins-in-dplyr",
    "href": "slides/day1-afternoon.html#introduction-to-joins-in-dplyr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Introduction to Joins in dplyr",
    "text": "Introduction to Joins in dplyr\n\n\nJoining datasets is a powerful tool for combining info. from multiple sources.\nIn R, dplyr provides several functions to perform different types of joins.\nWe’ll demonstrate joining penguins_complete (our penguin counts dataset) with island_info (dataset containing additional info. about the islands).\n\n\n# Island information dataset\nisland_info &lt;- tibble(\n  island = c(\"Biscoe\", \"Dream\", \"Torgersen\"),\n  location = c(\"Antarctica\", \"Antarctica\", \"Antarctica\"),\n  region = c(\"West\", \"East\", \"East\")\n)\n\nisland_info\n\n# A tibble: 3 × 3\n  island    location   region\n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe    Antarctica West  \n2 Dream     Antarctica East  \n3 Torgersen Antarctica East  \n\n\n\nNotice that the island_info dataset includes an island, Torgersen, that is not in penguins_complete."
  },
  {
    "objectID": "slides/day1-afternoon.html#left-join-keep-all-rows-from-the-first-dataset",
    "href": "slides/day1-afternoon.html#left-join-keep-all-rows-from-the-first-dataset",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Left Join: Keep All Rows from the First Dataset",
    "text": "Left Join: Keep All Rows from the First Dataset\n\nA left join keeps all rows from the first dataset (penguins_complete), and adds matching data from the second dataset (island_info).\nSo all rows from the first dataset (penguins_complete) will be preserved.\nThe datasets are joined by matching the island column, specified by the by argument.\n\n\n# Left join: combining penguins data with island info\npenguins_with_info &lt;- penguins_complete %&gt;%\n  left_join(island_info, by = \"island\")\n\npenguins_with_info\n\n# A tibble: 4 × 6\n  island  year Adelie Gentoo location   region\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe  2007     10     34 Antarctica West  \n2 Biscoe  2008     18     46 Antarctica West  \n3 Dream   2007     20      0 Antarctica East  \n4 Dream   2008     NA     NA Antarctica East"
  },
  {
    "objectID": "slides/day1-afternoon.html#right-join-keep-all-rows-from-the-second-dataset",
    "href": "slides/day1-afternoon.html#right-join-keep-all-rows-from-the-second-dataset",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Right Join: Keep All Rows from the Second Dataset",
    "text": "Right Join: Keep All Rows from the Second Dataset\n\nA right join keeps all rows from the second dataset (island_info), and adds matching data from the first dataset (penguins_complete).\nIf a row in the second dataset doesn’t have a match in the first, then the columns from the first will be filled with NA.\nWe can see this for the Torgersen row from island_info…\n\n\n# Right join: keep all rows from island_info\npenguins_right_join &lt;- penguins_complete %&gt;%\n  right_join(island_info, by = \"island\")\n\npenguins_right_join\n\n# A tibble: 5 × 6\n  island     year Adelie Gentoo location   region\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe     2007     10     34 Antarctica West  \n2 Biscoe     2008     18     46 Antarctica West  \n3 Dream      2007     20      0 Antarctica East  \n4 Dream      2008     NA     NA Antarctica East  \n5 Torgersen    NA     NA     NA Antarctica East"
  },
  {
    "objectID": "slides/day1-afternoon.html#inner-join-only-keeping-matching-rows",
    "href": "slides/day1-afternoon.html#inner-join-only-keeping-matching-rows",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Inner Join: Only Keeping Matching Rows",
    "text": "Inner Join: Only Keeping Matching Rows\n\nAn inner join will only keep rows where there is a match in both datasets.\nSo, if an island in island_info does not have a corresponding entry in penguins_complete, then that row will be excluded.\n\n\n# Inner join: only matching rows are kept\npenguins_inner_join &lt;- penguins_complete %&gt;%\n  inner_join(island_info, by = \"island\")\n\npenguins_inner_join\n\n# A tibble: 4 × 6\n  island  year Adelie Gentoo location   region\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe  2007     10     34 Antarctica West  \n2 Biscoe  2008     18     46 Antarctica West  \n3 Dream   2007     20      0 Antarctica East  \n4 Dream   2008     NA     NA Antarctica East"
  },
  {
    "objectID": "slides/day1-afternoon.html#full-join-keeping-all-rows-from-both-datasets",
    "href": "slides/day1-afternoon.html#full-join-keeping-all-rows-from-both-datasets",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Full Join: Keeping All Rows from Both Datasets",
    "text": "Full Join: Keeping All Rows from Both Datasets\n\nA full join will keep all rows from both datasets.\nIf an island in either dataset has no match in the other, the missing values will be filled with NA.\n\n\n# Full join: keep all rows from both datasets\npenguins_full_join &lt;- penguins_complete %&gt;%\n  full_join(island_info, by = \"island\")\n\npenguins_full_join\n\n# A tibble: 5 × 6\n  island     year Adelie Gentoo location   region\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe     2007     10     34 Antarctica West  \n2 Biscoe     2008     18     46 Antarctica West  \n3 Dream      2007     20      0 Antarctica East  \n4 Dream      2008     NA     NA Antarctica East  \n5 Torgersen    NA     NA     NA Antarctica East"
  },
  {
    "objectID": "slides/day1-afternoon.html#summary-of-the-four-join-functions",
    "href": "slides/day1-afternoon.html#summary-of-the-four-join-functions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Summary of the Four Join Functions",
    "text": "Summary of the Four Join Functions\n\nLeft join: All rows from the left dataset and matching rows from the right dataset.\nRight join: All rows from the right dataset and matching rows from the left dataset.\nInner join: Only matching rows from both datasets.\nFull join: All rows from both datasets, with NA where no match exists."
  },
  {
    "objectID": "slides/day1-afternoon.html#final-thoughts-on-joins",
    "href": "slides/day1-afternoon.html#final-thoughts-on-joins",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Final thoughts on joins",
    "text": "Final thoughts on joins\n\nJoins are an essential part of data wrangling in R.\nThe choice of join depends on the analysis you need to perform:\n\nUse left joins when you want to keep all data from the first dataset.\nUse right joins when you want to keep all data from the second dataset.\nUse inner joins when you’re only interested in matching rows.\nUse full joins when you want to preserve all information from both datasets."
  },
  {
    "objectID": "slides/day1-afternoon.html#goodbye-palmer-penguins",
    "href": "slides/day1-afternoon.html#goodbye-palmer-penguins",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Goodbye palmer penguins",
    "text": "Goodbye palmer penguins\n\nWhat’s a penguin’s favorite tool?\n%&gt;% — to keep the fish moving, from one catch to the next! 🐧\n\n\nLogo from the palmerpenguins website"
  },
  {
    "objectID": "slides/day1-afternoon.html#epiverse-software-ecosystem",
    "href": "slides/day1-afternoon.html#epiverse-software-ecosystem",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Epiverse software ecosystem",
    "text": "Epiverse software ecosystem\n\n\n\n\n\n\nflowchart LR\nA(\"{epidatr}\") --&gt; C(\"{epiprocess}\")\nB(\"{epidatpy}\") --&gt; C\nD(\"{other sources}\") --&gt; C\nC --&gt; E(\"{epipredict}\")\n\nclassDef smallText fill:#fff,stroke:#000,stroke-width:1px,font-size:14px,font-weight:bold,color:royalblue;\n  class A,B,C,D,E smallText;"
  },
  {
    "objectID": "slides/day1-afternoon.html#what-is-panel-data",
    "href": "slides/day1-afternoon.html#what-is-panel-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What is panel data?",
    "text": "What is panel data?\n\n\nRecall that panel data, or longitudinal data, contain cross-sectional measurements of subjects over time.\nBuilt-in example: covid_case_death_rates dataset, which is a snapshot as of May 31, 2022 that contains daily state-wise measures of case_rate and death_rate for COVID-19 in 2021:\n\n\n\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n* &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 as        2020-12-31       0        0    \n5 az        2020-12-31      76.8      1.10 \n6 ca        2020-12-31      96.0      0.751\n\n\n\nQuestion: How do we store & work with such snapshots in the epiverse software ecosystem?\n\n\n\nepi_df: snapshot of a data set\n\na tibble that requires columns geo_value and time_value.\narbitrary additional columns containing measured values\nadditional keys to index (age_group, ethnicity, etc.)\n\n\n\n\n\n\n\nepi_df\n\n\nRepresents a snapshot that contains the most up-to-date values of the signal variables, as of a given time.\n\n\n\n\n\nepi_df: snapshot of a data set\n\n\n\n\n\n\nExample data object documentation, license, attribution\n\n\nNo documentation for ‘covid_case_death_rates’ in specified packages and libraries: you could try ‘??covid_case_death_rates’\n\n\n\n\n\nAn `epi_df` object, 20,496 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 20,496 × 4\n   geo_value time_value case_rate death_rate\n * &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 ak        2020-12-31      35.9      0.158\n 2 al        2020-12-31      65.1      0.438\n 3 ar        2020-12-31      66.0      1.27 \n 4 as        2020-12-31       0        0    \n 5 az        2020-12-31      76.8      1.10 \n 6 ca        2020-12-31      96.0      0.751\n 7 co        2020-12-31      35.8      0.649\n 8 ct        2020-12-31      52.1      0.819\n 9 dc        2020-12-31      31.0      0.601\n10 de        2020-12-31      64.3      0.912\n# ℹ 20,486 more rows\n\n\n\n\nSliding examples on epi_df\nGrowth rates\n\nedf &lt;- filter(edf, geo_value %in% c(\"ut\", \"ca\")) %&gt;%\n  group_by(geo_value) %&gt;%\n  mutate(gr_cases = growth_rate(time_value, case_rate, method = \"trend_filter\"))\n\n\n\n\nepi_archive: collection of epi_dfs\n\nfull version history of a data set\nacts like a bunch of epi_dfs — but stored compactly\nallows similar functionality as epi_df but using only data that would have been available at the time\n\n\n\n\n\n\n\nRevisions\n\n\nEpidemiology data gets revised frequently. (Happens in Economics as well.)\n\nWe may want to use the data as it looked in the past\nor we may want to examine the history of revisions.\n\n\n\n\n\n\nRevision patterns\n\n\n\nExample data object documentation, license, attribution\n\n\narchive_cases_dv_subset       package:epiprocess       R Documentation\n\nSubset of daily COVID-19 doctor visits and cases from 6 states in\narchive format\n\nDescription:\n\n     This data source is based on information about outpatient visits,\n     provided to us by health system partners, and also contains\n     confirmed COVID-19 cases based on reports made available by the\n     Center for Systems Science and Engineering at Johns Hopkins\n     University. This example data ranges from June 1, 2020 to December\n     1, 2021, issued on dates from June 1, 2020 to December 1, 2021. It\n     is limited to California, Florida, Texas, and New York.\n\n     It is used in the epiprocess 'compactify', 'epi_archive', and\n     advanced-use ('advanced') vignettes.\n\nUsage:\n\n     archive_cases_dv_subset\n     \nFormat:\n\n     An object of class 'epi_archive' of length 6.\n\nData dictionary:\n\n     The data in the 'epi_archive$DT' attribute has columns:\n\n     geo_value the geographic value associated with each row of\n          measurements.\n\n     time_value the time value associated with each row of\n          measurements.\n\n     version the time value specifying the version for each row of\n          measurements.\n\n     percent_cli percentage of doctor’s visits with CLI (COVID-like\n          illness) computed from medical insurance claims\n\n     case_rate_7d_av 7-day average signal of number of new confirmed\n          cases due to COVID-19 per 100,000 population, daily\n\nSource:\n\n     This object contains a modified part of the COVID-19 Data\n     Repository by the Center for Systems Science and Engineering\n     (CSSE) at Johns Hopkins University as republished in the COVIDcast\n     Epidata API. This data set is licensed under the terms of the\n     Creative Commons Attribution 4.0 International license by Johns\n     Hopkins University on behalf of its Center for Systems Science in\n     Engineering. Copyright Johns Hopkins University 2020.\n\n     Modifications:\n\n        • From the COVIDcast Epidata API: 'case_rate_7d_av' signal was\n          computed by Delphi from the original JHU-CSSE data by\n          calculating moving averages of the preceding 7 days, so the\n          signal for June 7 is the average of the underlying data for\n          June 1 through 7, inclusive.\n\n        • Furthermore, the data has been limited to a very small number\n          of rows, the signal names slightly altered, and formatted\n          into an 'epi_archive'.\n\n     This object contains a modified part of the Delphi 'doctor-visits'\n     indicator. This data source is computed by the Delphi Group from\n     information about outpatient visits, provided to Delphi by health\n     system partners, and published in the COVIDcast Epidata API. This\n     data set is licensed under the terms of the Creative Commons\n     Attribution 4.0 International license by the Delphi group.\n\n     Modifications:\n\n        • From the COVIDcast Doctor Visits signal: The signal\n          'smoothed_adj_cli' is taken directly from the API without\n          changes.\n\n        • Furthermore, the data has been limited to a very small number\n          of rows, the signal names slightly altered, and formatted\n          into an 'epi_archive'.\n\nExamples:\n\n     # Since this is a re-exported dataset, it cannot be loaded using\n     # the `data()` function. `data()` looks for a file of the same name\n     # in the `data/` directory, which doesn't exist in this package.\n     # works\n     epiprocess::archive_cases_dv_subset\n     \n     # works\n     library(epiprocess)\n     archive_cases_dv_subset\n     \n     # fails\n     ## Not run:\n     \n     data(archive_cases_dv_subset, package = \"epiprocess\")\n     ## End(Not run)\n     \n\n\n\n\n\n\n\nFinalized data\n\n\nCounts are revised as time proceeds\nWant to know the final value\nOften not available until weeks/months later\n\n\n\nForecasting\n\nAt time \\(t\\), predict the final value for time \\(t+h\\), \\(h &gt; 0\\)\n\n\n\n\nBackcasting\n\nAt time \\(t\\), predict the final value for time \\(t-h\\), \\(h &lt; 0\\)\n\n\n\n\nNowcasting\n\nAt time \\(t\\), predict the final value for time \\(t\\)\n\n\n\n\nNowcasting with one time series\n\n\n\nBackfill Canadian edition\n\nEvery week the BC CDC releases COVID-19 hospitalization data.\nFollowing week they revise the number upward (by ~25%) due to lagged reports.\n\n \n\nTakeaway: Once the data is backfilled, hospitalizations rarely show a decline, challenging the common media narrative.\n\n\n\nAside on Nowcasting\n\nTo some Epis, “nowcasting” can be equated with “estimate the time-varying instantaneous reproduction number, \\(R_t\\)”\nExample using the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. \n\n\n\n\n\n\n\n\n\n\n\nGroup built {rtestim} doing for this nonparametrically.\nWe may come back to this later…\n\n\n\nMathematical setup\n\nSuppose today is time \\(t\\)\nLet \\(y_i\\) denote a series of interest observed at times \\(i=1,\\ldots, t\\).\n\n\n\n\nOur goal\n\n\n\nProduce a point nowcast for the finalized values of \\(y_t\\).\nAccompany with time-varying prediction intervals\n\n\n\n\n\nWe also have access to \\(p\\) other time series \\(x_{ij},\\; i=1,\\ldots,t, \\; j = 1,\\ldots,p\\)\nAll may be subject to revisions.\n\n\n\nFinal slide\nThanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nExplore, clean & transform data — cmu-delphi/insightnet-workshop-2024"
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_df-snapshot-of-a-data-set",
    "href": "slides/day1-afternoon.html#epi_df-snapshot-of-a-data-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "epi_df: snapshot of a data set",
    "text": "epi_df: snapshot of a data set\n\na tibble that requires columns geo_value and time_value.\narbitrary additional columns containing measured values\nadditional keys to index (age_group, ethnicity, etc.)\n\n\n\n\n\n\n\nepi_df\n\n\nRepresents a snapshot that contains the most up-to-date values of the signal variables, as of a given time."
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_df-snapshot-of-a-data-set-1",
    "href": "slides/day1-afternoon.html#epi_df-snapshot-of-a-data-set-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "epi_df: snapshot of a data set",
    "text": "epi_df: snapshot of a data set\n\n\n\n\n\n\nExample data object documentation, license, attribution\n\n\nNo documentation for ‘covid_case_death_rates’ in specified packages and libraries: you could try ‘??covid_case_death_rates’\n\n\n\n\n\nAn `epi_df` object, 20,496 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 20,496 × 4\n   geo_value time_value case_rate death_rate\n * &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 ak        2020-12-31      35.9      0.158\n 2 al        2020-12-31      65.1      0.438\n 3 ar        2020-12-31      66.0      1.27 \n 4 as        2020-12-31       0        0    \n 5 az        2020-12-31      76.8      1.10 \n 6 ca        2020-12-31      96.0      0.751\n 7 co        2020-12-31      35.8      0.649\n 8 ct        2020-12-31      52.1      0.819\n 9 dc        2020-12-31      31.0      0.601\n10 de        2020-12-31      64.3      0.912\n# ℹ 20,486 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#sliding-examples-on-epi_df",
    "href": "slides/day1-afternoon.html#sliding-examples-on-epi_df",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Sliding examples on epi_df",
    "text": "Sliding examples on epi_df\nGrowth rates\n\nedf &lt;- filter(edf, geo_value %in% c(\"ut\", \"ca\")) %&gt;%\n  group_by(geo_value) %&gt;%\n  mutate(gr_cases = growth_rate(time_value, case_rate, method = \"trend_filter\"))"
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs",
    "href": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "epi_archive: collection of epi_dfs",
    "text": "epi_archive: collection of epi_dfs\n\nfull version history of a data set\nacts like a bunch of epi_dfs — but stored compactly\nallows similar functionality as epi_df but using only data that would have been available at the time\n\n\n\n\n\n\n\nRevisions\n\n\nEpidemiology data gets revised frequently. (Happens in Economics as well.)\n\nWe may want to use the data as it looked in the past\nor we may want to examine the history of revisions."
  },
  {
    "objectID": "slides/day1-afternoon.html#revision-patterns",
    "href": "slides/day1-afternoon.html#revision-patterns",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Revision patterns",
    "text": "Revision patterns\n\n\n\nExample data object documentation, license, attribution\n\n\narchive_cases_dv_subset       package:epiprocess       R Documentation\n\nSubset of daily COVID-19 doctor visits and cases from 6 states in\narchive format\n\nDescription:\n\n     This data source is based on information about outpatient visits,\n     provided to us by health system partners, and also contains\n     confirmed COVID-19 cases based on reports made available by the\n     Center for Systems Science and Engineering at Johns Hopkins\n     University. This example data ranges from June 1, 2020 to December\n     1, 2021, issued on dates from June 1, 2020 to December 1, 2021. It\n     is limited to California, Florida, Texas, and New York.\n\n     It is used in the epiprocess 'compactify', 'epi_archive', and\n     advanced-use ('advanced') vignettes.\n\nUsage:\n\n     archive_cases_dv_subset\n     \nFormat:\n\n     An object of class 'epi_archive' of length 6.\n\nData dictionary:\n\n     The data in the 'epi_archive$DT' attribute has columns:\n\n     geo_value the geographic value associated with each row of\n          measurements.\n\n     time_value the time value associated with each row of\n          measurements.\n\n     version the time value specifying the version for each row of\n          measurements.\n\n     percent_cli percentage of doctor’s visits with CLI (COVID-like\n          illness) computed from medical insurance claims\n\n     case_rate_7d_av 7-day average signal of number of new confirmed\n          cases due to COVID-19 per 100,000 population, daily\n\nSource:\n\n     This object contains a modified part of the COVID-19 Data\n     Repository by the Center for Systems Science and Engineering\n     (CSSE) at Johns Hopkins University as republished in the COVIDcast\n     Epidata API. This data set is licensed under the terms of the\n     Creative Commons Attribution 4.0 International license by Johns\n     Hopkins University on behalf of its Center for Systems Science in\n     Engineering. Copyright Johns Hopkins University 2020.\n\n     Modifications:\n\n        • From the COVIDcast Epidata API: 'case_rate_7d_av' signal was\n          computed by Delphi from the original JHU-CSSE data by\n          calculating moving averages of the preceding 7 days, so the\n          signal for June 7 is the average of the underlying data for\n          June 1 through 7, inclusive.\n\n        • Furthermore, the data has been limited to a very small number\n          of rows, the signal names slightly altered, and formatted\n          into an 'epi_archive'.\n\n     This object contains a modified part of the Delphi 'doctor-visits'\n     indicator. This data source is computed by the Delphi Group from\n     information about outpatient visits, provided to Delphi by health\n     system partners, and published in the COVIDcast Epidata API. This\n     data set is licensed under the terms of the Creative Commons\n     Attribution 4.0 International license by the Delphi group.\n\n     Modifications:\n\n        • From the COVIDcast Doctor Visits signal: The signal\n          'smoothed_adj_cli' is taken directly from the API without\n          changes.\n\n        • Furthermore, the data has been limited to a very small number\n          of rows, the signal names slightly altered, and formatted\n          into an 'epi_archive'.\n\nExamples:\n\n     # Since this is a re-exported dataset, it cannot be loaded using\n     # the `data()` function. `data()` looks for a file of the same name\n     # in the `data/` directory, which doesn't exist in this package.\n     # works\n     epiprocess::archive_cases_dv_subset\n     \n     # works\n     library(epiprocess)\n     archive_cases_dv_subset\n     \n     # fails\n     ## Not run:\n     \n     data(archive_cases_dv_subset, package = \"epiprocess\")\n     ## End(Not run)"
  },
  {
    "objectID": "slides/day1-afternoon.html#finalized-data",
    "href": "slides/day1-afternoon.html#finalized-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Finalized data",
    "text": "Finalized data\n\n\nCounts are revised as time proceeds\nWant to know the final value\nOften not available until weeks/months later\n\n\n\nForecasting\n\nAt time \\(t\\), predict the final value for time \\(t+h\\), \\(h &gt; 0\\)\n\n\n\n\nBackcasting\n\nAt time \\(t\\), predict the final value for time \\(t-h\\), \\(h &lt; 0\\)\n\n\n\n\nNowcasting\n\nAt time \\(t\\), predict the final value for time \\(t\\)"
  },
  {
    "objectID": "slides/day1-afternoon.html#backfill-canadian-edition",
    "href": "slides/day1-afternoon.html#backfill-canadian-edition",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Backfill Canadian edition",
    "text": "Backfill Canadian edition\n\nEvery week the BC CDC releases COVID-19 hospitalization data.\nFollowing week they revise the number upward (by ~25%) due to lagged reports.\n\n \n\nTakeaway: Once the data is backfilled, hospitalizations rarely show a decline, challenging the common media narrative."
  },
  {
    "objectID": "slides/day1-afternoon.html#aside-on-nowcasting",
    "href": "slides/day1-afternoon.html#aside-on-nowcasting",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Aside on Nowcasting",
    "text": "Aside on Nowcasting\n\nTo some Epis, “nowcasting” can be equated with “estimate the time-varying instantaneous reproduction number, \\(R_t\\)”\nExample using the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. \n\n\n\n\n\n\n\n\n\n\n\nGroup built {rtestim} doing for this nonparametrically.\nWe may come back to this later…"
  },
  {
    "objectID": "slides/day1-afternoon.html#mathematical-setup",
    "href": "slides/day1-afternoon.html#mathematical-setup",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Mathematical setup",
    "text": "Mathematical setup\n\nSuppose today is time \\(t\\)\nLet \\(y_i\\) denote a series of interest observed at times \\(i=1,\\ldots, t\\).\n\n\n\n\nOur goal\n\n\n\nProduce a point nowcast for the finalized values of \\(y_t\\).\nAccompany with time-varying prediction intervals\n\n\n\n\n\nWe also have access to \\(p\\) other time series \\(x_{ij},\\; i=1,\\ldots,t, \\; j = 1,\\ldots,p\\)\nAll may be subject to revisions."
  },
  {
    "objectID": "slides/day1-afternoon.html#final-slide",
    "href": "slides/day1-afternoon.html#final-slide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Final slide",
    "text": "Final slide\nThanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nExplore, clean & transform data — cmu-delphi/insightnet-workshop-2024"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Schedule",
    "text": "Schedule\nShort description\n\nDay 1 Morning\nDay 1 Afternoon\nDay 2 Morning\nDay 2 Afternoon"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Instructors",
    "text": "Instructors\n\nRyan J. Tibshirani\nDaniel J. McDonald\nAlice Cima\nRachel Lobay"
  },
  {
    "objectID": "slides/day2-afternoon.html#section",
    "href": "slides/day2-afternoon.html#section",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Forecasting with {epipredict}",
    "text": "Forecasting with {epipredict}\n\n\n\n\nVenue – dd Somemonth yyyy"
  },
  {
    "objectID": "slides/day2-afternoon.html#outline",
    "href": "slides/day2-afternoon.html#outline",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Outline",
    "text": "Outline\n\n{epipredict}\nARX Models\nForecasting with Versioned Data"
  },
  {
    "objectID": "slides/day2-afternoon.html#fit-arx-on-training-set",
    "href": "slides/day2-afternoon.html#fit-arx-on-training-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fit ARX on training set",
    "text": "Fit ARX on training set\n\n# Split into train and test (before/after t0_date)\nt0_date &lt;- as.Date('2021-03-01')\ntrain &lt;- ca %&gt;% filter(time_value &lt;= t0_date)\ntest &lt;- ca %&gt;% filter(time_value &gt; t0_date)\n\nh &lt;- 28 \n\nepi_arx &lt;- arx_forecaster(epi_data = train %&gt;% as_epi_df(), \n                          outcome = \"deaths\", \n                          predictors = c(\"cases\", \"deaths\"),\n                          trainer = linear_reg() %&gt;% set_engine(\"lm\"),\n                          args_list = arx_args_list(lags = 0, \n                                                    ahead = h))\nepi_arx"
  },
  {
    "objectID": "slides/day2-afternoon.html#extract-workflow",
    "href": "slides/day2-afternoon.html#extract-workflow",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Extract workflow",
    "text": "Extract workflow\n\nepi_arx$epi_workflow\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n (Intercept)   lag_0_cases  lag_0_deaths  \n     0.08133       0.01030       0.14011"
  },
  {
    "objectID": "slides/day2-afternoon.html#extract-prediction",
    "href": "slides/day2-afternoon.html#extract-prediction",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Extract prediction",
    "text": "Extract prediction\n\nepi_arx$predictions\n\n# A tibble: 1 × 5\n  geo_value .pred        .pred_distn forecast_date target_date\n  &lt;chr&gt;     &lt;dbl&gt;             &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n1 ca        0.368 quantiles(0.37)[2] 2021-03-01    2021-03-29 \n\n\n\nepi_arx$predictions %&gt;%\n  # first create a \"nested\" list-column\n  mutate(.pred_distn = nested_quantiles(.pred_distn)) %&gt;%\n  unnest(.pred_distn) # then unnest it\n\n# A tibble: 2 × 6\n  geo_value .pred values quantile_levels forecast_date target_date\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt; &lt;date&gt;        &lt;date&gt;     \n1 ca        0.368  0.224            0.05 2021-03-01    2021-03-29 \n2 ca        0.368  0.513            0.95 2021-03-01    2021-03-29"
  },
  {
    "objectID": "slides/day2-afternoon.html#arx-on-trailing-window",
    "href": "slides/day2-afternoon.html#arx-on-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX on trailing window",
    "text": "ARX on trailing window\n\n# Specify the forecast dates\nfc_time_values &lt;- seq(\n  from = t0_date,\n  to = tail(ca$time_value, 1) - h,\n  by = \"1 day\"\n)\n\nw &lt;- 200  # trailing window length\n\n# Slide an arx_forecaster with appropriate outcome, predictions, lags, and trailing window\nepi_pred_cv_trailing &lt;- ca %&gt;%\n  epi_slide(\n    ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\", \n                   predictors = c(\"cases\", \"deaths\"), \n                   trainer = linear_reg() %&gt;% set_engine(\"lm\"),\n                   args_list = arx_args_list(lags = 0, ahead = h)\n                   )$predictions %&gt;%\n      pivot_quantiles_wider(.pred_distn),\n  # notice that `before` is not simply equal to w-1. That's because previously, \n  # when considering a window from t to t+w, we had access to y_t, ..., y_{t+w} \n  # and also to x_{t-h}, ..., x_{t+w-h}. (That's because of how we structured \n  # the dataframe after manually lagging x.) So we were \"cheating\" by saying that \n  # the trailing window had length w, as its actual size was w+h! \n  .window_size = (w+h-1), \n  .ref_time_values = fc_time_values,\n  .new_col_name = \"fc\"\n)\n\n# they match exactly\nhead(epi_pred_cv_trailing %&gt;% select(fc))\nhead(test %&gt;% select(pred_trailing_cv, time_value))\n\nThe method fitting on all past data up to the forecasting date can be implemented by changing before = Inf in epi_slide."
  },
  {
    "objectID": "slides/day2-afternoon.html#thanks",
    "href": "slides/day2-afternoon.html#thanks",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Thanks:",
    "text": "Thanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nForecasting with {epipredict} — cmu-delphi/insightnet-workshop-2024"
  },
  {
    "objectID": "slides/day1-morning.html#section",
    "href": "slides/day1-morning.html#section",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Signal Discovery & Data Fetching",
    "text": "Signal Discovery & Data Fetching"
  },
  {
    "objectID": "slides/day1-morning.html#outline",
    "href": "slides/day1-morning.html#outline",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Outline",
    "text": "Outline\n\nAbout Delphi\nGoals of Delphi Epidata platform\nEpidata API\nFinding data sources and signals\nVersioned data"
  },
  {
    "objectID": "slides/day1-morning.html#about-delphi",
    "href": "slides/day1-morning.html#about-delphi",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "About Delphi",
    "text": "About Delphi\n\nFounded in 2012 at Carnegie Mellon University, now expanded to UC Berkeley, and University of British Columbia.\nCurrently 5 faculty, ~10 PhD students, ~15 staff (mostly software engineers).\nEasy to join us from anywhere (lots of volunteers during Covid-19 pandemic).\nWe are:\n\nCDC Center of Excellence for Influenza and Covid-19 Forecasting (2019-24).\nCDC Innovation Center for Outbreak Analytics and Disease Modeling (2024-29).\n\n\nOur mission: To develop the theory and practice of epidemic detection, tracking and forecasting, and their use in decision making, both public and private."
  },
  {
    "objectID": "slides/day1-morning.html#what-does-delphi-do",
    "href": "slides/day1-morning.html#what-does-delphi-do",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What does Delphi do?",
    "text": "What does Delphi do?\n\nProcure real-time, aggregated data streams informative of infectious diseases and syndromes, in collaboration with partners in industry and government.\nExtract signals and make them widely available via the Epidata platform & API.\nDevelop and deploy algorithms for epidemic detection, tracking, forecasting.\nDevelop and maintain statistical software packages for these tasks.\nMake it all production-grade, maximally-accessible, and open-source (to serve CDC, state and local public health agencies, epi-forecasting researchers, data journalists, the public)"
  },
  {
    "objectID": "slides/day1-morning.html#what-we-provide",
    "href": "slides/day1-morning.html#what-we-provide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What we provide",
    "text": "What we provide"
  },
  {
    "objectID": "slides/day1-morning.html#goals-of-delphi-epidata-platform-and-repository",
    "href": "slides/day1-morning.html#goals-of-delphi-epidata-platform-and-repository",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Goals of Delphi Epidata platform and repository",
    "text": "Goals of Delphi Epidata platform and repository\n\n\nBe the one-stop shop for aggregated epi-surveillance time-series (“epi-signals”)\n\nHence: include also signals available elsewhere, especially if they don’t keep data revisions - E.g. CDC’s own NSSP, NWSS\nBe the national historical repository of record & preserve the raw data\n\nBe the national clearinghouse for epi-signals, including those held elsewhere\n\nThe go-to place for signal discovery\n\nAdd value to existing signals and synthesize new ones\n\nAdded value: see next slide\nSynthesize new: via signal fusion, e.g. nowcasting\n\nBe the focal point for community-wide efforts to open up privately held data\n\nBetter positioned than government or industry"
  },
  {
    "objectID": "slides/day1-morning.html#the-bigger-goal",
    "href": "slides/day1-morning.html#the-bigger-goal",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "The bigger goal",
    "text": "The bigger goal\nThe goal is to make epi-surveillance more nimble, complete, standardized, robust, and real-time; and less burdensome on the health system itself. Epidata is not the solution; but we hope it is a blueprint towards such a solution."
  },
  {
    "objectID": "slides/day1-morning.html#what-is-the-epidata-repository",
    "href": "slides/day1-morning.html#what-is-the-epidata-repository",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What is the Epidata repository",
    "text": "What is the Epidata repository\nEpidata is a repository of aggregated epi-surveillance time series. To the full extent we can, we make everything free and open-source.\n\nTo date, it has accumulated over 5 billion records (each record is the value of a signal, at a particular date, and a particular location).\nAt the peak of the pandemic, we were receiving millions of API queries per day.\nData comes from: public health reporting, medical insurance claims, medical device data, Google search queries, wastewater, app-based mobility patterns.\nMany of our data streams simply aren’t available anywhere else.\nAdded value we provide: revision tracking, anomaly detection, trend detection, smoothing, imputation, geo-temporal-demographic disaggregation"
  },
  {
    "objectID": "slides/day1-morning.html#features-of-delphi-epidata",
    "href": "slides/day1-morning.html#features-of-delphi-epidata",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Features of Delphi Epidata",
    "text": "Features of Delphi Epidata\n\n\nBuilt-in support for:\n\nData revisions (“backfill”). Concepts of “reporting date” and “as of”.\nBackfill projection and alerting to changes in backfill dynamics\nGeo levels w/ auto-aggregation: county, MSA, HRR, state, HHS region, nation\n\nAlso esoteric ones: DMA, sewer sheds\n\nDemographic breakdown\nRepresentation for missingness and censoring\nPopulation sizes and fine-grained population density\n\nPre-computed smoothing and normalization (customization planned)\nAccess control\nCode is Open Source. Signals are as accessible (w/ API, SDK) as allowed by DUAs"
  },
  {
    "objectID": "slides/day1-morning.html#epidata-api-1",
    "href": "slides/day1-morning.html#epidata-api-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Epidata API",
    "text": "Epidata API\n\nDelphi’s Epidata API provides real-time access to epidemiological surveillance data.\nThe main endpoint (covidcast) providing daily updates about current COVID-19 and influenza activity across the United States.\nA variety of other endpoints, providing primarily historical data about various diseases including COVID-19, influenza, dengue fever, and norovirus in several countries.\nA full-featured R client is available for quick access to all data.\nA Legacy Python client is available, full-featured Python client in development."
  },
  {
    "objectID": "slides/day1-morning.html#some-of-our-data-sources",
    "href": "slides/day1-morning.html#some-of-our-data-sources",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Some of our data sources",
    "text": "Some of our data sources\n\nOngoing Sources:\n\nInsurance claims: %Covid {inpatient, outpatient}, by {county x day}\nGoogle Symptom searches: 7 symptoms groups, by {county x day}\nQuidel/Ortho antigen tests: %Covid by age group, by {county x day}\nNCHS Deaths: all-cause, pneumonia, flu, Covid, by {state x week}\nNSSP ED visits: %Covid, %flu, %RSV, by {county x week} (new!)\nNWSS Covid, by {sampling-site x day} (in progress)"
  },
  {
    "objectID": "slides/day1-morning.html#some-of-our-data-sources-1",
    "href": "slides/day1-morning.html#some-of-our-data-sources-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Some of our data sources",
    "text": "Some of our data sources\n\nActive during the pandemic, and could be restarted for the next PHE:\n\nHHS Hosp/ICU beds: Covid, flu, by age-group, by {state x day}, {facility x week}\nCTIS (“Delphi Facebook Survey”): many dozens of questions, by (county x day)\nSTLT-reported {cases, deaths} via {JHU, USAFacts}, by (country x day)\nSafegraph mobility: misc measures by {county x day},{county x week}"
  },
  {
    "objectID": "slides/day1-morning.html#some-of-our-pre-pandemic-data-sources",
    "href": "slides/day1-morning.html#some-of-our-pre-pandemic-data-sources",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Some of our Pre-Pandemic Data Sources",
    "text": "Some of our Pre-Pandemic Data Sources\n\n\nFluView ILINet, by {state x week}\nFluView Clinical (% positive flu, PH and clinical labs)\nGoogle Health Trends (GHT), precursor to Google Symptoms\nGoogle Flu Trends (GFT), precursor to to GHT\nTwitter flu\nAccess counts for flu-related CDC pages, by {city x week}\nAccess counts for flu-related Wikipedia entries by {day x hour}\nFlu-surv (flu hosp rates, now expanded to RESP-NET)\nMisc signals for dengue, norovirus\nMisc signals for PAHO countries, ECDC, KCDC, Taiwan,…\nDelphi ILI nowcasts, by {state x week}, visualized in “ILI Nearby” website\nDelphi ILI forecasts, by {state x week}"
  },
  {
    "objectID": "slides/day1-morning.html#severity-pyramid",
    "href": "slides/day1-morning.html#severity-pyramid",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Severity pyramid",
    "text": "Severity pyramid"
  },
  {
    "objectID": "slides/day1-morning.html#installing-epidatr",
    "href": "slides/day1-morning.html#installing-epidatr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Installing epidatr",
    "text": "Installing epidatr\nInstalling the package is straightforward:\n\n# Install the CRAN version\npak::pkg_install(\"epidatr\")\n# Install the development version from the GitHub dev branch\n# pak::pkg_install(\"cmu-delphi/epidatr@dev\")\n\nThe CRAN listing is here.\n\n\n\nPython\n\n\nIn Python, install delphi-epidata from PyPI with pip install delphi-epidata."
  },
  {
    "objectID": "slides/day1-morning.html#using-epidatr-and-epidatpy",
    "href": "slides/day1-morning.html#using-epidatr-and-epidatpy",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using epidatr and epidatpy",
    "text": "Using epidatr and epidatpy\n\nThe following shows how to import the library and fetch Delphi’s COVID-19 Surveillance Streams from Facebook Survey CLI for county 06001:\n\n\n# Configure API key interactively, if needed. See\n# https://cmu-delphi.github.io/epidatr/articles/epidatr.html#api-keys for details.\n#save_api_key()\nlibrary(epidatr)\nres &lt;- pub_covidcast('fb-survey', 'smoothed_cli', 'county', 'day', geo_values = '06001',\n                     time_values = c(20200401, 20200405:20200414))\nhead(res)\n\n# A tibble: 6 × 15\n  geo_value signal     source geo_type time_type time_value direction issue     \n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;    \n1 06001     smoothed_… fb-su… county   day       2020-04-06        NA 2020-09-03\n2 06001     smoothed_… fb-su… county   day       2020-04-07        NA 2020-09-03\n3 06001     smoothed_… fb-su… county   day       2020-04-08        NA 2020-09-03\n4 06001     smoothed_… fb-su… county   day       2020-04-09        NA 2020-09-03\n5 06001     smoothed_… fb-su… county   day       2020-04-10        NA 2020-09-03\n6 06001     smoothed_… fb-su… county   day       2020-04-11        NA 2020-09-03\n# ℹ 7 more variables: lag &lt;dbl&gt;, missing_value &lt;dbl&gt;, missing_stderr &lt;dbl&gt;,\n#   missing_sample_size &lt;dbl&gt;, value &lt;dbl&gt;, stderr &lt;dbl&gt;, sample_size &lt;dbl&gt;"
  },
  {
    "objectID": "slides/day1-morning.html#api-keys",
    "href": "slides/day1-morning.html#api-keys",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "API keys",
    "text": "API keys\n\n\nAnyone may access the Epidata API anonymously without providing any personal data.\nAnonymous API access is subject to the following restrictions:\n\npublic datasets only\nrate limited to 60 requests per hour\nonly two parameters may have multiple selections\n\nAn API key grants priviledged access to the Epidata API and can be obtained by registering with us.\nPrivileges of registration:\n\nno rate limit\nno limit on multiple selections\n\n\n\n\n\n\n\n\n\nTip\n\n\nThe epidatr client automatically searches for the key in the DELPHI_EPIDATA_KEY environment variable. We recommend storing it in your .Renviron file, which R reads by default. More on setting your API key here."
  },
  {
    "objectID": "slides/day1-morning.html#finding-data-sources-and-signals-of-interest",
    "href": "slides/day1-morning.html#finding-data-sources-and-signals-of-interest",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Finding data sources and signals of interest",
    "text": "Finding data sources and signals of interest\n\n\nDiverse Data Streams\n\nVariety of Data: Access to medical claims data, cases and deaths, mobility data, and more.\nGeographic Coverage: Includes multiple regions, making it comprehensive yet complex.\nChallenge: Difficulty in pinpointing the specific data stream of interest.\n\nUsing the Documentation\n\nComprehensive Listings: Documentation details all available data sources and signals for both COVID-19 and other endpoints.\n\nDocs are great for a deep dive into the data, whereas the apps & tools are useful to see what is available…"
  },
  {
    "objectID": "slides/day1-morning.html#cheatsheet-of-tools-we-provide",
    "href": "slides/day1-morning.html#cheatsheet-of-tools-we-provide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Cheatsheet of tools we provide",
    "text": "Cheatsheet of tools we provide\nWe provide…\n\nA signal discovery app, to explore what epi-signals are available in Delphi Epidata and elsewhere in the community.\nA general signal visualization tool.\nA signal dashboard and a “classic” map-based version to visualize a core set of COVID-19 and flu indicators.\nA COVID-19 signal export app, a dashboard builder, and more!"
  },
  {
    "objectID": "slides/day1-morning.html#signal-dashboard---for-covid-19-flu-data",
    "href": "slides/day1-morning.html#signal-dashboard---for-covid-19-flu-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Signal dashboard - For COVID-19 & flu data",
    "text": "Signal dashboard - For COVID-19 & flu data\n\n\n\nThe signal dashboard displays a selection of signals for COVID-19 & flu.\nBrowse by location or indicator to choose which signal you are interested in & then export the data for analysis.\nExample: Symptom searches on Google in NC"
  },
  {
    "objectID": "slides/day1-morning.html#signal-discovery-app---browse-for-more-data",
    "href": "slides/day1-morning.html#signal-discovery-app---browse-for-more-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Signal discovery app - Browse for more data",
    "text": "Signal discovery app - Browse for more data\n\nSignal discovery app: An easy way to find data sources and signals (no programming required).\n\nSearch tool that is a good to browse & find data.\n\nLet’s try it out together!"
  },
  {
    "objectID": "slides/day1-morning.html#example---nchs-weekly-flu-mortality-data-in-states",
    "href": "slides/day1-morning.html#example---nchs-weekly-flu-mortality-data-in-states",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example - NCHS Weekly Flu Mortality Data in States",
    "text": "Example - NCHS Weekly Flu Mortality Data in States"
  },
  {
    "objectID": "slides/day1-morning.html#interactive-tooling",
    "href": "slides/day1-morning.html#interactive-tooling",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Interactive tooling",
    "text": "Interactive tooling\n\nOther main way to find data sources and signals in R… Functions to enhance data discovery in epidatr:\n\navail_endpoints() Function:\n\nLists all endpoints with brief descriptions.\nHighlights specific endpoints that cover non-US locations, facilitating targeted searches.\n\nOutput Format: Returns a tibble for easy viewing and analysis of available data sources.\n\n\n\navail_endpoints()"
  },
  {
    "objectID": "slides/day1-morning.html#using-the-covidcast_epidata-function",
    "href": "slides/day1-morning.html#using-the-covidcast_epidata-function",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using the covidcast_epidata() function",
    "text": "Using the covidcast_epidata() function\n\n\nIn-Depth Data Exploration\n\nFunction Overview: covidcast_epidata() provides detailed insights into data sources from the COVIDcast endpoint.\nSource List: Each data source is listed in covid_sources$sources, with associated tibbles describing included signals.\n\nTab Completion for Ease of Use\n\nEditor Support: In RStudio or similar editors, use tab completion to explore:\n\nData Sources: Type covid_sources$source$ to view available data sources.\nSignals: Type covid_sources$signals$ to see signal options with autocomplete assistance.\n\n\nFiltering Convenience: Signal names are prefixed with their respective data source for easier navigation.\n\n\n\ncovid_sources &lt;- covidcast_epidata()\nhead(covid_sources$sources, n = 2) # head(list, n = 2) will print the first two elements of the list"
  },
  {
    "objectID": "slides/day1-morning.html#fetching-data---covidcast-main-endpoint",
    "href": "slides/day1-morning.html#fetching-data---covidcast-main-endpoint",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetching data - COVIDcast main endpoint",
    "text": "Fetching data - COVIDcast main endpoint\n\n\nFetching data from the Delphi Epidata API is simple.\nThe pub_covidcast() function lets us access the covidcast endpoint.\nWe need to specify the following six arguments…\n\nsource: Data source name\nsignals: Signal name\ngeo_type: Geographic level\ntime_type: Time resolution\ngeo_values: Location(s)\ntime_values: times of interest\n\nLet’s give this a try!"
  },
  {
    "objectID": "slides/day1-morning.html#fetching-data---covidcast-main-endpoint-1",
    "href": "slides/day1-morning.html#fetching-data---covidcast-main-endpoint-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetching data - COVIDcast main endpoint",
    "text": "Fetching data - COVIDcast main endpoint\n\n\nlibrary(epidatr)\nlibrary(dplyr)\n\n\n# Obtain the most up-to-date version of the smoothed covid-like illness (CLI)\n# signal from the COVID-19 Trends and Impact survey for the US\nepidata &lt;- pub_covidcast(\n  source = \"fb-survey\",\n  signals = \"smoothed_cli\",\n  geo_type = \"nation\",\n  time_type = \"day\",\n  geo_values = \"us\",\n  time_values = epirange(20210105, 20210410)\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, geo_type, time_value, issue, lag, value, stderr)\n\n# A tibble: 6 × 9\n  geo_value signal      source geo_type time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;  &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 us        smoothed_c… fb-su… nation   2021-01-05 2021-01-10     5  1.18 0.0162\n2 us        smoothed_c… fb-su… nation   2021-01-06 2021-01-29    23  1.18 0.0163\n3 us        smoothed_c… fb-su… nation   2021-01-07 2021-01-29    22  1.20 0.0165\n4 us        smoothed_c… fb-su… nation   2021-01-08 2021-01-29    21  1.22 0.0167\n5 us        smoothed_c… fb-su… nation   2021-01-09 2021-01-29    20  1.22 0.0169\n6 us        smoothed_c… fb-su… nation   2021-01-10 2021-01-29    19  1.23 0.0171\n\n\nHere value is the requested signal – in this case, the smoothed estimate of the percentage of people with COVID-like illness, based on the symptom surveys, and stderr is its standard error."
  },
  {
    "objectID": "slides/day1-morning.html#returned-data---covidcast-main-endpoint",
    "href": "slides/day1-morning.html#returned-data---covidcast-main-endpoint",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Returned data - COVIDcast main endpoint",
    "text": "Returned data - COVIDcast main endpoint\n\n\npub_covidcast() outputs a tibble, where each row represents one observation.\nEach observation covers a set of events aggregated by time and by geographic region is a record in our database. Each such record includes:\ntime_value: time period when the events occurred.\ngeo_value: geographic region where the events occurred.\nvalue: estimated value.\nstderr: standard error of the estimate, usually referring to the sampling error.\nsample_size: number of events used in the estimation."
  },
  {
    "objectID": "slides/day1-morning.html#returned-data---covidcast-main-endpoint-1",
    "href": "slides/day1-morning.html#returned-data---covidcast-main-endpoint-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Returned data - COVIDcast main endpoint",
    "text": "Returned data - COVIDcast main endpoint\nCrucially—and unlike most other sources of COVID-19 data—our API reports two additional fields with each record:\n\nissue: The time period when this observation was published.\nlag: The time delay between when the events occurred and when this observation was published.\nMeaning that unlike most other sources of COVID data, it tracks the complete revision history of every signal.\nThis allows for historical reconstructions of what information was available at specific times. More on this soon!"
  },
  {
    "objectID": "slides/day1-morning.html#geographic-levels",
    "href": "slides/day1-morning.html#geographic-levels",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Geographic levels",
    "text": "Geographic levels\n\n\nThe Epidata API makes signals available at different geographic levels, depending on the endpoint\nFor the smoothed_cli signal, we can obtain values for each state\nSimply change geo_type and geo_values in the previous example to get…\n\n\n# Obtain the most up-to-date version of the smoothed covid-like illness (CLI)\n# signal from the COVID-19 Trends and Impact survey for all states\nstate_epidata &lt;- pub_covidcast(\n  source = \"fb-survey\",\n  signals = \"smoothed_cli\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"*\",\n  time_values = epirange(20210105, 20210410)\n)\nhead(state_epidata) %&gt;% select(geo_value, signal, source, geo_type, time_value, issue, lag, value, stderr)\n\n# A tibble: 6 × 9\n  geo_value signal      source geo_type time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;  &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 ak        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 0.747 0.250 \n2 al        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 2.36  0.187 \n3 ar        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 1.93  0.200 \n4 az        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 1.56  0.129 \n5 ca        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 1.24  0.0542\n6 co        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 0.934 0.107"
  },
  {
    "objectID": "slides/day1-morning.html#covidcast-main-endpoint---example-query",
    "href": "slides/day1-morning.html#covidcast-main-endpoint---example-query",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "COVIDcast main endpoint - Example query",
    "text": "COVIDcast main endpoint - Example query\n\nCounty geo_values are FIPS codes and are discussed in the API docs here. The example below is for Orange County, California.\n\nfb_county_data &lt;- pub_covidcast(\n  source = \"fb-survey\",\n  signals = \"smoothed_accept_covid_vaccine\",\n  geo_type = \"county\",\n  time_type = \"day\",\n  time_values = epirange(20201221, 20201225),\n  geo_values = \"06059\"\n)\nhead(fb_county_data) %&gt;% select(geo_value, signal, source, geo_type, time_value, issue, lag, value, stderr)\n\n# A tibble: 5 × 9\n  geo_value signal      source geo_type time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;  &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 06059     smoothed_a… fb-su… county   2020-12-21 2020-12-22     1  80.9   2.08\n2 06059     smoothed_a… fb-su… county   2020-12-22 2020-12-23     1  78.9   1.76\n3 06059     smoothed_a… fb-su… county   2020-12-23 2020-12-24     1  80.0   1.50\n4 06059     smoothed_a… fb-su… county   2020-12-24 2020-12-25     1  79.3   1.35\n5 06059     smoothed_a… fb-su… county   2020-12-25 2020-12-26     1  80.3   1.21\n\n\n\n\n\n\nNote\n\n\nThe covidcast endpoint supports * in its time and geo fields. Try to obtain the signal values for all available counties by replacing geo_values = \"06059\" with geo_values = \"*\"."
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-hospitalizations",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-hospitalizations",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Hospitalizations",
    "text": "Example queries - Other endpoints  Hospitalizations\nCOVID-19 Hospitalization: Facility Lookup\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/covid_hosp_facility_lookup.html \n\npub_covid_hosp_facility_lookup(city = \"southlake\")\n\n# A tibble: 2 × 10\n  hospital_pk state ccn    hospital_name    address city  zip   hospital_subtype\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           \n1 450888      TX    450888 TEXAS HEALTH HA… 1545 E… SOUT… 76092 Short Term      \n2 670132      TX    670132 METHODIST SOUTH… 421 E … SOUT… 76092 Short Term      \n# ℹ 2 more variables: fips_code &lt;chr&gt;, is_metro_micro &lt;dbl&gt;\n\n# pub_covid_hosp_facility_lookup(state = \"WY\")\n# A non-example (there is no city called New York in Wyoming)\n# pub_covid_hosp_facility_lookup(state = \"WY\", city = \"New York\")"
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-hospitalizations-1",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-hospitalizations-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Hospitalizations",
    "text": "Example queries - Other endpoints  Hospitalizations\nCOVID-19 Hospitalization by Facility\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/covid_hosp_facility.html \n\npub_covid_hosp_facility(\n  hospital_pks = \"100075\",\n  collection_weeks = epirange(20200101, 20200501)\n)\n\nCOVID-19 Hospitalization by State\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/covid_hosp.html \n\npub_covid_hosp_state_timeseries(states = \"MA\", dates = \"20200510\")"
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-flu-endpoints",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-flu-endpoints",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Flu endpoints",
    "text": "Example queries - Other endpoints  Flu endpoints\nFluSurv hospitalization data\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/flusurv.html \n\npub_flusurv(locations = \"ca\", epiweeks = 202001)\n\nFluview data\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/fluview.html \n\npub_fluview(regions = \"nat\", epiweeks = epirange(201201, 202001))\n\nNIDSS Flu\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/nidss_flu.html \n\npub_nidss_flu(regions = \"taipei\", epiweeks = epirange(200901, 201301))"
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-dengue-endpoints",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-dengue-endpoints",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Dengue endpoints",
    "text": "Example queries - Other endpoints  Dengue endpoints\nDelphi’s Dengue Nowcast\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/dengue_nowcast.html \n\npub_dengue_nowcast(locations = \"pr\", epiweeks = epirange(201401, 202301))\n\nNIDSS dengue\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/nidss_dengue.html \n\npub_nidss_dengue(locations = \"taipei\", epiweeks = epirange(200301, 201301))\n\nPAHO Dengue\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/paho_dengue.html \n\npub_paho_dengue(regions = \"ca\", epiweeks = epirange(200201, 202319))"
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-wikipedia",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-wikipedia",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Wikipedia",
    "text": "Example queries - Other endpoints  Wikipedia\nWikipedia access\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/wiki.html \n\npub_wiki(\n  language = \"en\",\n  articles = \"influenza\",\n  time_type = \"week\",\n  time_values = epirange(202001, 202319)\n)\n\n# A tibble: 64 × 6\n   article   count     total  hour epiweek     value\n   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 influenza  6516 663604044    -1 2019-12-29   9.82\n 2 influenza 10244 789885521    -1 2020-01-05  13.0 \n 3 influenza 10728 783760384    -1 2020-01-12  13.7 \n 4 influenza 24843 785222292    -1 2020-01-19  31.6 \n 5 influenza 62850 780291898    -1 2020-01-26  80.5 \n 6 influenza 41768 778222703    -1 2020-02-02  53.7 \n 7 influenza 29434 767244708    -1 2020-02-09  38.4 \n 8 influenza 22714 764074572    -1 2020-02-16  29.7 \n 9 influenza 88758 767718009    -1 2020-02-23 116.  \n10 influenza 62433 759825311    -1 2020-03-01  82.2 \n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nTip - public vs private methods\n\n\nAside from these public methods we’ve gone through (these start with pub_), there are private methods (these start with pvt_ when you type avail_endpoints()). These require private access keys to use (separate from the Delphi Epidata API key). To run these locally, you will need to store these secrets in your .Reviron file, or set them as environmental variables. See Private methods for examples of using private endpoints."
  },
  {
    "objectID": "slides/day1-morning.html#signal-metadata",
    "href": "slides/day1-morning.html#signal-metadata",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Signal metadata",
    "text": "Signal metadata\n\nSome endpoints provide additional metadata for signals.\n\nTime Information: Details on available time frames and last update times.\nGeography Information: Details on available geography types.\n\nKey Endpoints for Metadata\n\npub_covidcast_meta(): Access metadata for the COVIDcast endpoint.\npub_fluview_meta(): Get metadata for the FluView endpoint.\npub_meta(): General metadata for the Delphi Epidata API."
  },
  {
    "objectID": "slides/day1-morning.html#panel-data-1",
    "href": "slides/day1-morning.html#panel-data-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Panel data",
    "text": "Panel data\n\n\nPanel data or longitudinal data, contain cross-sectional measurements of subjects over time.\nIn table form, panel data is a time index + one or more locations/keys.\nFor example: The estimated percentage of outpatient doctor visits that are COVID-related in WA from Dec. 2021 to Feb. 2022 (docs):\n\n\n\nAn `epi_df` object, 6 x 3 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-02-01\n\n# A tibble: 6 × 3\n  geo_value time_value percent_cli\n* &lt;chr&gt;     &lt;date&gt;           &lt;dbl&gt;\n1 wa        2021-12-01        4.70\n2 wa        2021-12-02        4.60\n3 wa        2021-12-03        4.56\n4 wa        2021-12-04        4.93\n5 wa        2021-12-05        4.17\n6 wa        2021-12-06        4.12"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---covid-19-cases",
    "href": "slides/day1-morning.html#examples-of-panel-data---covid-19-cases",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of panel data - COVID-19 cases",
    "text": "Examples of panel data - COVID-19 cases\nJHU CSSE COVID cases in the U.S. (smoothed with a 7-day trailing average) over the year of the pandemic (April 15, 2020 – April 15, 2021).\nAPI docs: https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/jhu-csse.html"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---chng-cli",
    "href": "slides/day1-morning.html#examples-of-panel-data---chng-cli",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of panel data - CHNG-CLI",
    "text": "Examples of panel data - CHNG-CLI\nChange Healthcare COVID-like illness (CHNG-CLI) reports the percentage of outpatient visits for COVID-related symptoms, based on deidentified Change Healthcare claims data.\nAPI docs: https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/chng.html"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---chng-covid",
    "href": "slides/day1-morning.html#examples-of-panel-data---chng-covid",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of panel data - CHNG-COVID",
    "text": "Examples of panel data - CHNG-COVID\nChange Healthcare COVID (CHNG-COVID) reports the percentage of outpatient visits with confirmed COVID-19, based on Change Healthcare claims data.\nAPI docs: https://cmu-delphi.github.io/delphi-epidata/api/paho_dengue.html"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---ctis-cli",
    "href": "slides/day1-morning.html#examples-of-panel-data---ctis-cli",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of panel data - CTIS-CLI",
    "text": "Examples of panel data - CTIS-CLI\nCOVID-19 Trends and Impact Survey CLI (CTIS-CLI) estimates the percentage of the population with COVID-like illness based on Delphi’s Facebook user surveys.\nAPI docs: https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/fb-survey.html"
  },
  {
    "objectID": "slides/day1-morning.html#all-together---visualizing-multiple-panel-data-signals",
    "href": "slides/day1-morning.html#all-together---visualizing-multiple-panel-data-signals",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "All together - Visualizing multiple panel data signals",
    "text": "All together - Visualizing multiple panel data signals\nExample: gathering different signals + visualizing panel data"
  },
  {
    "objectID": "slides/day1-morning.html#all-together---visualizing-multiple-panel-data-signals-1",
    "href": "slides/day1-morning.html#all-together---visualizing-multiple-panel-data-signals-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "All together - Visualizing multiple panel data signals",
    "text": "All together - Visualizing multiple panel data signals\nExample: gathering different signals + scaling + visualizing panel data\n\n\nFigure 1 from Reinhart et al. (2021)\n\nTakeaway: The auxiliary signals track changes in the official reported cases quite well. This is clearer when they have all been placed on the same range as reported cases per 100,000 people."
  },
  {
    "objectID": "slides/day1-morning.html#covid-19-cases-and-deaths-in-ca-example",
    "href": "slides/day1-morning.html#covid-19-cases-and-deaths-in-ca-example",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "COVID-19 cases and deaths in CA example",
    "text": "COVID-19 cases and deaths in CA example\n\nTakeaway: Cases appear to strongly correlate with deaths several weeks later."
  },
  {
    "objectID": "slides/day1-morning.html#intro-to-versioned-data",
    "href": "slides/day1-morning.html#intro-to-versioned-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Intro to versioned data",
    "text": "Intro to versioned data\n\nIn panel data, we’ve seen that time is indicated by time_value.\nNow, we add a second time index to indicate the data version…\n\n\n\nKey: &lt;geo_value, time_value, version&gt;\n   geo_value time_value    version percent_cli\n      &lt;char&gt;     &lt;Date&gt;     &lt;Date&gt;       &lt;num&gt;\n1:        wa 2021-12-01 2021-12-05    0.884362\n2:        wa 2021-12-01 2021-12-06    0.917057\n3:        wa 2021-12-01 2021-12-07    0.896221\n4:        wa 2021-12-01 2021-12-08    0.984512\n5:        wa 2021-12-01 2021-12-09    1.027853\n6:        wa 2021-12-01 2021-12-10    0.999755\n\n\n\nNote that this feature can be indicated in different ways (ex. version, issue, release, as_of)."
  },
  {
    "objectID": "slides/day1-morning.html#versioned-panel-data",
    "href": "slides/day1-morning.html#versioned-panel-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Versioned panel data",
    "text": "Versioned panel data\nEstimated percentage of outpatient (DV-CLI) data across multiple issue dates, with updates and revisions to past data as new issue dates are released:\n\n\nFigure 5 from Reinhart et al. (2021)"
  },
  {
    "objectID": "slides/day1-morning.html#latency-and-revision-in-signals",
    "href": "slides/day1-morning.html#latency-and-revision-in-signals",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Latency and revision in signals",
    "text": "Latency and revision in signals\n\nLatency refers to the delay between data collection and availability.\n\nExample: A signal based on medical insurance claims may take several days to appear but is subject to delays as claims are processed over weeks.\n\nRevision occurs when data is updated or corrected after initial publication, often due to new information or late reporting.\n\nExample: COVID-19 case reports are revised frequently after initial publication as new data comes in or reporting backlogs are cleared."
  },
  {
    "objectID": "slides/day1-morning.html#latency-and-revision-in-signals---example",
    "href": "slides/day1-morning.html#latency-and-revision-in-signals---example",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Latency and revision in signals - Example",
    "text": "Latency and revision in signals - Example\n\n\nRecall the first example of panel & versioned data we’ve seen…\nThis signal is 4 days latent (min(version - time_value))\n\n\n\n# A tibble: 6 × 5\n# Groups:   time_value [6]\n  geo_value time_value version    percent_cli version_time_diff\n  &lt;chr&gt;     &lt;date&gt;     &lt;date&gt;           &lt;dbl&gt; &lt;drtn&gt;           \n1 wa        2021-12-01 2021-12-05       0.884 4 days           \n2 wa        2021-12-02 2021-12-06       0.737 4 days           \n3 wa        2021-12-03 2021-12-07       0.662 4 days           \n4 wa        2021-12-04 2021-12-08       0.663 4 days           \n5 wa        2021-12-05 2021-12-09       0.872 4 days           \n6 wa        2021-12-06 2021-12-10       0.642 4 days           \n\n\n\nAnd clearly undergoes revision over time (ex. consider Dec. 1’s percent_cli across version):\n\n\n\nKey: &lt;geo_value, time_value, version&gt;\n   geo_value time_value    version percent_cli version_time_diff\n      &lt;char&gt;     &lt;Date&gt;     &lt;Date&gt;       &lt;num&gt;        &lt;difftime&gt;\n1:        wa 2021-12-01 2021-12-05    0.884362            4 days\n2:        wa 2021-12-01 2021-12-06    0.917057            5 days\n3:        wa 2021-12-01 2021-12-07    0.896221            6 days\n4:        wa 2021-12-01 2021-12-08    0.984512            7 days\n5:        wa 2021-12-01 2021-12-09    1.027853            8 days\n6:        wa 2021-12-01 2021-12-10    0.999755            9 days"
  },
  {
    "objectID": "slides/day1-morning.html#revision-triangle-outpatient-visits-in-wa-2022",
    "href": "slides/day1-morning.html#revision-triangle-outpatient-visits-in-wa-2022",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Revision triangle, Outpatient visits in WA 2022",
    "text": "Revision triangle, Outpatient visits in WA 2022"
  },
  {
    "objectID": "slides/day1-morning.html#revisions",
    "href": "slides/day1-morning.html#revisions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Revisions",
    "text": "Revisions\nMany data sources are subject to revisions:\n\nCase and death counts are frequently corrected or adjusted by authorities.\nMedical claims data can take weeks to be submitted and processed.\n\n\n\nLab tests and medical records can be backlogged for a variety of reasons.\nSurveys are not always completed promptly.\nKey: An accurate revision log is crucial for researchers building forecasts.\n\nA forecast that is made today can should rely on information we have access to today."
  },
  {
    "objectID": "slides/day1-morning.html#three-types-of-revisions",
    "href": "slides/day1-morning.html#three-types-of-revisions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Three types of revisions",
    "text": "Three types of revisions\n\nSources that don’t revise - Ex. Facebook or Google symptoms (provisional and final are the same)\nPredictable revisions - Ex. Claims data (CHNG) and public health reports aligned by observation/test, hosp, or death date\nRevisions that are large and erratic to predict - Ex. COVID cases and deaths"
  },
  {
    "objectID": "slides/day1-morning.html#types-of-revisions---comparison-between-2.-and-3.",
    "href": "slides/day1-morning.html#types-of-revisions---comparison-between-2.-and-3.",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Types of revisions - Comparison between 2. and 3.",
    "text": "Types of revisions - Comparison between 2. and 3.\n\n\nRevision behavior for two indicators in the HRR containing Charlotte, NC.\n\n\n\nDV-CLI signal (left)  was regularly revised throughout the period, although effects fade farther back.\nJHU CSSE cases (right)  remain “as reported” on Sept. 28, with a spike toward the end of this period, until a major correction is made on Oct. 19, which brings this down & affects prior data.\n\n\n\n\nFigure 1 from McDonald et al. (2021)"
  },
  {
    "objectID": "slides/day1-morning.html#key-takeaways",
    "href": "slides/day1-morning.html#key-takeaways",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nMedical claims revisions: More systematic and predictable.\nCOVID-19 case report revisions: Erratic and often unpredictable.\nLarge spikes or anomalies can occur as:\n\nReporting backlogs are cleared.\nChanges in case definitions are implemented."
  },
  {
    "objectID": "slides/day1-morning.html#reporting-backlogs---example",
    "href": "slides/day1-morning.html#reporting-backlogs---example",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Reporting backlogs - Example",
    "text": "Reporting backlogs - Example\n\n\nLeft: Reported cases per day in Bexar County, Texas, during the summer of 2020. On July 16, 4,810 backlogged cases were reported, reflecting a 2-week delay. This caused a prolonged spike due to the 7-day trailing average applied to the counts.\nRight: CTIS estimates of CLI-in-community showed more stable underlying trends.\n\n\n\n\nFigure 4 from Reinhart et al. (2021)"
  },
  {
    "objectID": "slides/day1-morning.html#reporting-backlogs---key-takeaways",
    "href": "slides/day1-morning.html#reporting-backlogs---key-takeaways",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Reporting backlogs - Key takeaways",
    "text": "Reporting backlogs - Key takeaways\n\nReporting issues been common across U.S. jurisdictions.\nFor example, audits have regularly discovered misclassified or unreported cases and deaths.\nThis underscores the value of cross-checking data with external sources not part of the same reporting systems."
  },
  {
    "objectID": "slides/day1-morning.html#versioned-data-in-epidatr",
    "href": "slides/day1-morning.html#versioned-data-in-epidatr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Versioned data in epidatr",
    "text": "Versioned data in epidatr\n\n\nEpidata API contains comprehensive data record, capturing each signal’s estimate, location, date, and update timeline.\nExample: Doctor Visits Signal (from the covidcast endpoint)\n\nEstimates the percentage of outpatient doctor visits that are COVID-related. To give a specific example, let’s consider the estimate for PA on May 1, 2020:\n\n\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-01\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  as_of = \"2020-05-07\"\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value)\n\n# A tibble: 1 × 7\n  geo_value signal           source        time_value issue        lag value\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-visits 2020-05-01 2020-05-07     6  2.58\n\n\n\nInitial estimate was issued on May 7, 2020 (due to delay from aggregation and ingestion by the API)."
  },
  {
    "objectID": "slides/day1-morning.html#understanding-data-as-of-a-specific-date",
    "href": "slides/day1-morning.html#understanding-data-as-of-a-specific-date",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Understanding data “as of” a specific date",
    "text": "Understanding data “as of” a specific date\n\n\nRequesting Specific Data Versions:\n\nUse as_of, issues, or lag arguments to specify data availability.\nOnly one argument can be used at a time; not all endpoints support all three.\n\nWe’ve already used the as_of argument, so let’s try lag\n\nExample for May 7, 2020 (we should get the same output as before):\n\n\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-01\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  lag = 6\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 1 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-07     6  2.58     NA\n\n\nResult: Estimate of &lt;3% issued on May 7, 2020."
  },
  {
    "objectID": "slides/day1-morning.html#understanding-data-as-of-a-specific-date-1",
    "href": "slides/day1-morning.html#understanding-data-as-of-a-specific-date-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Understanding data “as of” a specific date",
    "text": "Understanding data “as of” a specific date\n\nDefault behaviour: If we don’t specify as_of, we get the most recent estimate:\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-01\"),\n  geo_type = \"state\",\n  geo_values = \"pa\"\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 1 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-07-04    64  5.97     NA\n\n\n\nSubstantial Estimate Change:\n\nEstimate increased from &lt;3% to almost 6% after May 7, reflecting new data on visits from May 1.\n\nCritical for Forecasting:\n\nAccurate backtesting requires using data available at the time of model fitting, not later updates, to ensure valid forecasting results."
  },
  {
    "objectID": "slides/day1-morning.html#multiple-issues-of-observations",
    "href": "slides/day1-morning.html#multiple-issues-of-observations",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Multiple issues of observations",
    "text": "Multiple issues of observations\nBy using the issues argument, we can request all issues in a certain time period:\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-01\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  issues = epirange(\"2020-05-01\", \"2020-05-15\")\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 6 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-07     6  2.58     NA\n2 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-08     7  3.28     NA\n3 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-09     8  3.32     NA\n4 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-12    11  3.59     NA\n5 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-13    12  3.63     NA\n6 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-14    13  3.66     NA"
  },
  {
    "objectID": "slides/day1-morning.html#observations-issued-with-a-specific-lag",
    "href": "slides/day1-morning.html#observations-issued-with-a-specific-lag",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Observations issued with a specific lag",
    "text": "Observations issued with a specific lag\n\n\nWe can use the lag argument to request only data reported with a certain lag.\nExample: Request a lag of 7 days fetches only data issued exactly 7 days after the corresponding time_value:\n\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-07\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  lag = 7\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 5 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-08     7  3.28     NA\n2 pa        smoothed_adj_cli doctor-vi… 2020-05-02 2020-05-09     7  3.23     NA\n3 pa        smoothed_adj_cli doctor-vi… 2020-05-05 2020-05-12     7  2.78     NA\n4 pa        smoothed_adj_cli doctor-vi… 2020-05-06 2020-05-13     7  2.56     NA\n5 pa        smoothed_adj_cli doctor-vi… 2020-05-07 2020-05-14     7  2.19     NA"
  },
  {
    "objectID": "slides/day1-morning.html#query-results-exclusion",
    "href": "slides/day1-morning.html#query-results-exclusion",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Query results exclusion",
    "text": "Query results exclusion\n\n\nAlthough the query we ran on the previous slide requested values from May 1 to May 7, May 3 and May 4 were not included due to a 7-day lag.\nResults for those dates appear only if updates are issued on the corresponding lag day (e.g., May 10).\n\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-03\", \"2020-05-03\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  issues = epirange(\"2020-05-09\", \"2020-05-15\")\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 5 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-09     6  2.79     NA\n2 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-12     9  3.02     NA\n3 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-13    10  3.04     NA\n4 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-14    11  3.02     NA\n5 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-15    12  3.05     NA"
  },
  {
    "objectID": "slides/day1-morning.html#main-takeaways",
    "href": "slides/day1-morning.html#main-takeaways",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Main takeaways",
    "text": "Main takeaways\n\n\nDelphi Epidata: A one-stop platform for real-time epidemic data, providing aggregated signals for disease tracking and forecasting from diverse sources like health records, mobility patterns, and more.\nEpidata API: Open-access API delivering up-to-date, granular epidemiological data + makes all historical versions available.\nEpidatr: Enables you to access Delphi’s epidemiological data through R and Python, offering easy installation, powerful API functions, and interactive tools for discovering and analyzing health signals.\nVersioned Data and Latency: Panel data captures time-series trends, which are often subject to revision. A standout feature of this API is its inclusion of two critical fields…\n\nissue: When the data was published\nlag: The delay between the event and when it was published\n\nto manage latency and revisions for transparency and more accurate analysis."
  },
  {
    "objectID": "slides/day1-morning.html#final-slide",
    "href": "slides/day1-morning.html#final-slide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Final slide",
    "text": "Final slide\nThanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nDay 1 Morning — cmu-delphi/insightnet-workshop-2024"
  }
]