[
  {
    "objectID": "slides/day1-afternoon.html#section",
    "href": "slides/day1-afternoon.html#section",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Explore, clean & transform data",
    "text": "Explore, clean & transform data"
  },
  {
    "objectID": "slides/day1-afternoon.html#outline",
    "href": "slides/day1-afternoon.html#outline",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Outline",
    "text": "Outline\n\nEssentials of dplyr and tidyr\nEpiverse software ecosystem\nPanel and versioned data in the epiverse\nBasic Nowcasting using epiprocess\nMotivating case study"
  },
  {
    "objectID": "slides/day1-afternoon.html#down-with-spreadsheets-for-data-manipulation",
    "href": "slides/day1-afternoon.html#down-with-spreadsheets-for-data-manipulation",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Down with Spreadsheets for Data Manipulation",
    "text": "Down with Spreadsheets for Data Manipulation\n\nSpreadsheets make it difficult to rerun analyses consistently.\nUsing R (and dplyr) allows for:\n\nReproducibility\nEase of modification\n\nRecommendation: Avoid manual edits; instead, use code for transformations."
  },
  {
    "objectID": "slides/day1-afternoon.html#introduction-to-dplyr",
    "href": "slides/day1-afternoon.html#introduction-to-dplyr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Introduction to dplyr",
    "text": "Introduction to dplyr\n\ndplyr is a powerful package in R for data manipulation.\nIt is part of the tidyverse, which includes a collection of packages designed to work together.\nWe focus on basic operations like selecting and filtering data.\nMake sure to load the necessary libraries before using dplyr."
  },
  {
    "objectID": "slides/day1-afternoon.html#meet-the-palmers",
    "href": "slides/day1-afternoon.html#meet-the-palmers",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Meet the Palmers",
    "text": "Meet the Palmers\n\nIllustration from the palmerpenguins website"
  },
  {
    "objectID": "slides/day1-afternoon.html#working-with-the-palmerpenguins-dataset",
    "href": "slides/day1-afternoon.html#working-with-the-palmerpenguins-dataset",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Working with the palmerpenguins Dataset",
    "text": "Working with the palmerpenguins Dataset\n\nThe palmerpenguins dataset is included in the palmerpenguins package.\nLoad both the tidyverse and palmerpenguins libraries to access and explore the dataset.\nThe dataset includes measurements of penguins such as species, bill length, flipper length, and body mass.\n\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#ways-to-inspect-the-dataset",
    "href": "slides/day1-afternoon.html#ways-to-inspect-the-dataset",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Ways to Inspect the Dataset",
    "text": "Ways to Inspect the Dataset\n\nUse head() to view the first 6 row of the data (tail() to view the last 6 rows)\n\n\nhead(penguins)  # First 6 rows\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n#tail(penguins)  # Last 6 rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#ways-to-inspect-the-dataset-1",
    "href": "slides/day1-afternoon.html#ways-to-inspect-the-dataset-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Ways to Inspect the Dataset",
    "text": "Ways to Inspect the Dataset\n\nglimpse() to get a compact overview of the dataset.\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-tibbles",
    "href": "slides/day1-afternoon.html#creating-tibbles",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Creating Tibbles",
    "text": "Creating Tibbles\n\nTibbles: Modern data frames with enhanced features.\nRows represent observations (or cases).\nColumns represent variables (or features).\nYou can create tibbles manually using the tibble() function.\n\n\ntibble(x = letters, y = 1:26)\n\n# A tibble: 26 × 2\n   x         y\n   &lt;chr&gt; &lt;int&gt;\n 1 a         1\n 2 b         2\n 3 c         3\n 4 d         4\n 5 e         5\n 6 f         6\n 7 g         7\n 8 h         8\n 9 i         9\n10 j        10\n# ℹ 16 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#selecting-columns-with-select",
    "href": "slides/day1-afternoon.html#selecting-columns-with-select",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Selecting Columns with select()",
    "text": "Selecting Columns with select()\n\nThe select() function is used to pick specific columns from your dataset.\n\n\nselect(penguins, species, body_mass_g)  # Select the 'species' and 'body_mass_g' columns\n\n# A tibble: 344 × 2\n   species body_mass_g\n   &lt;fct&gt;         &lt;int&gt;\n 1 Adelie         3750\n 2 Adelie         3800\n 3 Adelie         3250\n 4 Adelie           NA\n 5 Adelie         3450\n 6 Adelie         3650\n 7 Adelie         3625\n 8 Adelie         4675\n 9 Adelie         3475\n10 Adelie         4250\n# ℹ 334 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#selecting-columns-with-select-1",
    "href": "slides/day1-afternoon.html#selecting-columns-with-select-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Selecting Columns with select()",
    "text": "Selecting Columns with select()\n\nYou can exclude columns by prefixing the column names with a minus sign -.\n\n\nselect(penguins, -species)  # Exclude the 'species' column from the dataset\n\n# A tibble: 344 × 7\n   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Torge…           39.1          18.7               181        3750 male   2007\n 2 Torge…           39.5          17.4               186        3800 fema…  2007\n 3 Torge…           40.3          18                 195        3250 fema…  2007\n 4 Torge…           NA            NA                  NA          NA &lt;NA&gt;   2007\n 5 Torge…           36.7          19.3               193        3450 fema…  2007\n 6 Torge…           39.3          20.6               190        3650 male   2007\n 7 Torge…           38.9          17.8               181        3625 fema…  2007\n 8 Torge…           39.2          19.6               195        4675 male   2007\n 9 Torge…           34.1          18.1               193        3475 &lt;NA&gt;   2007\n10 Torge…           42            20.2               190        4250 &lt;NA&gt;   2007\n# ℹ 334 more rows\n\n\n\nSo, this is useful when you want to keep only certain columns or remove unnecessary ones."
  },
  {
    "objectID": "slides/day1-afternoon.html#extracting-columns-with-pull",
    "href": "slides/day1-afternoon.html#extracting-columns-with-pull",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Extracting Columns with pull()",
    "text": "Extracting Columns with pull()\n\npull(): Extract a column as a vector.\nLet’s try this with the species column…\n\n\npull(penguins, species)\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo"
  },
  {
    "objectID": "slides/day1-afternoon.html#filtering-rows-with-filter",
    "href": "slides/day1-afternoon.html#filtering-rows-with-filter",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Filtering Rows with filter()",
    "text": "Filtering Rows with filter()\n\nThe filter() function allows you to select rows that meet specific conditions.\nConditions can involve column values, such as selecting only “Gentoo” penguins or filtering based on measurements like flipper length.\nThis enables you to narrow down your dataset to focus on relevant data.\n\n\nfilter(penguins, species == \"Gentoo\", flipper_length_mm &lt; 208)  # Filter Gentoo penguins with flipper length &lt; 208mm\n\n# A tibble: 2 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo  Biscoe           45.1          14.5               207        5050\n2 Gentoo  Biscoe           48.4          14.4               203        4625\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#combining-select-and-filter-functions",
    "href": "slides/day1-afternoon.html#combining-select-and-filter-functions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Combining select() and filter() Functions",
    "text": "Combining select() and filter() Functions\n\nYou can combine select() and filter() functions to refine the dataset further.\nUse select() to choose columns and filter() to narrow rows based on conditions.\nThis helps in extracting the exact data needed for analysis.\n\n\nselect(filter(penguins, species == \"Gentoo\", flipper_length_mm &lt; 208), species, flipper_length_mm)\n\n# A tibble: 2 × 2\n  species flipper_length_mm\n  &lt;fct&gt;               &lt;int&gt;\n1 Gentoo                207\n2 Gentoo                203"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-the-pipe-operator",
    "href": "slides/day1-afternoon.html#using-the-pipe-operator",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using the Pipe Operator %>%",
    "text": "Using the Pipe Operator %&gt;%\n\nThe pipe operator (%&gt;%) makes code more readable by chaining multiple operations together.\nThe output of one function is automatically passed to the next function.\nThis allows you to perform multiple steps (e.g., select() followed by filter()) in a clear and concise manner.\n\n\n# This code reads more like poetry!\npenguins %&gt;% \n  select(species, flipper_length_mm) %&gt;%\n  filter(species == \"Gentoo\", flipper_length_mm &lt; 208)\n\n# A tibble: 2 × 2\n  species flipper_length_mm\n  &lt;fct&gt;               &lt;int&gt;\n1 Gentoo                207\n2 Gentoo                203"
  },
  {
    "objectID": "slides/day1-afternoon.html#key-practices-in-dplyr",
    "href": "slides/day1-afternoon.html#key-practices-in-dplyr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Key Practices in dplyr",
    "text": "Key Practices in dplyr\n\nUse tibbles for easier data handling.\nUse select() and filter() for data manipulation.\nUse pull() to extract columns as vectors.\nUse head(), tail(), and glimpse() for quick data inspection.\nChain functions with %&gt;% for cleaner code."
  },
  {
    "objectID": "slides/day1-afternoon.html#grouping-data-with-group_by",
    "href": "slides/day1-afternoon.html#grouping-data-with-group_by",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Grouping Data with group_by()",
    "text": "Grouping Data with group_by()\n\nUse group_by() to group data by one or more columns.\nAllows performing operations on specific groups of data.\n\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  filter(body_mass_g == min(body_mass_g, na.rm = TRUE))\n\n# A tibble: 4 × 8\n# Groups:   species [3]\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Biscoe           36.5          16.6               181        2850\n2 Adelie    Biscoe           36.4          17.1               184        2850\n3 Gentoo    Biscoe           42.7          13.7               208        3950\n4 Chinstrap Dream            46.9          16.6               192        2700\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#penguin-bill-length-and-depth",
    "href": "slides/day1-afternoon.html#penguin-bill-length-and-depth",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Penguin bill length and depth",
    "text": "Penguin bill length and depth\n\n\nIllustration from the palmerpenguins website"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-new-columns-with-mutate",
    "href": "slides/day1-afternoon.html#creating-new-columns-with-mutate",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Creating New Columns with mutate()",
    "text": "Creating New Columns with mutate()\n\nmutate() is used to create new columns.\nPerform calculations using existing columns and assign to new columns.\n\n\npenguins %&gt;%\n  mutate(bill_size_mm2 = bill_depth_mm * bill_length_mm) %&gt;% \n  select(-c(flipper_length_mm, body_mass_g, sex)) # too many cols to print\n\n# A tibble: 344 × 6\n   species island    bill_length_mm bill_depth_mm  year bill_size_mm2\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7  2007          731.\n 2 Adelie  Torgersen           39.5          17.4  2007          687.\n 3 Adelie  Torgersen           40.3          18    2007          725.\n 4 Adelie  Torgersen           NA            NA    2007           NA \n 5 Adelie  Torgersen           36.7          19.3  2007          708.\n 6 Adelie  Torgersen           39.3          20.6  2007          810.\n 7 Adelie  Torgersen           38.9          17.8  2007          692.\n 8 Adelie  Torgersen           39.2          19.6  2007          768.\n 9 Adelie  Torgersen           34.1          18.1  2007          617.\n10 Adelie  Torgersen           42            20.2  2007          848.\n# ℹ 334 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-new-columns-with-mutate-1",
    "href": "slides/day1-afternoon.html#creating-new-columns-with-mutate-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Creating New Columns with mutate()",
    "text": "Creating New Columns with mutate()\n\nmutate() can create multiple new columns in one step.\nLogical comparisons (e.g., sex == \"male\") can be used within mutate().\n\n\npenguins %&gt;%\n  mutate(bill_size_mm2 = bill_depth_mm * bill_length_mm, \n         TF = sex == \"male\") %&gt;% \n  select(-c(flipper_length_mm, body_mass_g, year)) \n\n# A tibble: 344 × 7\n   species island    bill_length_mm bill_depth_mm sex    bill_size_mm2 TF   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;lgl&gt;\n 1 Adelie  Torgersen           39.1          18.7 male            731. TRUE \n 2 Adelie  Torgersen           39.5          17.4 female          687. FALSE\n 3 Adelie  Torgersen           40.3          18   female          725. FALSE\n 4 Adelie  Torgersen           NA            NA   &lt;NA&gt;             NA  NA   \n 5 Adelie  Torgersen           36.7          19.3 female          708. FALSE\n 6 Adelie  Torgersen           39.3          20.6 male            810. TRUE \n 7 Adelie  Torgersen           38.9          17.8 female          692. FALSE\n 8 Adelie  Torgersen           39.2          19.6 male            768. TRUE \n 9 Adelie  Torgersen           34.1          18.1 &lt;NA&gt;            617. NA   \n10 Adelie  Torgersen           42            20.2 &lt;NA&gt;            848. NA   \n# ℹ 334 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#combining-group_by-and-mutate",
    "href": "slides/day1-afternoon.html#combining-group_by-and-mutate",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Combining group_by() and mutate()",
    "text": "Combining group_by() and mutate()\n\nFirst, group data using group_by().\nThen, use drop_na() from tidyr to exclude rows with missing values.\nFinally, mutate to perform calculations for each group.\n\n\npenguins %&gt;%\n  drop_na() %&gt;% # Remove all non-complete rows\n  group_by(species) %&gt;%\n  mutate(body_mass_median = median(body_mass_g)) %&gt;% \n  select(-c(flipper_length_mm, body_mass_g, sex)) %&gt;% \n  head()\n\n# A tibble: 6 × 6\n# Groups:   species [1]\n  species island    bill_length_mm bill_depth_mm  year body_mass_median\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;            &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7  2007             3700\n2 Adelie  Torgersen           39.5          17.4  2007             3700\n3 Adelie  Torgersen           40.3          18    2007             3700\n4 Adelie  Torgersen           36.7          19.3  2007             3700\n5 Adelie  Torgersen           39.3          20.6  2007             3700\n6 Adelie  Torgersen           38.9          17.8  2007             3700"
  },
  {
    "objectID": "slides/day1-afternoon.html#conditional-calculations-in-mutate-with-if_else",
    "href": "slides/day1-afternoon.html#conditional-calculations-in-mutate-with-if_else",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Conditional Calculations in mutate() with if_else()",
    "text": "Conditional Calculations in mutate() with if_else()\n\nif_else() allows conditional logic within mutate().\nPerform different operations depending on conditions, like “big” or “small.”\n\n\nt &lt;- 800\npenguins %&gt;%\n  mutate(bill_size_mm2 = bill_depth_mm * bill_length_mm, \n         TF = sex == \"male\",\n         bill_size_binary = if_else(bill_size_mm2 &gt; t, \"big\", \"small\")) %&gt;% \n  select(-c(flipper_length_mm, body_mass_g, sex)) %&gt;% # too many cols to print\n  head()\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm  year bill_size_mm2 TF   \n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt; &lt;lgl&gt;\n1 Adelie  Torgersen           39.1          18.7  2007          731. TRUE \n2 Adelie  Torgersen           39.5          17.4  2007          687. FALSE\n3 Adelie  Torgersen           40.3          18    2007          725. FALSE\n4 Adelie  Torgersen           NA            NA    2007           NA  NA   \n5 Adelie  Torgersen           36.7          19.3  2007          708. FALSE\n6 Adelie  Torgersen           39.3          20.6  2007          810. TRUE \n# ℹ 1 more variable: bill_size_binary &lt;chr&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#summarizing-data-with-summarise",
    "href": "slides/day1-afternoon.html#summarizing-data-with-summarise",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Summarizing Data with summarise()",
    "text": "Summarizing Data with summarise()\n\nsummarise() reduces data to summary statistics (e.g., mean, median).\nTypically used after group_by() to summarize each group.\n\n\npenguins %&gt;%\n  drop_na() %&gt;%\n  group_by(species) %&gt;%\n  summarise(body_mass_median = median(body_mass_g))\n\n# A tibble: 3 × 2\n  species   body_mass_median\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie                3700\n2 Chinstrap             3700\n3 Gentoo                5050"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-summarise-with-multiple-calculations",
    "href": "slides/day1-afternoon.html#using-summarise-with-multiple-calculations",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using summarise() with Multiple Calculations",
    "text": "Using summarise() with Multiple Calculations\n\nUse summarise() to calculate multiple summary statistics at once.\nInclude multiple columns and functions in a single call.\n\n\npenguins %&gt;%\n  drop_na() %&gt;%\n  group_by(species) %&gt;%\n  summarise(body_mass_median = median(body_mass_g), \n            bill_depth_median = median(bill_depth_mm),\n            flipper_length_median = median(flipper_length_mm),\n            bill_length_mm = median(bill_length_mm))\n\n# A tibble: 3 × 5\n  species   body_mass_median bill_depth_median flipper_length_median\n  &lt;fct&gt;                &lt;dbl&gt;             &lt;dbl&gt;                 &lt;dbl&gt;\n1 Adelie                3700              18.4                   190\n2 Chinstrap             3700              18.4                   196\n3 Gentoo                5050              15                     216\n# ℹ 1 more variable: bill_length_mm &lt;dbl&gt;\n\n\n\nYikes! This can get long. Is there a way to condense this?"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-across-to-apply-functions-to-multiple-columns-in-one-swoop",
    "href": "slides/day1-afternoon.html#using-across-to-apply-functions-to-multiple-columns-in-one-swoop",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using across() to Apply Functions to Multiple Columns in one Swoop",
    "text": "Using across() to Apply Functions to Multiple Columns in one Swoop\n\nacross() applies a function (e.g., median) to multiple columns.\nIt’s especially useful for summarizing multiple numeric columns in one step.\n\n\npenguins %&gt;%\n  drop_na() %&gt;% \n  group_by(species) %&gt;%\n  summarise(across(where(is.numeric), median))\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.4               190        3700  2008\n2 Chinstrap           49.6          18.4               196        3700  2008\n3 Gentoo              47.4          15                 216        5050  2008"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-count-to-aggregate-data",
    "href": "slides/day1-afternoon.html#using-count-to-aggregate-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using count() to Aggregate Data",
    "text": "Using count() to Aggregate Data\n\ncount() is a shortcut for grouping and summarizing the data:\n\nFor example, if we want to count the number of penguins by species, then\n\npenguins_count &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(count = n())\n\nis equivalent to\n\npenguins_count &lt;- penguins %&gt;%\n  count(species)\n\npenguins_count # Let's see what the counts are.\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124"
  },
  {
    "objectID": "slides/day1-afternoon.html#tidy-data",
    "href": "slides/day1-afternoon.html#tidy-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Tidy Data",
    "text": "Tidy Data\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.” — Leo Tolstoy\n\n\nTidy datasets are like happy families: consistent, standardized, and easy to work with.\n\nMessy datasets are like unhappy families: each one messy in its own unique way.\nIn this section:\nWe’ll define what makes data tidy and how to transform between the tidy and messy formats."
  },
  {
    "objectID": "slides/day1-afternoon.html#what-is-tidy-data",
    "href": "slides/day1-afternoon.html#what-is-tidy-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What is Tidy Data?",
    "text": "What is Tidy Data?\n\nTidy data follows a consistent structure: each row represents one observation, and each column represents one variable.\n\n\n\n# Simple example of messy data (wide format)\npenguins_wide &lt;- tibble(\n  island = c(\"Biscoe\", \"Biscoe\", \"Dream\"),\n  year = c(2007, 2008, 2007),\n  Adelie = c(10, 18, 20),\n  Gentoo = c(34, 46, 0)\n)\npenguins_wide\n\n# A tibble: 3 × 4\n  island  year Adelie Gentoo\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0"
  },
  {
    "objectID": "slides/day1-afternoon.html#tidying-messy-data-with-pivot_longer",
    "href": "slides/day1-afternoon.html#tidying-messy-data-with-pivot_longer",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Tidying Messy Data with pivot_longer()",
    "text": "Tidying Messy Data with pivot_longer()\n\nTo turn messy data into tidy data, we often use the tidyr package in the tidyverse.\nUse pivot_longer() to convert data from wide format (multiple columns for the same variable) to long format (one column per variable).\nThis makes it easier to perform group-based calculations or create visualizations.\n\n\n\n# A tibble: 6 × 4\n  island  year species count\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Biscoe  2007 Adelie     10\n2 Biscoe  2007 Gentoo     34\n3 Biscoe  2008 Adelie     18\n4 Biscoe  2008 Gentoo     46\n5 Dream   2007 Adelie     20\n6 Dream   2007 Gentoo      0"
  },
  {
    "objectID": "slides/day1-afternoon.html#making-data-wider-with-pivot_wider",
    "href": "slides/day1-afternoon.html#making-data-wider-with-pivot_wider",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Making Data Wider with pivot_wider()",
    "text": "Making Data Wider with pivot_wider()\n\nSometimes, you need to convert data from long format to wide format using pivot_wider().\nThis can be useful when you want to separate variables into individual columns.\nLet’s try converting penguins_tidy back to penguins_wide!\n\n\n# Pivoting long data back to wide format\npenguins_wide_back &lt;- penguins_tidy %&gt;%\n  pivot_wider(names_from = species, values_from = count)\n\npenguins_wide_back\n\n# A tibble: 3 × 4\n  island  year Adelie Gentoo\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0"
  },
  {
    "objectID": "slides/day1-afternoon.html#complete-and-fill-to-handle-missing-data",
    "href": "slides/day1-afternoon.html#complete-and-fill-to-handle-missing-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "complete() and fill() to Handle Missing Data",
    "text": "complete() and fill() to Handle Missing Data\n\n\ncomplete(): Adds missing rows for combinations of specified variables.\nfill(): Fills missing values in columns, typically from previous or next available values (default is LOCF).\n\n\n\n# First, use complete() to add missing year (2008 for Dream)\npenguins_complete &lt;- penguins_wide %&gt;%\n  complete(island, year)\npenguins_complete\n\n# A tibble: 4 × 4\n  island  year Adelie Gentoo\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0\n4 Dream   2008     NA     NA\n\n\n\n# Then, use fill() to fill the missing penguin counts\npenguins_complete %&gt;%\n  fill(Adelie, Gentoo)\n\n# A tibble: 4 × 4\n  island  year Adelie Gentoo\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Biscoe  2007     10     34\n2 Biscoe  2008     18     46\n3 Dream   2007     20      0\n4 Dream   2008     20      0"
  },
  {
    "objectID": "slides/day1-afternoon.html#introduction-to-joins-in-dplyr",
    "href": "slides/day1-afternoon.html#introduction-to-joins-in-dplyr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Introduction to Joins in dplyr",
    "text": "Introduction to Joins in dplyr\n\n\nJoining datasets is a powerful tool for combining info. from multiple sources.\nIn R, dplyr provides several functions to perform different types of joins.\nWe’ll demonstrate joining penguins_complete (our penguin counts dataset) with island_info (dataset containing additional info. about the islands).\n\n\n# Island information dataset\nisland_info &lt;- tibble(\n  island = c(\"Biscoe\", \"Dream\", \"Torgersen\"),\n  location = c(\"Antarctica\", \"Antarctica\", \"Antarctica\"),\n  region = c(\"West\", \"East\", \"East\")\n)\n\nisland_info\n\n# A tibble: 3 × 3\n  island    location   region\n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe    Antarctica West  \n2 Dream     Antarctica East  \n3 Torgersen Antarctica East  \n\n\n\nNotice that the island_info dataset includes an island, Torgersen, that is not in penguins_complete."
  },
  {
    "objectID": "slides/day1-afternoon.html#left-join-keep-all-rows-from-the-first-dataset",
    "href": "slides/day1-afternoon.html#left-join-keep-all-rows-from-the-first-dataset",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Left Join: Keep All Rows from the First Dataset",
    "text": "Left Join: Keep All Rows from the First Dataset\n\nA left join keeps all rows from the first dataset (penguins_complete), and adds matching data from the second dataset (island_info).\nSo all rows from the first dataset (penguins_complete) will be preserved.\nThe datasets are joined by matching the island column, specified by the by argument.\n\n\n# Left join: combining penguins data with island info\npenguins_with_info &lt;- penguins_complete %&gt;%\n  left_join(island_info, by = \"island\")\n\npenguins_with_info\n\n# A tibble: 4 × 6\n  island  year Adelie Gentoo location   region\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe  2007     10     34 Antarctica West  \n2 Biscoe  2008     18     46 Antarctica West  \n3 Dream   2007     20      0 Antarctica East  \n4 Dream   2008     NA     NA Antarctica East"
  },
  {
    "objectID": "slides/day1-afternoon.html#right-join-keep-all-rows-from-the-second-dataset",
    "href": "slides/day1-afternoon.html#right-join-keep-all-rows-from-the-second-dataset",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Right Join: Keep All Rows from the Second Dataset",
    "text": "Right Join: Keep All Rows from the Second Dataset\n\nA right join keeps all rows from the second dataset (island_info), and adds matching data from the first dataset (penguins_complete).\nIf a row in the second dataset doesn’t have a match in the first, then the columns from the first will be filled with NA.\nWe can see this for the Torgersen row from island_info…\n\n\n# Right join: keep all rows from island_info\npenguins_right_join &lt;- penguins_complete %&gt;%\n  right_join(island_info, by = \"island\")\n\npenguins_right_join\n\n# A tibble: 5 × 6\n  island     year Adelie Gentoo location   region\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe     2007     10     34 Antarctica West  \n2 Biscoe     2008     18     46 Antarctica West  \n3 Dream      2007     20      0 Antarctica East  \n4 Dream      2008     NA     NA Antarctica East  \n5 Torgersen    NA     NA     NA Antarctica East"
  },
  {
    "objectID": "slides/day1-afternoon.html#inner-join-only-keeping-matching-rows",
    "href": "slides/day1-afternoon.html#inner-join-only-keeping-matching-rows",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Inner Join: Only Keeping Matching Rows",
    "text": "Inner Join: Only Keeping Matching Rows\n\nAn inner join will only keep rows where there is a match in both datasets.\nSo, if an island in island_info does not have a corresponding entry in penguins_complete, then that row will be excluded.\n\n\n# Inner join: only matching rows are kept\npenguins_inner_join &lt;- penguins_complete %&gt;%\n  inner_join(island_info, by = \"island\")\n\npenguins_inner_join\n\n# A tibble: 4 × 6\n  island  year Adelie Gentoo location   region\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe  2007     10     34 Antarctica West  \n2 Biscoe  2008     18     46 Antarctica West  \n3 Dream   2007     20      0 Antarctica East  \n4 Dream   2008     NA     NA Antarctica East"
  },
  {
    "objectID": "slides/day1-afternoon.html#full-join-keeping-all-rows-from-both-datasets",
    "href": "slides/day1-afternoon.html#full-join-keeping-all-rows-from-both-datasets",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Full Join: Keeping All Rows from Both Datasets",
    "text": "Full Join: Keeping All Rows from Both Datasets\n\nA full join will keep all rows from both datasets.\nIf an island in either dataset has no match in the other, the missing values will be filled with NA.\n\n\n# Full join: keep all rows from both datasets\npenguins_full_join &lt;- penguins_complete %&gt;%\n  full_join(island_info, by = \"island\")\n\npenguins_full_join\n\n# A tibble: 5 × 6\n  island     year Adelie Gentoo location   region\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1 Biscoe     2007     10     34 Antarctica West  \n2 Biscoe     2008     18     46 Antarctica West  \n3 Dream      2007     20      0 Antarctica East  \n4 Dream      2008     NA     NA Antarctica East  \n5 Torgersen    NA     NA     NA Antarctica East"
  },
  {
    "objectID": "slides/day1-afternoon.html#summary-of-the-four-join-functions",
    "href": "slides/day1-afternoon.html#summary-of-the-four-join-functions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Summary of the Four Join Functions",
    "text": "Summary of the Four Join Functions\n\nLeft join: All rows from the left dataset and matching rows from the right dataset.\nRight join: All rows from the right dataset and matching rows from the left dataset.\nInner join: Only matching rows from both datasets.\nFull join: All rows from both datasets, with NA where no match exists."
  },
  {
    "objectID": "slides/day1-afternoon.html#final-thoughts-on-joins",
    "href": "slides/day1-afternoon.html#final-thoughts-on-joins",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Final thoughts on joins",
    "text": "Final thoughts on joins\n\nJoins are an essential part of data wrangling in R.\nThe choice of join depends on the analysis you need to perform:\n\nUse left joins when you want to keep all data from the first dataset.\nUse right joins when you want to keep all data from the second dataset.\nUse inner joins when you’re only interested in matching rows.\nUse full joins when you want to preserve all information from both datasets."
  },
  {
    "objectID": "slides/day1-afternoon.html#goodbye-palmer-penguins",
    "href": "slides/day1-afternoon.html#goodbye-palmer-penguins",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Goodbye palmer penguins",
    "text": "Goodbye palmer penguins\n\nWhat’s a penguin’s favorite tool?\n%&gt;% — to keep the fish moving, from one catch to the next! 🐧\n\n\nLogo from the palmerpenguins website"
  },
  {
    "objectID": "slides/day1-afternoon.html#epiverse-software-ecosystem",
    "href": "slides/day1-afternoon.html#epiverse-software-ecosystem",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Epiverse software ecosystem",
    "text": "Epiverse software ecosystem"
  },
  {
    "objectID": "slides/day1-afternoon.html#what-is-panel-data",
    "href": "slides/day1-afternoon.html#what-is-panel-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What is panel data?",
    "text": "What is panel data?\n\n\nRecall that panel data, or longitudinal data, contain cross-sectional measurements of subjects over time.\nBuilt-in example: covid_case_death_rates dataset, which is a snapshot as of May 31, 2022 that contains daily state-wise measures of case_rate and death_rate for COVID-19 in 2021:\n\n\n\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n* &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 as        2020-12-31       0        0    \n5 az        2020-12-31      76.8      1.10 \n6 ca        2020-12-31      96.0      0.751\n\n\n\nHow do we store & work with such snapshots in the epiverse software ecosystem?"
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_df-snapshot-of-a-data-set",
    "href": "slides/day1-afternoon.html#epi_df-snapshot-of-a-data-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "epi_df: snapshot of a data set",
    "text": "epi_df: snapshot of a data set\n\na tibble that requires columns geo_value and time_value.\narbitrary additional columns containing measured values\nadditional keys to index (age_group, ethnicity, etc.)\n\n\n\n\n\n\n\nepi_df\n\n\nRepresents a snapshot that contains the most up-to-date values of the signal variables, as of a given time."
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_df-snapshot-of-a-data-set-1",
    "href": "slides/day1-afternoon.html#epi_df-snapshot-of-a-data-set-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "epi_df: snapshot of a data set",
    "text": "epi_df: snapshot of a data set\n\nedf &lt;- covid_case_death_rates\nedf\n\nAn `epi_df` object, 20,496 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 20,496 × 4\n   geo_value time_value case_rate death_rate\n * &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 ak        2020-12-31      35.9      0.158\n 2 al        2020-12-31      65.1      0.438\n 3 ar        2020-12-31      66.0      1.27 \n 4 as        2020-12-31       0        0    \n 5 az        2020-12-31      76.8      1.10 \n 6 ca        2020-12-31      96.0      0.751\n 7 co        2020-12-31      35.8      0.649\n 8 ct        2020-12-31      52.1      0.819\n 9 dc        2020-12-31      31.0      0.601\n10 de        2020-12-31      64.3      0.912\n# ℹ 20,486 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#sliding-examples-on-epi_df",
    "href": "slides/day1-afternoon.html#sliding-examples-on-epi_df",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Sliding examples on epi_df",
    "text": "Sliding examples on epi_df\nGrowth rates\n\nedf &lt;- filter(edf, geo_value %in% c(\"ut\", \"ca\")) %&gt;%\n  group_by(geo_value) %&gt;%\n  mutate(gr_cases = growth_rate(time_value, case_rate, method = \"trend_filter\"))"
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs",
    "href": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "epi_archive: collection of epi_dfs",
    "text": "epi_archive: collection of epi_dfs\n\nfull version history of a data set\nacts like a bunch of epi_dfs — but stored compactly\nallows similar functionality as epi_df but using only data that would have been available at the time\n\n\n\n\n\n\n\nRevisions\n\n\nEpidemiology data gets revised frequently.\n\nWe may want to use the data as it looked in the past\nor we may want to examine the history of revisions."
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs-1",
    "href": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "epi_archive: collection of epi_dfs",
    "text": "epi_archive: collection of epi_dfs\n\narchive_cases_dv_subset\n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-06-01 / 2021-11-30\nℹ First/last version with update: 2020-06-02 / 2021-12-01\nℹ Versions end: 2021-12-01\nℹ A preview of the table (129638 rows x 5 columns):\nKey: &lt;geo_value, time_value, version&gt;\n        geo_value time_value    version percent_cli case_rate_7d_av\n           &lt;char&gt;     &lt;Date&gt;     &lt;Date&gt;       &lt;num&gt;           &lt;num&gt;\n     1:        ca 2020-06-01 2020-06-02          NA        6.628329\n     2:        ca 2020-06-01 2020-06-06    2.140116        6.628329\n     3:        ca 2020-06-01 2020-06-07    2.140116        6.628329\n     4:        ca 2020-06-01 2020-06-08    2.140379        6.628329\n     5:        ca 2020-06-01 2020-06-09    2.114430        6.628329\n    ---                                                            \n129634:        tx 2021-11-26 2021-11-29    1.858596        7.957657\n129635:        tx 2021-11-27 2021-11-28          NA        7.174299\n129636:        tx 2021-11-28 2021-11-29          NA        6.834681\n129637:        tx 2021-11-29 2021-11-30          NA        8.841247\n129638:        tx 2021-11-30 2021-12-01          NA        9.566218"
  },
  {
    "objectID": "slides/day1-afternoon.html#revision-patterns",
    "href": "slides/day1-afternoon.html#revision-patterns",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Revision patterns",
    "text": "Revision patterns"
  },
  {
    "objectID": "slides/day1-afternoon.html#finalized-data",
    "href": "slides/day1-afternoon.html#finalized-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Finalized data",
    "text": "Finalized data\n\n\nCounts are revised as time proceeds\nWant to know the final value\nOften not available until weeks/months later\n\n\n\nForecasting\n\nAt time \\(t\\), predict the final value for time \\(t+h\\), \\(h &gt; 0\\)\n\n\n\n\nBackcasting\n\nAt time \\(t\\), predict the final value for time \\(t-h\\), \\(h &lt; 0\\)\n\n\n\n\nNowcasting\n\nAt time \\(t\\), predict the final value for time \\(t\\)"
  },
  {
    "objectID": "slides/day1-afternoon.html#backfill-canadian-edition",
    "href": "slides/day1-afternoon.html#backfill-canadian-edition",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Backfill Canadian edition",
    "text": "Backfill Canadian edition\n\nEvery week the BC CDC releases COVID-19 hospitalization data.\nFollowing week they revise the number upward (by ~25%) due to lagged reports.\n\n \n\nTakeaway: Once the data is backfilled, hospitalizations rarely show a decline, challenging the common media narrative."
  },
  {
    "objectID": "slides/day1-afternoon.html#aside-on-nowcasting",
    "href": "slides/day1-afternoon.html#aside-on-nowcasting",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Aside on Nowcasting",
    "text": "Aside on Nowcasting\n\nTo some Epis, “nowcasting” can be equated with “estimate the time-varying instantaneous reproduction number, \\(R_t\\)”\nExample using the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. \n\n\n\n\n\n\n\n\n\n\n\nGroup built {rtestim} doing for this nonparametrically.\nWe may come back to this later…"
  },
  {
    "objectID": "slides/day1-afternoon.html#mathematical-setup",
    "href": "slides/day1-afternoon.html#mathematical-setup",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Mathematical setup",
    "text": "Mathematical setup\n\nSuppose today is time \\(t\\)\nLet \\(y_i\\) denote a series of interest observed at times \\(i=1,\\ldots, t\\).\n\n\n\n\nOur goal\n\n\n\nProduce a point nowcast for the finalized values of \\(y_t\\).\nAccompany with time-varying prediction intervals\n\n\n\n\n\nWe also have access to \\(p\\) other time series \\(x_{ij},\\; i=1,\\ldots,t, \\; j = 1,\\ldots,p\\)\nAll may be subject to revisions."
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-simple-ratio-ex-hhs-admissions",
    "href": "slides/day1-afternoon.html#nowcasting-simple-ratio-ex-hhs-admissions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Nowcasting Simple Ratio Ex: HHS Admissions",
    "text": "Nowcasting Simple Ratio Ex: HHS Admissions\n\nIn this example, we will demonstrate the concept of nowcasting using HHS influenza admissions data (All confirmed influenza hospital admissions occurring each day).\nWe will work with provisional data (real-time reports) and compare them to finalized data (final reports).\nThe goal is to estimate or nowcast the admissions values for days when only provisional data is available."
  },
  {
    "objectID": "slides/day1-afternoon.html#fetch-versioned-data",
    "href": "slides/day1-afternoon.html#fetch-versioned-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetch Versioned Data",
    "text": "Fetch Versioned Data\nLet’s fetch versioned admissions data from the API (pub_covidcast) for Georgia (geo_values = \"ga\") and the signal of interest (confirmed_admissions_influenza_1d) over February 2023.\n\n# Set the start and end dates for the analysis\nstart_time = as.Date(\"2023-02-01\")\nend_time = as.Date(\"2023-02-28\")\n\nadmissions_archive &lt;- pub_covidcast(\n  source = \"hhs\",\n  signals = \"confirmed_admissions_influenza_1d\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = \"*\", # all time values\n  issues = \"*\"\n) %&gt;% \n  filter(time_value &gt;= start_time & time_value &lt;= end_time) %&gt;%\n  select(geo_value, time_value, version = issue, admissions = value) %&gt;%\n  as_epi_archive(compactify = TRUE)"
  },
  {
    "objectID": "slides/day1-afternoon.html#latency-in-reporting---minimum-lag",
    "href": "slides/day1-afternoon.html#latency-in-reporting---minimum-lag",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Latency in Reporting - Minimum Lag",
    "text": "Latency in Reporting - Minimum Lag\n\nA quick inspection reveals that admissions are systematically 2 days latent (fixed lag).\n\n\nadmissions_revision_inspect = admissions_archive$DT %&gt;% mutate(version_time_diff = version - time_value)\nadmissions_revision_inspect %&gt;% group_by(time_value) %&gt;% slice(1) %&gt;% head()\n\n# A tibble: 6 × 5\n# Groups:   time_value [6]\n  geo_value time_value version    admissions version_time_diff\n  &lt;chr&gt;     &lt;date&gt;     &lt;date&gt;          &lt;dbl&gt; &lt;drtn&gt;           \n1 ga        2023-02-01 2023-02-03          6 2 days           \n2 ga        2023-02-02 2023-02-04         68 2 days           \n3 ga        2023-02-03 2023-02-05         11 2 days           \n4 ga        2023-02-04 2023-02-06         74 2 days           \n5 ga        2023-02-05 2023-02-07          9 2 days           \n6 ga        2023-02-06 2023-02-08          9 2 days           \n\n\n\nUse revision_summary() from epiprocess to generate basic statistics about the revision behavior for the dataset."
  },
  {
    "objectID": "slides/day1-afternoon.html#latency-in-reporting---finalized-value-attainment",
    "href": "slides/day1-afternoon.html#latency-in-reporting---finalized-value-attainment",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Latency in Reporting - Finalized Value Attainment",
    "text": "Latency in Reporting - Finalized Value Attainment\n\n\nWhen is the finalized value first attained for each date? (we wouldn’t have access to this in real-time)\nHow fast are the final values attained & what’s the pattern for these times, if any?\n\n\n\n# A tibble: 6 × 4\n  geo_value time_value min_version diff   \n  &lt;chr&gt;     &lt;date&gt;     &lt;date&gt;      &lt;drtn&gt; \n1 ga        2023-02-01 2023-02-26  25 days\n2 ga        2023-02-02 2023-02-26  24 days\n3 ga        2023-02-03 2023-02-26  23 days\n4 ga        2023-02-04 2023-02-07   3 days\n5 ga        2023-02-05 2023-02-26  21 days\n6 ga        2023-02-06 2023-02-26  20 days\n\n\nAnd here’s a numerical summary:\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    7.00   10.00   22.54   18.25  320.00 \n\n\n\nConclusion: Varies somewhat. Even for this relatively small time period… Could be as low as 2 days or as high as 320 days. Yikes.\nSo if we were doing this in real-time, then we don’t necessarily have access to finalized data until much later in time."
  },
  {
    "objectID": "slides/day1-afternoon.html#comparison-of-final-vs.-multiple-revisions",
    "href": "slides/day1-afternoon.html#comparison-of-final-vs.-multiple-revisions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Comparison of Final vs. Multiple Revisions",
    "text": "Comparison of Final vs. Multiple Revisions\nThis figure shows the finalized admissions in comparison to multiple revisions to see how the data changes over time:"
  },
  {
    "objectID": "slides/day1-afternoon.html#comparison-of-final-vs.-one-revision",
    "href": "slides/day1-afternoon.html#comparison-of-final-vs.-one-revision",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Comparison of Final vs. One Revision",
    "text": "Comparison of Final vs. One Revision\n\nThe below figure compares the finalized admissions (in black) to one revision (in yellow) for Feb. 16, 2023.\n\n\n\n\n\n\n\n\n\nThe real-time data is biased downwards (systematically below the true value). That is, the signal tends to get scaled up with future revisions."
  },
  {
    "objectID": "slides/day1-afternoon.html#calculate-one-ratio-provisional-vs.-finalized-data",
    "href": "slides/day1-afternoon.html#calculate-one-ratio-provisional-vs.-finalized-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Calculate One Ratio: Provisional vs. Finalized Data",
    "text": "Calculate One Ratio: Provisional vs. Finalized Data\n\n\n\nSuppose that the day is February 16, 2023. Then, because the data is 2 days latent, we can compute the ratio between provisional and finalized data for February 14, 2023.\n\n\ndate_under_investigation = as.Date(\"2023-02-14\")\n\n# Load the finalized influenza data for GA\nga_finalized &lt;- admissions_latest %&gt;%\n  filter(time_value == date_under_investigation) %&gt;%\n  dplyr::select(admissions)\n\n# Look at the same data but \"as_of\" \"2023-02-16\"\n# Load the provisional case data (reported on \"2023-02-16\" but for \"2022-02-14\")\nadmissions_old = epix_as_of(admissions_archive, max_version = (date_under_investigation + 2L))\n\nga_provisional &lt;- admissions_old %&gt;%\n  filter(time_value == date_under_investigation) %&gt;%\n  dplyr::select(admissions)\n\n# Calculate ratio between provisional and finalized cases for 2022-02-14\nratio &lt;- ga_provisional$admissions / ga_finalized$admissions\nratio\n\n[1] 0.3571429\n\n\nConclusion: The real-time counts tend to be far below the finalized counts for this time (5 vs 14 here).\nQuestion: Can we generalize this over many days?"
  },
  {
    "objectID": "slides/day1-afternoon.html#calculating-the-ratio-over-multiple-dates",
    "href": "slides/day1-afternoon.html#calculating-the-ratio-over-multiple-dates",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Calculating the Ratio over Multiple Dates",
    "text": "Calculating the Ratio over Multiple Dates\n\nLet’s move from calculating the ratio for one day to multiple day with the goal to use it to nowcast for Feb. 27, which has a provisional value of 6\n\n\nas_of_date = as.Date(\"2023-03-01\")\n\nprovisional_feb_27 &lt;- epix_as_of(admissions_archive, max_version = as_of_date) %&gt;%\n  filter(time_value == as_of_date - 2L) %&gt;%\n  pull(admissions)\nprovisional_feb_27\n\n[1] 6\n\n\nand a finalized value of 8\n\nfinalized_value &lt;- admissions_latest %&gt;%\n  filter(time_value == as_of_date - 2L) %&gt;%\n  pull(admissions)\nfinalized_value\n\n[1] 8"
  },
  {
    "objectID": "slides/day1-afternoon.html#calculating-the-ratio-over-multiple-dates-1",
    "href": "slides/day1-afternoon.html#calculating-the-ratio-over-multiple-dates-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Calculating the Ratio over Multiple Dates",
    "text": "Calculating the Ratio over Multiple Dates\nFirst, let’s download the real-time number of cases for GA in February 2023, and compare them to their finalized version.\n\ndates &lt;- seq(as.Date(\"2023-02-01\"), as.Date(\"2023-02-26\"), by = \"day\")\nadmissions_real_time &lt;- function(date) {\n  epix_as_of(admissions_archive, max_version = (date + 2L)) %&gt;%\n    filter(time_value == date)\n}\nadmissions_ga_real_time &lt;- map_dfr(dates, admissions_real_time)\nhead(admissions_ga_real_time)\n\nAn `epi_df` object, 6 x 3 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2023-02-03\n\n# A tibble: 6 × 3\n  geo_value time_value admissions\n* &lt;chr&gt;     &lt;date&gt;          &lt;dbl&gt;\n1 ga        2023-02-01          6\n2 ga        2023-02-02         68\n3 ga        2023-02-03         11\n4 ga        2023-02-04         74\n5 ga        2023-02-05          9\n6 ga        2023-02-06          9"
  },
  {
    "objectID": "slides/day1-afternoon.html#calculating-the-ratio-over-multiple-dates-2",
    "href": "slides/day1-afternoon.html#calculating-the-ratio-over-multiple-dates-2",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Calculating the Ratio over Multiple Dates",
    "text": "Calculating the Ratio over Multiple Dates\nNow, let’s plot the real-time vs the finalized number of admissions:\n\n\nTakeaways: Aside from two major overreporting spikes near the beginning, the real-time counts are generally biased below the finalized counts.\nSystematic underreporting begins around Feb. 6, where the bias becomes more consistent."
  },
  {
    "objectID": "slides/day1-afternoon.html#calculating-the-ratio-over-multiple-dates-3",
    "href": "slides/day1-afternoon.html#calculating-the-ratio-over-multiple-dates-3",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Calculating the Ratio over Multiple Dates",
    "text": "Calculating the Ratio over Multiple Dates\nSince we expect that this pattern in bias continues going forward (and the massive overreporting near the beginning may drown this out), we’ll start estimating the ratio from Feb. 6 and beyond.\n\nadmissions_ga_real_time = admissions_ga_real_time %&gt;% filter(time_value &gt;= \"2023-02-06\")\nhead(admissions_ga_real_time)\n\nAn `epi_df` object, 6 x 3 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2023-02-03\n\n# A tibble: 6 × 3\n  geo_value time_value admissions\n* &lt;chr&gt;     &lt;date&gt;          &lt;dbl&gt;\n1 ga        2023-02-06          9\n2 ga        2023-02-07          5\n3 ga        2023-02-08          4\n4 ga        2023-02-09          8\n5 ga        2023-02-10          6\n6 ga        2023-02-11          7"
  },
  {
    "objectID": "slides/day1-afternoon.html#realistic-limitation-of-nowcasting---finalized-data",
    "href": "slides/day1-afternoon.html#realistic-limitation-of-nowcasting---finalized-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Realistic Limitation of Nowcasting - Finalized Data",
    "text": "Realistic Limitation of Nowcasting - Finalized Data\n\nRecall that real-time access to finalized data is limited as finalized values can take months to report (e.g., Feb 10 finalized 320 days later).\nTo nowcast accurately, we must rely on the best available approximation of finalized data at the time of estimation (March 1).\n\n\nadmissions_as_of_mar_1 &lt;- epix_as_of(admissions_archive, max_version = as_of_date) %&gt;%\n  filter(time_value &gt;= \"2023-02-06\" & time_value &lt;= \"2023-02-26\")"
  },
  {
    "objectID": "slides/day1-afternoon.html#ratio-calculation-summary",
    "href": "slides/day1-afternoon.html#ratio-calculation-summary",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Ratio calculation & summary",
    "text": "Ratio calculation & summary\nWe then use the “finalized” and real-time counts to compute the mean ratio.\n\nratio_real_time_to_mar1 &lt;- admissions_ga_real_time$admissions / admissions_as_of_mar_1$admissions\nsummary(ratio_real_time_to_mar1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.2667  0.4000  0.5000  0.7811  0.7778  3.3333 \n\n\nOn average, the real-time counts are ~78.1% of the finalized counts."
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-on-march-1",
    "href": "slides/day1-afternoon.html#nowcasting-on-march-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Nowcasting on March 1",
    "text": "Nowcasting on March 1\nSince the average ratio between real-time and finalized values is 0.781 (i.e., real-time counts are typically 78% of the finalized counts), then the nowcast is\n\n# Now we can nowcast properly:\nnowcast &lt;- provisional_feb_27 *\n  1 / mean(ratio_real_time_to_mar1)\nnowcast\n\n[1] 7.681254\n\n\nSo, this nowcast is 7.68, which is closer to the true finalized value of 8 than the provisional value of 6."
  },
  {
    "objectID": "slides/day1-afternoon.html#summary-of-three-main-steps",
    "href": "slides/day1-afternoon.html#summary-of-three-main-steps",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Summary of three main steps",
    "text": "Summary of three main steps\nSo the main steps for this type of fixed lag nowcasting are…\n\nObtain the provisional value for the target.\nEstimate the ratio using the real-time and “finalized” data (for all previous dates that follow a consistent pattern in reporting).\nProfit.\n\n\n\nExpand for the accompanying code\n# Today\nas_of_date = as.Date(\"2023-03-01\")\n\n# 1. Obtain the provisional value\nprovisional_feb_27 &lt;- epix_as_of(admissions_archive, max_version = as_of_date) %&gt;%\n  filter(time_value == as_of_date - 2L) %&gt;%\n  pull(admissions)\nprovisional_feb_27\n\n# 2. Estimate the ratio \nadmissions_ga_real_time = admissions_ga_real_time %&gt;% filter(time_value &gt;= \"2023-02-06\") # Real-time data\nadmissions_as_of_mar_1 &lt;- epix_as_of(admissions_archive, max_version = as_of_date) %&gt;% # \"Finalized\" data\n  filter(time_value &gt;= \"2023-02-06\" & time_value &lt;= \"2023-02-26\")\n\nmean_ratio &lt;- mean(admissions_ga_real_time$admissions / admissions_as_of_mar_1$admissions) # Mean ratio\n\n# 3. Profit.\nnowcast &lt;- provisional_feb_27 *\n  1 / mean(ratio_real_time_to_mar1)\nnowcast"
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-admissions-data-for-multiple-dates",
    "href": "slides/day1-afternoon.html#nowcasting-admissions-data-for-multiple-dates",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Nowcasting Admissions Data for Multiple Dates",
    "text": "Nowcasting Admissions Data for Multiple Dates\n\nDefine Nowcast Function:\n\nInput: Takes in the dates to nowcast and the fixed lag\nOutput: The nowcasted admissions based on the ratio of real-time to finalized data.\n\n\n\n\nCode\nnowcast_function &lt;- function(nowcast_date, fixed_lag) {\n  as_of_date = nowcast_date + fixed_lag\n  \n  # 1. Obtain the provisional value for the target.\n  provisional &lt;- epix_as_of(admissions_archive, max_version = as_of_date) %&gt;%\n  filter(time_value == as_of_date - 2L) %&gt;%\n  pull(admissions)\n  \n  #2. Estimate the ratio multiplier using\n  # real-time\n  dates &lt;- seq(as.Date(\"2023-02-06\"), nowcast_date - 1, by = \"day\")\n  admissions_ga_real_time &lt;- map_dfr(dates, admissions_real_time)\n  \n  # and \"finalized\" data\n  finalized &lt;- epix_as_of(admissions_archive, max_version = as_of_date) %&gt;% filter(time_value &gt;= \"2023-02-06\" & time_value &lt;= (nowcast_date - 1)) \n\n  ratios &lt;- admissions_ga_real_time$admissions / finalized$admissions\n  \n  # Remove infinite or NaN ratios (i.e., keep only finite values)\n  mean_ratio &lt;- mean(ratios[is.finite(ratios)])\n\n  #3. Profit.\n  nowcast &lt;- provisional * (1 / mean_ratio)\n  \n  # Return a dataframe with the nowcast and date\n  tibble(\n    time_value = nowcast_date,\n    nowcast_admissions = nowcast\n  )\n}"
  },
  {
    "objectID": "slides/day1-afternoon.html#map-nowcast-over-multiple-dates",
    "href": "slides/day1-afternoon.html#map-nowcast-over-multiple-dates",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Map Nowcast Over Multiple Dates",
    "text": "Map Nowcast Over Multiple Dates\n\nWe can use map2_df() to apply the function to a series of dates (e.g., Feb 27–28).\nReturns a dataframe with nowcasted results.\n\n\n# Apply Nowcast Function Over Multiple Dates\nnowcast_dates &lt;- seq(as.Date(\"2023-02-27\"), as.Date(\"2023-02-28\"), by = \"day\")\nfixed_lag &lt;- 2\nnowcast_results_df &lt;- map2_df(nowcast_dates, fixed_lag, nowcast_function)\n\n# View Nowcast Results\nnowcast_results_df\n\n# A tibble: 2 × 2\n  time_value nowcast_admissions\n  &lt;date&gt;                  &lt;dbl&gt;\n1 2023-02-27               7.68\n2 2023-02-28               8.49"
  },
  {
    "objectID": "slides/day1-afternoon.html#visualize-nowcast-real-time-and-finalized-values",
    "href": "slides/day1-afternoon.html#visualize-nowcast-real-time-and-finalized-values",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Visualize nowcast, real-time, and finalized values",
    "text": "Visualize nowcast, real-time, and finalized values\nFinally, we can compare these nowcast results to the real-time and finalized values:\n\nThe real-time counts tend to be biased slightly below the finalized counts. Nowcasted values tend to provide a better approximation of the truth (at least for these dates)."
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-moving-from-one-signal-to-two",
    "href": "slides/day1-afternoon.html#nowcasting-moving-from-one-signal-to-two",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Nowcasting: Moving from One Signal to Two",
    "text": "Nowcasting: Moving from One Signal to Two\n\nRecall that in nowcasting the goal is to predict a finalized value from a provisional value.\nNow, we’ll move from one signal to two, creating a simple linear model to nowcast.\nExogenous features (predictors) could include relevant signals, such as Google symptom search trends.\nWe will use these signals to nowcast hospital admissions related to influenza."
  },
  {
    "objectID": "slides/day1-afternoon.html#data-sources-google-search-trends-hospital-admissions",
    "href": "slides/day1-afternoon.html#data-sources-google-search-trends-hospital-admissions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Data Sources: Google Search Trends & Hospital Admissions",
    "text": "Data Sources: Google Search Trends & Hospital Admissions\n\nGoogle Search Trends: Symptoms like cough, fever, and shortness of breath.\n\ns01: Cough, Phlegm, Sputum, Upper respiratory tract infection\ns03: Fever, Hyperthermia, Chills, Shivering\n\nHospital Admissions: Data from the Department of Health & Human Services on confirmed influenza admissions.\nUsing these, we will nowcast hospital admissions by using Google symptom search trends for GA from April to June 2023.\nThe first step is to fetch this data…"
  },
  {
    "objectID": "slides/day1-afternoon.html#data-sources-google-search-trends-hospital-admissions-1",
    "href": "slides/day1-afternoon.html#data-sources-google-search-trends-hospital-admissions-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Data Sources: Google Search Trends & Hospital Admissions",
    "text": "Data Sources: Google Search Trends & Hospital Admissions\n\n\nCode\n# Fetch Google symptom data for s01 and s03\nx1 &lt;- pub_covidcast(\n  source = \"google-symptoms\",\n  signals = \"s01_smoothed_search\", \n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = epirange(20230401, 20230701),\n  issues = \"*\"\n) %&gt;%\n  select(geo_value, time_value, version = issue, avg_search_vol_s01 = value) %&gt;%\n  as_epi_archive(compactify = FALSE)\n\nx2 &lt;- pub_covidcast(\n  source = \"google-symptoms\",\n  signals = \"s03_smoothed_search\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = epirange(20230401, 20230701),\n  issues = \"*\"\n) %&gt;%\n  select(geo_value, time_value, version = issue, avg_search_vol_s03 = value) %&gt;%\n  as_epi_archive(compactify = FALSE)\n\n# Fetch hospital admissions data\ny1 &lt;- pub_covidcast(\n  source = \"hhs\",\n  signals = \"confirmed_admissions_influenza_1d\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ga\",\n  time_values = epirange(20230401, 20230701),\n  issues = \"*\"\n) %&gt;%\n  select(geo_value, time_value, version = issue, admissions = value) %&gt;%\n  as_epi_archive(compactify = FALSE)"
  },
  {
    "objectID": "slides/day1-afternoon.html#merging-the-archives",
    "href": "slides/day1-afternoon.html#merging-the-archives",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Merging the Archives",
    "text": "Merging the Archives\n\nWe’ll merge the symptom search trends (x1, x2) with hospital admissions data (y) using epix_merge() from epiprocess.\nThis allows us to match data by time and geography, & fill any missing values with the most recent observation (LOCF)."
  },
  {
    "objectID": "slides/day1-afternoon.html#linear-model-a-simple-approach-for-nowcasting",
    "href": "slides/day1-afternoon.html#linear-model-a-simple-approach-for-nowcasting",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Linear Model: A Simple Approach for Nowcasting",
    "text": "Linear Model: A Simple Approach for Nowcasting\n\nAside from ratios, one of the simplest approach to nowcasting is to use a linear regression model.\nWe model the relationship between provisional (predictor) data and response data.\nThis model helps us make predictions for the finalized data based on the current (provisional) signals."
  },
  {
    "objectID": "slides/day1-afternoon.html#linear-regression",
    "href": "slides/day1-afternoon.html#linear-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nGoal: Estimate the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) that describe the relationship between the predictor \\(x_i\\) and the outcome \\(y_i\\).\nLinear Model: The relationship is assumed to be:\n\\[y_i \\approx \\beta_0 + \\beta_1 x_i \\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope.\nIn R: Use lm(y ~ x) to estimate the coefficients, where y is the outcome variable and x is the predictor."
  },
  {
    "objectID": "slides/day1-afternoon.html#multiple-linear-regression",
    "href": "slides/day1-afternoon.html#multiple-linear-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nGoal: Estimate coefficients \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) that describe the relationship between multiple predictors \\(x_{i1}, x_{i2}, \\dots, x_{ip}\\) and the outcome \\(y_i\\).\nModel: The relationship is assumed to be:\n\\[y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}\\]\nwhere: \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\dots, \\beta_p\\) are the coefficients.\nIn R: Use lm(y ~ x1 + x2 + ... + xp) to estimate the coefficients, where y is the outcome and x1, x2, ..., xp are the predictors."
  },
  {
    "objectID": "slides/day1-afternoon.html#multiple-linear-regression-model",
    "href": "slides/day1-afternoon.html#multiple-linear-regression-model",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Multiple Linear Regression Model",
    "text": "Multiple Linear Regression Model\n\nA linear model is a good choice to describe the relationship between search trends and hospital admissions.\nThe model will include two predictors (s01 and s03).\nWe’ll use these two search trend signals to predict hospital admissions (response)."
  },
  {
    "objectID": "slides/day1-afternoon.html#multiple-linear-regression-model-1",
    "href": "slides/day1-afternoon.html#multiple-linear-regression-model-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Multiple Linear Regression Model",
    "text": "Multiple Linear Regression Model\n\n# Define the function for lm model fit and prediction\nlm_mod_pred &lt;- function(data, gk, rtv, ...) {\n  \n  # Fit the linear model\n  model &lt;- lm(admissions ~ avg_search_vol_s01 + avg_search_vol_s03, data = data)\n  \n  # Make predictions\n  predictions = predict(model,\n                        newdata = data %&gt;%\n                          # Use tidyr::fill() for LOCF if predictor data is incomplete \n                          fill(avg_search_vol_s01, .direction = \"down\") %&gt;% \n                          fill(avg_search_vol_s03, .direction = \"down\") %&gt;%\n                          filter(time_value == max(time_value)),\n                        interval = \"prediction\", level = 0.9\n  )\n\n  # Pull off true time value for comparison to target\n  real_time_val = data %&gt;% filter(time_value == max(time_value)) %&gt;% pull(admissions)\n\n  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val))\n}\n\nNote that this code is intentionally simple; while it can be refined to handle cases like negatives or other boundary conditions, we aim to avoid unnecessary complexity."
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-with-epix_slide",
    "href": "slides/day1-afternoon.html#nowcasting-with-epix_slide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Nowcasting with epix_slide()",
    "text": "Nowcasting with epix_slide()\n\nWe will use epix_slide() to create a sliding window of training data.\nThe model will be trained on a 14-day window before the target date, and predictions will be made for the target date.\nThe beauty of this function is that it is version-aware - the sliding computation at any given reference time t is performed on data that would have been available as of t automatically."
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-with-epix_slide-1",
    "href": "slides/day1-afternoon.html#nowcasting-with-epix_slide-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Nowcasting with epix_slide()",
    "text": "Nowcasting with epix_slide()\n\n# Define the reference time points for nowcasting\ntargeted_nowcast_dates &lt;- seq(as.Date(\"2023-04-15\"), as.Date(\"2023-06-15\"), by = \"1 week\")\nref_time_values = targeted_nowcast_dates + 2  # Adjust for the systematic 2-day latency in the response\n# Determine this from revision_summary(y1, print_inform = TRUE) \n\n# Perform nowcasting using epix_slide\nnowcast_res &lt;- archive %&gt;%\n  group_by(geo_value) %&gt;%\n  epix_slide(\n    .f = lm_mod_pred,\n    .before = 14,  # 14-day training period\n    .versions = ref_time_values, \n    .new_col_name = \"res\"\n  ) %&gt;%\n  unnest() %&gt;% # Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns. \n  mutate(targeted_nowcast_date = targeted_nowcast_dates, time_value = actual_nowcast_date) %&gt;%\n  ungroup()\n\n# View results\nhead(nowcast_res, n=2)\n\n# A tibble: 2 × 9\n  geo_value version      fit    lwr   upr actual_nowcast_date real_time_val\n  &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;                      &lt;dbl&gt;\n1 ga        2023-04-17  4.53 -0.331  9.39 2023-04-15                      4\n2 ga        2023-04-24  7.58  1.64  13.5  2023-04-22                      4\n# ℹ 2 more variables: targeted_nowcast_date &lt;date&gt;, time_value &lt;date&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#compare-with-the-actual-admissions",
    "href": "slides/day1-afternoon.html#compare-with-the-actual-admissions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Compare with the Actual Admissions",
    "text": "Compare with the Actual Admissions\n\nAfter making predictions, we compare them to the actual hospital admissions.\n\n\n# Left join with latest results \n# Latest snapshot of data (with the latest/finalized admissions)\nx_latest &lt;- epix_as_of(archive, max_version = max(archive$DT$version)) %&gt;% select(-c(avg_search_vol_s01, avg_search_vol_s03))\n\nres &lt;- nowcast_res %&gt;% left_join(x_latest, by = c(\"geo_value\", \"time_value\"))\nhead(res)\n\n# A tibble: 6 × 10\n  geo_value version      fit    lwr   upr actual_nowcast_date real_time_val\n  &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;                      &lt;dbl&gt;\n1 ga        2023-04-17  4.53 -0.331  9.39 2023-04-15                      4\n2 ga        2023-04-24  7.58  1.64  13.5  2023-04-22                      4\n3 ga        2023-05-01  5.89  1.88   9.91 2023-04-29                      5\n4 ga        2023-05-08  4.57  0.450  8.70 2023-05-06                      6\n5 ga        2023-05-15  7.95  4.91  11.0  2023-05-11                      8\n6 ga        2023-05-22  2.84 -2.00   7.68 2023-05-20                      4\n# ℹ 3 more variables: targeted_nowcast_date &lt;date&gt;, time_value &lt;date&gt;,\n#   admissions &lt;dbl&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#visualizing-the-nowcast-results",
    "href": "slides/day1-afternoon.html#visualizing-the-nowcast-results",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Visualizing the Nowcast Results",
    "text": "Visualizing the Nowcast Results\n\nWe can then visualize the nowcast results alongside the true values using ggplot2:"
  },
  {
    "objectID": "slides/day1-afternoon.html#key-takeaways-linear-regression-nowcasting-example",
    "href": "slides/day1-afternoon.html#key-takeaways-linear-regression-nowcasting-example",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Key Takeaways: Linear Regression Nowcasting Example",
    "text": "Key Takeaways: Linear Regression Nowcasting Example\n\nProvisional Data as Predictors: Using Google symptom search trends (s01, s03) to predict influenza hospital admissions.\nSimple Linear Model: A linear regression model captures the relationship between symptom searches and hospital admissions.\nActionable Predictions: Nowcasts provide timely insights for hospital admissions, even before data is finalized.\nSliding Window Approach: Predictions are based on data up to the current time, ensuring no future information influences the nowcast.\nEvaluation: Predictions are compared with actual admissions visually."
  },
  {
    "objectID": "slides/day1-afternoon.html#goal-of-this-case-study",
    "href": "slides/day1-afternoon.html#goal-of-this-case-study",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Goal of this Case Study",
    "text": "Goal of this Case Study\nGoal: Nowcast COVID-19 Case Rates in Massachusetts using %CLI.\n\n%CLI is contained in the Epidata API.\nCase rates by specimen collection date are not. They are from the MA gov website.\nCase rates in the API (JHU) are aligned by report date, not specimen collection/test date.\nAligning by test date allows us to avoid the more unpredictable delays introduced by the report date."
  },
  {
    "objectID": "slides/day1-afternoon.html#summary-of-main-steps",
    "href": "slides/day1-afternoon.html#summary-of-main-steps",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Summary of main steps",
    "text": "Summary of main steps\n\nThe workflow is similar to the previous example where we nowcasted using two variables, only more involved. The main steps are…\n\nFetch Data: Retrieve %CLI and COVID-19 case data (by specimen collection date) for MA.\nMerge Data: Align %CLI and case rate data using epix_merge, filling missing values via last observation carried forward (LOCF).\nModel & Prediction: Fit a linear model to predict case rates based on %CLI, trained on a 30-day rolling window.\nNowcast Execution: Use epix_slide to nowcast the case rates dynamically.\nVisualization: Plot actual vs. nowcasted case rates with confidence intervals to assess model accuracy.\n\nSo the first step is to fetch the data…"
  },
  {
    "objectID": "slides/day1-afternoon.html#construct-an-epi_archive-from-scratch",
    "href": "slides/day1-afternoon.html#construct-an-epi_archive-from-scratch",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Construct an epi_archive from scratch",
    "text": "Construct an epi_archive from scratch\n\nHere’s the archive of COVID-19 case excel files from the MA gov website, which we’ll use to construct our own epi_archive.  Brief summary of this data:\n\nFirst release: Raw .xlsx data was first released early January 2021.\nChange in reporting: Starting July 1, 2021, the dashboard shifted from 7 days/week to 5 days/week (Monday-Friday).\nFriday, Saturday, and Sunday data is included in the Monday dashboard.\nWhen Monday is a holiday, the Friday through Monday data is posted on Tuesday."
  },
  {
    "objectID": "slides/day1-afternoon.html#construct-an-epi_archive-from-scratch-1",
    "href": "slides/day1-afternoon.html#construct-an-epi_archive-from-scratch-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Construct an epi_archive from scratch",
    "text": "Construct an epi_archive from scratch\n\n\nPurpose: To create an epi_archive object for storing versioned time series data.\nRequired Columns:\n\ngeo_value: Geographic data (e.g., region).\ntime_value: Time-related data (e.g., date, time).\nversion: Tracks when the data was available (enables version-aware forecasting).\n\nConstructor:\n\nnew_epi_archive(): For manual construction of epi_archive (assumes validation of inputs).\n\nRecommended Method:\n\nas_epi_archive(): Simplifies the creation process, ensuring proper formatting and validation. We’ll use this one when we download some data from the MA gov website!"
  },
  {
    "objectID": "slides/day1-afternoon.html#main-steps-to-construct-the-epi_archive",
    "href": "slides/day1-afternoon.html#main-steps-to-construct-the-epi_archive",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Main steps to construct the epi_archive",
    "text": "Main steps to construct the epi_archive\n\n\nLoad necessary Libraries: Such as tidyverse, readxl, epiprocess.\nProcess Each Date’s Data:\n\nA function we’ll make (process_covid_data) downloads and processes daily COVID-19 data from the MA gov Excel files on their website.\nThe data is cleaned and formatted with columns: geo_value, time_value, version, and values.\n\nHandle Missing Data: Checks if a date’s data is available (handle 404 errors).\nCreate epi_archive:\n\nCombine processed data into a tibble.\nConvert the tibble to an epi_archive object using as_epi_archive()."
  },
  {
    "objectID": "slides/day1-afternoon.html#fetch-data---code-for-one-date",
    "href": "slides/day1-afternoon.html#fetch-data---code-for-one-date",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetch Data - Code for one date",
    "text": "Fetch Data - Code for one date\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)\nlibrary(tibble)\nlibrary(epiprocess)\n\n# Function to download and process each Excel file for a given date\nprocess_covid_data &lt;- function(Date) {\n  # Generate the URL for the given date\n  url &lt;- paste0(\"https://www.mass.gov/doc/covid-19-raw-data-\", tolower(gsub(\"-0\", \"-\", format(Date, \"%B-%d-%Y\"))), \"/download\") \n  # Applies gsub(\"-0\", \"-\", ...) to replace any occurrence of -0 (such as in \"April-01\") with just - (resulting in \"April-1\").\n  \n  # Check if the URL exists (handle the 404 error by skipping that date)\n  response &lt;- GET(url)\n  \n  if (status_code(response) != 200) {\n    return(NULL)  # Skip if URL doesn't exist (404)\n  }\n  \n  # Define the destination file path for the Excel file\n  file_path &lt;- tempfile(fileext = \".xlsx\")\n  \n  # Download the Excel file\n  GET(url, write_disk(file_path, overwrite = TRUE))\n  \n  # Read the relevant sheet from the Excel file\n  data &lt;- read_excel(file_path, sheet = \"CasesByDate (Test Date)\")\n  \n  # Process the data: rename columns and convert Date\n  data &lt;- data %&gt;%\n    rename(\n      Date = `Date`,\n      Positive_Total = `Positive Total`,\n      Positive_New = `Positive New`,\n      Case_Average_7day = `7-day confirmed case average`\n    ) %&gt;%\n    mutate(Date = as.Date(Date))  # Convert to Date class\n  \n  # Create a tibble with the required columns for the epi_archive\n  tib &lt;- tibble(\n    geo_value = \"ma\",  # Massachusetts (geo_value)\n    time_value = data$Date,  # Date from the data\n    version = Date,  # The extracted version date\n    case_rate_7d_av = data$Case_Average_7day  # 7-day average case value\n  )\n  \n  return(tib)\n}"
  },
  {
    "objectID": "slides/day1-afternoon.html#fetch-data---code-breakdown",
    "href": "slides/day1-afternoon.html#fetch-data---code-breakdown",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetch Data - Code breakdown",
    "text": "Fetch Data - Code breakdown\n\n\nThis purpose of this function is to download and process each Excel file as of a date.\nURL Creation: Dynamically generates the URL based on the date, removing leading zeros in day values (e.g., “April-01” → “April-1”).\nCheck URL: Sends a request (GET(url)) and skips the date if the URL returns a non-200 status (e.g., 404 error).\nDownload File: Saves the Excel file to a temporary path using tempfile() and GET().\nRead Data: Loads the relevant sheet (“CasesByDate”) from the Excel file using read_excel().\nTibble Creation: Constructs a tibble with geo_value, time_value, version, and case_rate_7d_av to later compile into an epi_archive (you can think of an epi_archive as being a comprised of many epi_dfs)."
  },
  {
    "objectID": "slides/day1-afternoon.html#fetch-data---process-range-of-dates",
    "href": "slides/day1-afternoon.html#fetch-data---process-range-of-dates",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetch Data - Process range of dates",
    "text": "Fetch Data - Process range of dates\n\nNote that process_covid_data() works on one date at a time.\nSo now, we need a function that iterates over a date range and applies process_covid_data() to each date & combines the resulting tibbles into an epi_archive.\nWe call this function process_data_for_date_range()…"
  },
  {
    "objectID": "slides/day1-afternoon.html#fetch-data---process-range-of-dates-1",
    "href": "slides/day1-afternoon.html#fetch-data---process-range-of-dates-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetch Data - Process range of dates",
    "text": "Fetch Data - Process range of dates\n\n# Function to process data for a range of dates\nprocess_data_for_date_range &lt;- function(start_date, end_date) {\n  # Generate a sequence of dates between start_date and end_date\n  date_sequence &lt;- seq(as.Date(start_date), as.Date(end_date), by = \"day\")\n  \n  # Process data for each date and combine results\n  covid_data_list &lt;- lapply(date_sequence, function(Date) {\n    process_covid_data(Date)  # Skip over dates with no data (NULLs will be ignored)\n  })\n  \n  # Combine all non-null individual tibbles into one data frame\n  combined_data &lt;- bind_rows(covid_data_list[!sapply(covid_data_list, is.null)])\n  \n  # Convert the combined data into an epi_archive object\n  if (nrow(combined_data) &gt; 0) {\n    epi_archive_data &lt;- combined_data %&gt;%\n      as_epi_archive(compactify = FALSE)\n    \n    return(epi_archive_data)\n  } else {\n    message(\"No valid data available for the given date range.\")\n    return(NULL)\n  }\n}"
  },
  {
    "objectID": "slides/day1-afternoon.html#fetch-data---code-breakdown-1",
    "href": "slides/day1-afternoon.html#fetch-data---code-breakdown-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetch Data - Code breakdown",
    "text": "Fetch Data - Code breakdown\nHere’s a summary of what process_data_for_date_range() does: 1. Generates Date Range: Creates a sequence of dates between start_date and end_date.\n\nProcesses Data: Applies the process_covid_data function to each date in the range (skip over dates with no data).\nCombines Results: Combines all valid (non-NULL) tibbles into one single data frame.\nCreates epi_archive: Converts the combined data into an epi_archive object."
  },
  {
    "objectID": "slides/day1-afternoon.html#fetch-data---run-the-function-inspect-archive",
    "href": "slides/day1-afternoon.html#fetch-data---run-the-function-inspect-archive",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetch Data - Run the function & inspect archive",
    "text": "Fetch Data - Run the function & inspect archive\n\n\nNow, let’s run the function & inspect the resulting epi_archive of 7-day averaged COVID-19 case counts:\nExpect building the archive to take a nontrivial amount of time (enough for a cup of coffee or to meditate on life).\n\n\n\n\n# Example usage: process data between Jan. 10, 2021, and Dec. 1, 2021\ny &lt;- process_data_for_date_range(\"2021-01-10\", \"2021-12-01\")  # Raw .xlsx data is first released on Jan. 4, 2021\ny\n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-01-29 / 2021-11-30\nℹ First/last version with update: 2021-01-10 / 2021-12-01\nℹ Versions end: 2021-12-01\nℹ A preview of the table (135549 rows x 4 columns):\nKey: &lt;geo_value, time_value, version&gt;\n        geo_value time_value    version case_rate_7d_av\n           &lt;char&gt;     &lt;Date&gt;     &lt;Date&gt;           &lt;num&gt;\n     1:        ma 2020-01-29 2021-01-10              NA\n     2:        ma 2020-01-29 2021-01-11              NA\n     3:        ma 2020-01-29 2021-01-12              NA\n     4:        ma 2020-01-29 2021-01-13              NA\n     5:        ma 2020-01-29 2021-01-14              NA\n    ---                                                \n135545:        ma 2021-11-28 2021-11-30        2196.000\n135546:        ma 2021-11-28 2021-12-01        2352.286\n135547:        ma 2021-11-29 2021-11-30        1735.714\n135548:        ma 2021-11-29 2021-12-01        2371.286\n135549:        ma 2021-11-30 2021-12-01        1972.143"
  },
  {
    "objectID": "slides/day1-afternoon.html#fetch-data---outpatient-doctors-visits-for-cli",
    "href": "slides/day1-afternoon.html#fetch-data---outpatient-doctors-visits-for-cli",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetch Data - % Outpatient Doctors Visits for CLI",
    "text": "Fetch Data - % Outpatient Doctors Visits for CLI\n\n\nNow, from the Epidata API, let’s download the estimated percentage of outpatient doctor visits primarily for COVID-related symptoms, based on health system data.\nComes pre-smoothed in time using a Gaussian linear smoother\nThis will be the predictor when we nowcast COVID-19 Case Rates in MA.\n\n\n\n# Step 1: Fetch Versioned Data \nx &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ma\", # Just for MA to keep it simple (& to go with the case data by test date for that state)\n  time_value = epirange(20210301, 20212101), \n  issues = epirange(20210301, 20212101)\n) %&gt;%\n  select(geo_value, time_value,\n         version = issue,\n         percent_cli = value\n  ) %&gt;%\n  as_epi_archive(compactify = FALSE)"
  },
  {
    "objectID": "slides/day1-afternoon.html#use-epix_merge-to-merge-the-two-archives",
    "href": "slides/day1-afternoon.html#use-epix_merge-to-merge-the-two-archives",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Use epix_merge() to merge the two archives",
    "text": "Use epix_merge() to merge the two archives\nNow we’ll use epix_merge() to combine the two epi_archives that share the same geo_value & time_value.\n\n\narchive &lt;- epix_merge(\n  x, y,\n  sync = \"locf\",\n  compactify = FALSE\n)\narchive\n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-01-29 / 2021-12-13\nℹ First/last version with update: 2021-01-10 / 2021-12-17\nℹ Versions end: 2021-12-17\nℹ A preview of the table (139190 rows x 5 columns):\nKey: &lt;geo_value, time_value, version&gt;\n        geo_value time_value    version percent_cli case_rate_7d_av\n           &lt;char&gt;     &lt;Date&gt;     &lt;Date&gt;       &lt;num&gt;           &lt;num&gt;\n     1:        ma 2020-01-29 2021-01-10          NA              NA\n     2:        ma 2020-01-29 2021-01-11          NA              NA\n     3:        ma 2020-01-29 2021-01-12          NA              NA\n     4:        ma 2020-01-29 2021-01-13          NA              NA\n     5:        ma 2020-01-29 2021-01-14          NA              NA\n    ---                                                            \n139186:        ma 2021-12-11 2021-12-16    2.306966              NA\n139187:        ma 2021-12-11 2021-12-17    2.281141              NA\n139188:        ma 2021-12-12 2021-12-16    2.333759              NA\n139189:        ma 2021-12-12 2021-12-17    2.369756              NA\n139190:        ma 2021-12-13 2021-12-17    2.256551              NA"
  },
  {
    "objectID": "slides/day1-afternoon.html#fitting-and-predicting-with-linear-model",
    "href": "slides/day1-afternoon.html#fitting-and-predicting-with-linear-model",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fitting and Predicting with Linear Model",
    "text": "Fitting and Predicting with Linear Model\n\n\nDefine lm_mod_pred(): A function that fits a linear model to forecast case rates based on the percent_cli predictor.\nUse predict() with a 90% prediction interval.\nSave the actual case rates to compare to the nowcasts later.\n\n\n\nlm_mod_pred &lt;- function(data, ...) {\n  # Linear model\n  model &lt;- lm(case_rate_7d_av ~ percent_cli, data = data)\n\n  # Make predictions\n  predictions = predict(model,\n                        newdata = data %&gt;%\n                          fill(percent_cli, .direction = \"down\") %&gt;% \n                          filter(time_value == max(time_value)),\n                        interval = \"prediction\", level = 0.9)\n  \n  # Pull off real-time value for later comparison to the nowcast value\n  real_time_val = data %&gt;% filter(time_value == max(time_value)) %&gt;% pull(case_rate_7d_av)\n  \n  # Could clip predictions and bounds at 0\n  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val)) \n}"
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-with-epix_slide-2",
    "href": "slides/day1-afternoon.html#nowcasting-with-epix_slide-2",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Nowcasting with epix_slide()",
    "text": "Nowcasting with epix_slide()\n\nSpecify targets: Define the target dates for nowcasting (e.g., 1st of each month) & adjust training data to include the lag for the latent case data.\nSliding window: Use epix_slide() to apply the linear model across a sliding window of data for each region.\nTraining-test split: Use the last 30 days of data to train and predict case rates for each target nowcast date."
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-with-epix_slide-3",
    "href": "slides/day1-afternoon.html#nowcasting-with-epix_slide-3",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Nowcasting with epix_slide()",
    "text": "Nowcasting with epix_slide()\n\n# Define the reference time points (to give the training/test split)\ntargeted_nowcast_dates &lt;- seq(as.Date(\"2021-04-01\"), as.Date(\"2021-11-01\"), by = \"1 month\") \nref_time_values = targeted_nowcast_dates + 1 # + 1 because the case data is 1 day latent. \n# Determine this from revision_summary(y)\n\n# Use epix_slide to perform the nowcasting with a training-test split\nnowcast_res &lt;- archive %&gt;%\n  group_by(geo_value) %&gt;%\n  epix_slide(\n    .f = lm_mod_pred,  # Pass the function defined above\n    .before = 30,   # Training period of 30 days\n    .versions = ref_time_values, # Determines the day where training data goes up to (not inclusive)\n    .new_col_name = \"res\"\n  ) %&gt;%\n  unnest() %&gt;%\n  mutate(targeted_nowcast_date = targeted_nowcast_dates,\n         time_value = actual_nowcast_date)\n\n# Take a peek at the results\nhead(nowcast_res, n = 1)\n\n# A tibble: 1 × 9\n# Groups:   geo_value [1]\n  geo_value version      fit   lwr   upr actual_nowcast_date real_time_val\n  &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;                      &lt;dbl&gt;\n1 ma        2021-04-02 2114. 1975. 2254. 2021-04-01                  1556.\n# ℹ 2 more variables: targeted_nowcast_date &lt;date&gt;, time_value &lt;date&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#visualizing-nowcasts-vs.-actual-values",
    "href": "slides/day1-afternoon.html#visualizing-nowcasts-vs.-actual-values",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Visualizing Nowcasts vs. Actual Values",
    "text": "Visualizing Nowcasts vs. Actual Values\nMerge the nowcast results with the latest data for more direct comparison:\n\nx_latest &lt;- epix_as_of(archive, max_version = max(archive$DT$version)) %&gt;%\n  select(-percent_cli) \n\nres &lt;- nowcast_res %&gt;% left_join(x_latest, by = c(\"geo_value\", \"time_value\"))\n\nres\n\n# A tibble: 8 × 10\n# Groups:   geo_value [1]\n  geo_value version       fit    lwr   upr actual_nowcast_date real_time_val\n  &lt;chr&gt;     &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;                      &lt;dbl&gt;\n1 ma        2021-04-02 2114.  1975.  2254. 2021-04-01                  1556.\n2 ma        2021-05-02 1083.   851.  1316. 2021-05-01                   869.\n3 ma        2021-06-02  353.   164.   541. 2021-06-01                   117.\n4 ma        2021-07-02   57.1   11.3  103. 2021-07-01                    59 \n5 ma        2021-08-02  513.   284.   742. 2021-08-01                   572.\n6 ma        2021-09-02 1207.   888.  1527. 2021-09-01                  1099.\n7 ma        2021-10-02 1575.  1357.  1793. 2021-09-30                  1069 \n8 ma        2021-11-02 1299.  1257.  1340. 2021-11-01                   891.\n# ℹ 3 more variables: targeted_nowcast_date &lt;date&gt;, time_value &lt;date&gt;,\n#   case_rate_7d_av &lt;dbl&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#visualizing-nowcasts-vs.-actual-values-1",
    "href": "slides/day1-afternoon.html#visualizing-nowcasts-vs.-actual-values-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Visualizing Nowcasts vs. Actual Values",
    "text": "Visualizing Nowcasts vs. Actual Values\n\nFinally, plot the predictions & real-time values on top of latest COVID-19 case rates using ggplot2:"
  },
  {
    "objectID": "slides/day1-afternoon.html#takeaways",
    "href": "slides/day1-afternoon.html#takeaways",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Takeaways",
    "text": "Takeaways\nGoal: Predict COVID-19 case rates using %CLI, overcoming delays in report data.\nMain Steps:\n\nFetch Data: Collect case rates and %CLI data.\nMerge Data: Align datasets with epix_merge() and fill missing values.\nModel: Fit a linear model to predict case rates.\nNowcast: Apply dynamic forecasting with epix_slide().\nVisualize: Plot nowcasts vs. actual case rates with confidence intervals.\n\nOverall, nowcasting, based on the linear model, provided a closer approximation of true case rates compared to the real-time values."
  },
  {
    "objectID": "slides/day1-afternoon.html#final-slide",
    "href": "slides/day1-afternoon.html#final-slide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Final slide",
    "text": "Final slide\nThanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nExplore, clean & transform data — cmu-delphi/insightnet-workshop-2024"
  },
  {
    "objectID": "slides/day1-morning.html#section",
    "href": "slides/day1-morning.html#section",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Signal Discovery & Data Fetching",
    "text": "Signal Discovery & Data Fetching"
  },
  {
    "objectID": "slides/day1-morning.html#outline",
    "href": "slides/day1-morning.html#outline",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Outline",
    "text": "Outline\n\nAbout Delphi\nGoals of Delphi Epidata platform\nEpidata API\nFinding data sources and signals\nVersioned data"
  },
  {
    "objectID": "slides/day1-morning.html#about-delphi",
    "href": "slides/day1-morning.html#about-delphi",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "About Delphi",
    "text": "About Delphi\n\nFounded in 2012 at Carnegie Mellon University, now expanded to UC Berkeley, and University of British Columbia.\nCurrently 5 faculty, ~10 PhD students, ~15 staff (mostly software engineers).\nEasy to join us from anywhere (lots of volunteers during Covid-19 pandemic).\nWe are:\n\nCDC Center of Excellence for Influenza and Covid-19 Forecasting (2019-24).\nCDC Innovation Center for Outbreak Analytics and Disease Modeling (2024-29).\n\n\nOur mission: To develop the theory and practice of epidemic detection, tracking and forecasting, and their use in decision making, both public and private."
  },
  {
    "objectID": "slides/day1-morning.html#what-does-delphi-do",
    "href": "slides/day1-morning.html#what-does-delphi-do",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What does Delphi do?",
    "text": "What does Delphi do?\n\nProcure real-time, aggregated data streams informative of infectious diseases and syndromes, in collaboration with partners in industry and government.\nExtract signals and make them widely available via the Epidata platform & API.\nDevelop and deploy algorithms for epidemic detection, tracking, forecasting.\nDevelop and maintain statistical software packages for these tasks.\nMake it all production-grade, maximally-accessible, and open-source (to serve CDC, state and local public health agencies, epi-forecasting researchers, data journalists, the public)"
  },
  {
    "objectID": "slides/day1-morning.html#what-we-provide",
    "href": "slides/day1-morning.html#what-we-provide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What we provide",
    "text": "What we provide"
  },
  {
    "objectID": "slides/day1-morning.html#goals-of-delphi-epidata-platform-and-repository",
    "href": "slides/day1-morning.html#goals-of-delphi-epidata-platform-and-repository",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Goals of Delphi Epidata platform and repository",
    "text": "Goals of Delphi Epidata platform and repository\n\n\nBe the one-stop shop for aggregated epi-surveillance time-series (“epi-signals”)\n\nHence: include also signals available elsewhere, especially if they don’t keep data revisions - E.g. CDC’s own NSSP, NWSS\nBe the national historical repository of record & preserve the raw data\n\nBe the national clearinghouse for epi-signals, including those held elsewhere\n\nThe go-to place for signal discovery\n\nAdd value to existing signals and synthesize new ones\n\nAdded value: see next slide\nSynthesize new: via signal fusion, e.g. nowcasting\n\nBe the focal point for community-wide efforts to open up privately held data\n\nBetter positioned than government or industry"
  },
  {
    "objectID": "slides/day1-morning.html#the-bigger-goal",
    "href": "slides/day1-morning.html#the-bigger-goal",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "The bigger goal",
    "text": "The bigger goal\nThe goal is to make epi-surveillance more nimble, complete, standardized, robust, and real-time; and less burdensome on the health system itself. Epidata is not the solution; but we hope it is a blueprint towards such a solution."
  },
  {
    "objectID": "slides/day1-morning.html#what-is-the-epidata-repository",
    "href": "slides/day1-morning.html#what-is-the-epidata-repository",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What is the Epidata repository",
    "text": "What is the Epidata repository\nEpidata is a repository of aggregated epi-surveillance time series. To the full extent we can, we make everything free and open-source.\n\nTo date, it has accumulated over 5 billion records (each record is the value of a signal, at a particular date, and a particular location).\nAt the peak of the pandemic, we were receiving millions of API queries per day.\nData comes from: public health reporting, medical insurance claims, medical device data, Google search queries, wastewater, app-based mobility patterns.\nMany of our data streams simply aren’t available anywhere else.\nAdded value we provide: revision tracking, anomaly detection, trend detection, smoothing, imputation, geo-temporal-demographic disaggregation"
  },
  {
    "objectID": "slides/day1-morning.html#features-of-delphi-epidata",
    "href": "slides/day1-morning.html#features-of-delphi-epidata",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Features of Delphi Epidata",
    "text": "Features of Delphi Epidata\n\n\nBuilt-in support for:\n\nData revisions (“backfill”). Concepts of “reporting date” and “as of”.\nBackfill projection and alerting to changes in backfill dynamics\nGeo levels w/ auto-aggregation: county, MSA, HRR, state, HHS region, nation\n\nAlso esoteric ones: DMA, sewer sheds\n\nDemographic breakdown\nRepresentation for missingness and censoring\nPopulation sizes and fine-grained population density\n\nPre-computed smoothing and normalization (customization planned)\nAccess control\nCode is Open Source. Signals are as accessible (w/ API, SDK) as allowed by DUAs"
  },
  {
    "objectID": "slides/day1-morning.html#epidata-api-1",
    "href": "slides/day1-morning.html#epidata-api-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Epidata API",
    "text": "Epidata API\n\nDelphi’s Epidata API provides real-time access to epidemiological surveillance data.\nThe main endpoint (covidcast) providing daily updates about current COVID-19 and influenza activity across the United States.\nA variety of other endpoints, providing primarily historical data about various diseases including COVID-19, influenza, dengue fever, and norovirus in several countries.\nA full-featured R client is available for quick access to all data.\nA Legacy Python client is available, full-featured Python client in development."
  },
  {
    "objectID": "slides/day1-morning.html#some-of-our-data-sources",
    "href": "slides/day1-morning.html#some-of-our-data-sources",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Some of our data sources",
    "text": "Some of our data sources\n\nOngoing Sources:\n\nInsurance claims: %Covid {inpatient, outpatient}, by {county x day}\nGoogle Symptom searches: 7 symptoms groups, by {county x day}\nQuidel/Ortho antigen tests: %Covid by age group, by {county x day}\nNCHS Deaths: all-cause, pneumonia, flu, Covid, by {state x week}\nNSSP ED visits: %Covid, %flu, %RSV, by {county x week} (new!)\nNWSS Covid, by {sampling-site x day} (in progress)"
  },
  {
    "objectID": "slides/day1-morning.html#some-of-our-data-sources-1",
    "href": "slides/day1-morning.html#some-of-our-data-sources-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Some of our data sources",
    "text": "Some of our data sources\n\nActive during the pandemic, and could be restarted for the next PHE:\n\nHHS Hosp/ICU beds: Covid, flu, by age-group, by {state x day}, {facility x week}\nCTIS (“Delphi Facebook Survey”): many dozens of questions, by (county x day)\nSTLT-reported {cases, deaths} via {JHU, USAFacts}, by (country x day)\nSafegraph mobility: misc measures by {county x day},{county x week}"
  },
  {
    "objectID": "slides/day1-morning.html#some-of-our-pre-pandemic-data-sources",
    "href": "slides/day1-morning.html#some-of-our-pre-pandemic-data-sources",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Some of our Pre-Pandemic Data Sources",
    "text": "Some of our Pre-Pandemic Data Sources\n\n\nFluView ILINet, by {state x week}\nFluView Clinical (% positive flu, PH and clinical labs)\nGoogle Health Trends (GHT), precursor to Google Symptoms\nGoogle Flu Trends (GFT), precursor to to GHT\nTwitter flu\nAccess counts for flu-related CDC pages, by {city x week}\nAccess counts for flu-related Wikipedia entries by {day x hour}\nFlu-surv (flu hosp rates, now expanded to RESP-NET)\nMisc signals for dengue, norovirus\nMisc signals for PAHO countries, ECDC, KCDC, Taiwan,…\nDelphi ILI nowcasts, by {state x week}, visualized in “ILI Nearby” website\nDelphi ILI forecasts, by {state x week}"
  },
  {
    "objectID": "slides/day1-morning.html#severity-pyramid",
    "href": "slides/day1-morning.html#severity-pyramid",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Severity pyramid",
    "text": "Severity pyramid"
  },
  {
    "objectID": "slides/day1-morning.html#installing-epidatr",
    "href": "slides/day1-morning.html#installing-epidatr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Installing epidatr",
    "text": "Installing epidatr\nInstalling the package is straightforward:\n\n# Install the CRAN version\npak::pkg_install(\"epidatr\")\n# Install the development version from the GitHub dev branch\n# pak::pkg_install(\"cmu-delphi/epidatr@dev\")\n\nThe CRAN listing is here.\n\n\n\nPython\n\n\nIn Python, install delphi-epidata from PyPI with pip install delphi-epidata."
  },
  {
    "objectID": "slides/day1-morning.html#using-epidatr-and-epidatpy",
    "href": "slides/day1-morning.html#using-epidatr-and-epidatpy",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using epidatr and epidatpy",
    "text": "Using epidatr and epidatpy\n\nThe following shows how to import the library and fetch Delphi’s COVID-19 Surveillance Streams from Facebook Survey CLI for county 06001:\n\n\n# Configure API key interactively, if needed. See\n# https://cmu-delphi.github.io/epidatr/articles/epidatr.html#api-keys for details.\n#save_api_key()\nlibrary(epidatr)\nres &lt;- pub_covidcast('fb-survey', 'smoothed_cli', 'county', 'day', geo_values = '06001',\n                     time_values = c(20200401, 20200405:20200414))\nhead(res)\n\n# A tibble: 6 × 15\n  geo_value signal     source geo_type time_type time_value direction issue     \n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;    \n1 06001     smoothed_… fb-su… county   day       2020-04-06        NA 2020-09-03\n2 06001     smoothed_… fb-su… county   day       2020-04-07        NA 2020-09-03\n3 06001     smoothed_… fb-su… county   day       2020-04-08        NA 2020-09-03\n4 06001     smoothed_… fb-su… county   day       2020-04-09        NA 2020-09-03\n5 06001     smoothed_… fb-su… county   day       2020-04-10        NA 2020-09-03\n6 06001     smoothed_… fb-su… county   day       2020-04-11        NA 2020-09-03\n# ℹ 7 more variables: lag &lt;dbl&gt;, missing_value &lt;dbl&gt;, missing_stderr &lt;dbl&gt;,\n#   missing_sample_size &lt;dbl&gt;, value &lt;dbl&gt;, stderr &lt;dbl&gt;, sample_size &lt;dbl&gt;"
  },
  {
    "objectID": "slides/day1-morning.html#api-keys",
    "href": "slides/day1-morning.html#api-keys",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "API keys",
    "text": "API keys\n\n\nAnyone may access the Epidata API anonymously without providing any personal data.\nAnonymous API access is subject to the following restrictions:\n\npublic datasets only\nrate limited to 60 requests per hour\nonly two parameters may have multiple selections\n\nAn API key grants priviledged access to the Epidata API and can be obtained by registering with us.\nPrivileges of registration:\n\nno rate limit\nno limit on multiple selections\n\n\n\n\n\n\n\n\n\nTip\n\n\nThe epidatr client automatically searches for the key in the DELPHI_EPIDATA_KEY environment variable. We recommend storing it in your .Renviron file, which R reads by default. More on setting your API key here."
  },
  {
    "objectID": "slides/day1-morning.html#finding-data-sources-and-signals-of-interest",
    "href": "slides/day1-morning.html#finding-data-sources-and-signals-of-interest",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Finding data sources and signals of interest",
    "text": "Finding data sources and signals of interest\n\n\nDiverse Data Streams\n\nVariety of Data: Access to medical claims data, cases and deaths, mobility data, and more.\nGeographic Coverage: Includes multiple regions, making it comprehensive yet complex.\nChallenge: Difficulty in pinpointing the specific data stream of interest.\n\nUsing the Documentation\n\nComprehensive Listings: Documentation details all available data sources and signals for both COVID-19 and other endpoints.\n\nDocs are great for a deep dive into the data, whereas the apps & tools are useful to see what is available…"
  },
  {
    "objectID": "slides/day1-morning.html#cheatsheet-of-tools-we-provide",
    "href": "slides/day1-morning.html#cheatsheet-of-tools-we-provide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Cheatsheet of tools we provide",
    "text": "Cheatsheet of tools we provide\nWe provide…\n\nA signal discovery app, to explore what epi-signals are available in Delphi Epidata and elsewhere in the community.\nA general signal visualization tool.\nA signal dashboard and a “classic” map-based version to visualize a core set of COVID-19 and flu indicators.\nA COVID-19 signal export app, a dashboard builder, and more!"
  },
  {
    "objectID": "slides/day1-morning.html#signal-dashboard---for-covid-19-flu-data",
    "href": "slides/day1-morning.html#signal-dashboard---for-covid-19-flu-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Signal dashboard - For COVID-19 & flu data",
    "text": "Signal dashboard - For COVID-19 & flu data\n\n\n\nThe signal dashboard displays a selection of signals for COVID-19 & flu.\nBrowse by location or indicator to choose which signal you are interested in & then export the data for analysis.\nExample: Symptom searches on Google in NC"
  },
  {
    "objectID": "slides/day1-morning.html#signal-discovery-app---browse-for-more-data",
    "href": "slides/day1-morning.html#signal-discovery-app---browse-for-more-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Signal discovery app - Browse for more data",
    "text": "Signal discovery app - Browse for more data\n\nSignal discovery app: An easy way to find data sources and signals (no programming required).\n\nSearch tool that is a good to browse & find data.\n\nLet’s try it out together!"
  },
  {
    "objectID": "slides/day1-morning.html#example---nchs-weekly-flu-mortality-data-in-states",
    "href": "slides/day1-morning.html#example---nchs-weekly-flu-mortality-data-in-states",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example - NCHS Weekly Flu Mortality Data in States",
    "text": "Example - NCHS Weekly Flu Mortality Data in States"
  },
  {
    "objectID": "slides/day1-morning.html#interactive-tooling",
    "href": "slides/day1-morning.html#interactive-tooling",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Interactive tooling",
    "text": "Interactive tooling\n\nOther main way to find data sources and signals in R… Functions to enhance data discovery in epidatr:\n\navail_endpoints() Function:\n\nLists all endpoints with brief descriptions.\nHighlights specific endpoints that cover non-US locations, facilitating targeted searches.\n\nOutput Format: Returns a tibble for easy viewing and analysis of available data sources.\n\n\n\navail_endpoints()"
  },
  {
    "objectID": "slides/day1-morning.html#using-the-covidcast_epidata-function",
    "href": "slides/day1-morning.html#using-the-covidcast_epidata-function",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using the covidcast_epidata() function",
    "text": "Using the covidcast_epidata() function\n\n\nIn-Depth Data Exploration\n\nFunction Overview: covidcast_epidata() provides detailed insights into data sources from the COVIDcast endpoint.\nSource List: Each data source is listed in covid_sources$sources, with associated tibbles describing included signals.\n\nTab Completion for Ease of Use\n\nEditor Support: In RStudio or similar editors, use tab completion to explore:\n\nData Sources: Type covid_sources$source$ to view available data sources.\nSignals: Type covid_sources$signals$ to see signal options with autocomplete assistance.\n\n\nFiltering Convenience: Signal names are prefixed with their respective data source for easier navigation.\n\n\n\ncovid_sources &lt;- covidcast_epidata()\nhead(covid_sources$sources, n = 2) # head(list, n = 2) will print the first two elements of the list"
  },
  {
    "objectID": "slides/day1-morning.html#fetching-data---covidcast-main-endpoint",
    "href": "slides/day1-morning.html#fetching-data---covidcast-main-endpoint",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetching data - COVIDcast main endpoint",
    "text": "Fetching data - COVIDcast main endpoint\n\n\nFetching data from the Delphi Epidata API is simple.\nThe pub_covidcast() function lets us access the covidcast endpoint.\nWe need to specify the following six arguments…\n\nsource: Data source name\nsignals: Signal name\ngeo_type: Geographic level\ntime_type: Time resolution\ngeo_values: Location(s)\ntime_values: times of interest\n\nLet’s give this a try!"
  },
  {
    "objectID": "slides/day1-morning.html#fetching-data---covidcast-main-endpoint-1",
    "href": "slides/day1-morning.html#fetching-data---covidcast-main-endpoint-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fetching data - COVIDcast main endpoint",
    "text": "Fetching data - COVIDcast main endpoint\n\n\nlibrary(epidatr)\nlibrary(dplyr)\n\n\n# Obtain the most up-to-date version of the smoothed covid-like illness (CLI)\n# signal from the COVID-19 Trends and Impact survey for the US\nepidata &lt;- pub_covidcast(\n  source = \"fb-survey\",\n  signals = \"smoothed_cli\",\n  geo_type = \"nation\",\n  time_type = \"day\",\n  geo_values = \"us\",\n  time_values = epirange(20210105, 20210410)\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, geo_type, time_value, issue, lag, value, stderr)\n\n# A tibble: 6 × 9\n  geo_value signal      source geo_type time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;  &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 us        smoothed_c… fb-su… nation   2021-01-05 2021-01-10     5  1.18 0.0162\n2 us        smoothed_c… fb-su… nation   2021-01-06 2021-01-29    23  1.18 0.0163\n3 us        smoothed_c… fb-su… nation   2021-01-07 2021-01-29    22  1.20 0.0165\n4 us        smoothed_c… fb-su… nation   2021-01-08 2021-01-29    21  1.22 0.0167\n5 us        smoothed_c… fb-su… nation   2021-01-09 2021-01-29    20  1.22 0.0169\n6 us        smoothed_c… fb-su… nation   2021-01-10 2021-01-29    19  1.23 0.0171\n\n\nHere value is the requested signal – in this case, the smoothed estimate of the percentage of people with COVID-like illness, based on the symptom surveys, and stderr is its standard error."
  },
  {
    "objectID": "slides/day1-morning.html#returned-data---covidcast-main-endpoint",
    "href": "slides/day1-morning.html#returned-data---covidcast-main-endpoint",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Returned data - COVIDcast main endpoint",
    "text": "Returned data - COVIDcast main endpoint\n\n\npub_covidcast() outputs a tibble, where each row represents one observation.\nEach observation covers a set of events aggregated by time and by geographic region is a record in our database. Each such record includes:\ntime_value: time period when the events occurred.\ngeo_value: geographic region where the events occurred.\nvalue: estimated value.\nstderr: standard error of the estimate, usually referring to the sampling error.\nsample_size: number of events used in the estimation."
  },
  {
    "objectID": "slides/day1-morning.html#returned-data---covidcast-main-endpoint-1",
    "href": "slides/day1-morning.html#returned-data---covidcast-main-endpoint-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Returned data - COVIDcast main endpoint",
    "text": "Returned data - COVIDcast main endpoint\nCrucially—and unlike most other sources of COVID-19 data—our API reports two additional fields with each record:\n\nissue: The time period when this observation was published.\nlag: The time delay between when the events occurred and when this observation was published.\nMeaning that unlike most other sources of COVID data, it tracks the complete revision history of every signal.\nThis allows for historical reconstructions of what information was available at specific times. More on this soon!"
  },
  {
    "objectID": "slides/day1-morning.html#geographic-levels",
    "href": "slides/day1-morning.html#geographic-levels",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Geographic levels",
    "text": "Geographic levels\n\n\nThe Epidata API makes signals available at different geographic levels, depending on the endpoint\nFor the smoothed_cli signal, we can obtain values for each state\nSimply change geo_type and geo_values in the previous example to get…\n\n\n# Obtain the most up-to-date version of the smoothed covid-like illness (CLI)\n# signal from the COVID-19 Trends and Impact survey for all states\nstate_epidata &lt;- pub_covidcast(\n  source = \"fb-survey\",\n  signals = \"smoothed_cli\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"*\",\n  time_values = epirange(20210105, 20210410)\n)\nhead(state_epidata) %&gt;% select(geo_value, signal, source, geo_type, time_value, issue, lag, value, stderr)\n\n# A tibble: 6 × 9\n  geo_value signal      source geo_type time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;  &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 ak        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 0.747 0.250 \n2 al        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 2.36  0.187 \n3 ar        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 1.93  0.200 \n4 az        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 1.56  0.129 \n5 ca        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 1.24  0.0542\n6 co        smoothed_c… fb-su… state    2021-01-05 2021-01-10     5 0.934 0.107"
  },
  {
    "objectID": "slides/day1-morning.html#covidcast-main-endpoint---example-query",
    "href": "slides/day1-morning.html#covidcast-main-endpoint---example-query",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "COVIDcast main endpoint - Example query",
    "text": "COVIDcast main endpoint - Example query\n\nCounty geo_values are FIPS codes and are discussed in the API docs here. The example below is for Orange County, California.\n\nfb_county_data &lt;- pub_covidcast(\n  source = \"fb-survey\",\n  signals = \"smoothed_accept_covid_vaccine\",\n  geo_type = \"county\",\n  time_type = \"day\",\n  time_values = epirange(20201221, 20201225),\n  geo_values = \"06059\"\n)\nhead(fb_county_data) %&gt;% select(geo_value, signal, source, geo_type, time_value, issue, lag, value, stderr)\n\n# A tibble: 5 × 9\n  geo_value signal      source geo_type time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;  &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 06059     smoothed_a… fb-su… county   2020-12-21 2020-12-22     1  80.9   2.08\n2 06059     smoothed_a… fb-su… county   2020-12-22 2020-12-23     1  78.9   1.76\n3 06059     smoothed_a… fb-su… county   2020-12-23 2020-12-24     1  80.0   1.50\n4 06059     smoothed_a… fb-su… county   2020-12-24 2020-12-25     1  79.3   1.35\n5 06059     smoothed_a… fb-su… county   2020-12-25 2020-12-26     1  80.3   1.21\n\n\n\n\n\n\nNote\n\n\nThe covidcast endpoint supports * in its time and geo fields. Try to obtain the signal values for all available counties by replacing geo_values = \"06059\" with geo_values = \"*\"."
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-hospitalizations",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-hospitalizations",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Hospitalizations",
    "text": "Example queries - Other endpoints  Hospitalizations\nCOVID-19 Hospitalization: Facility Lookup\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/covid_hosp_facility_lookup.html \n\npub_covid_hosp_facility_lookup(city = \"southlake\")\n\n# A tibble: 2 × 10\n  hospital_pk state ccn    hospital_name    address city  zip   hospital_subtype\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           \n1 450888      TX    450888 TEXAS HEALTH HA… 1545 E… SOUT… 76092 Short Term      \n2 670132      TX    670132 METHODIST SOUTH… 421 E … SOUT… 76092 Short Term      \n# ℹ 2 more variables: fips_code &lt;chr&gt;, is_metro_micro &lt;dbl&gt;\n\n# pub_covid_hosp_facility_lookup(state = \"WY\")\n# A non-example (there is no city called New York in Wyoming)\n# pub_covid_hosp_facility_lookup(state = \"WY\", city = \"New York\")"
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-hospitalizations-1",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-hospitalizations-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Hospitalizations",
    "text": "Example queries - Other endpoints  Hospitalizations\nCOVID-19 Hospitalization by Facility\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/covid_hosp_facility.html \n\npub_covid_hosp_facility(\n  hospital_pks = \"100075\",\n  collection_weeks = epirange(20200101, 20200501)\n)\n\nCOVID-19 Hospitalization by State\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/covid_hosp.html \n\npub_covid_hosp_state_timeseries(states = \"MA\", dates = \"20200510\")"
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-flu-endpoints",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-flu-endpoints",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Flu endpoints",
    "text": "Example queries - Other endpoints  Flu endpoints\nFluSurv hospitalization data\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/flusurv.html \n\npub_flusurv(locations = \"ca\", epiweeks = 202001)\n\nFluview data\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/fluview.html \n\npub_fluview(regions = \"nat\", epiweeks = epirange(201201, 202001))\n\nNIDSS Flu\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/nidss_flu.html \n\npub_nidss_flu(regions = \"taipei\", epiweeks = epirange(200901, 201301))"
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-dengue-endpoints",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-dengue-endpoints",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Dengue endpoints",
    "text": "Example queries - Other endpoints  Dengue endpoints\nDelphi’s Dengue Nowcast\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/dengue_nowcast.html \n\npub_dengue_nowcast(locations = \"pr\", epiweeks = epirange(201401, 202301))\n\nNIDSS dengue\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/nidss_dengue.html \n\npub_nidss_dengue(locations = \"taipei\", epiweeks = epirange(200301, 201301))\n\nPAHO Dengue\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/paho_dengue.html \n\npub_paho_dengue(regions = \"ca\", epiweeks = epirange(200201, 202319))"
  },
  {
    "objectID": "slides/day1-morning.html#example-queries---other-endpoints-wikipedia",
    "href": "slides/day1-morning.html#example-queries---other-endpoints-wikipedia",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example queries - Other endpoints  Wikipedia",
    "text": "Example queries - Other endpoints  Wikipedia\nWikipedia access\n API docs: https://cmu-delphi.github.io/delphi-epidata/api/wiki.html \n\npub_wiki(\n  language = \"en\",\n  articles = \"influenza\",\n  time_type = \"week\",\n  time_values = epirange(202001, 202319)\n)\n\n# A tibble: 64 × 6\n   article   count     total  hour epiweek     value\n   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 influenza  6516 663604044    -1 2019-12-29   9.82\n 2 influenza 10244 789885521    -1 2020-01-05  13.0 \n 3 influenza 10728 783760384    -1 2020-01-12  13.7 \n 4 influenza 24843 785222292    -1 2020-01-19  31.6 \n 5 influenza 62850 780291898    -1 2020-01-26  80.5 \n 6 influenza 41768 778222703    -1 2020-02-02  53.7 \n 7 influenza 29434 767244708    -1 2020-02-09  38.4 \n 8 influenza 22714 764074572    -1 2020-02-16  29.7 \n 9 influenza 88758 767718009    -1 2020-02-23 116.  \n10 influenza 62433 759825311    -1 2020-03-01  82.2 \n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nTip - public vs private methods\n\n\nAside from these public methods we’ve gone through (these start with pub_), there are private methods (these start with pvt_ when you type avail_endpoints()). These require private access keys to use (separate from the Delphi Epidata API key). To run these locally, you will need to store these secrets in your .Reviron file, or set them as environmental variables. See Private methods for examples of using private endpoints."
  },
  {
    "objectID": "slides/day1-morning.html#signal-metadata",
    "href": "slides/day1-morning.html#signal-metadata",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Signal metadata",
    "text": "Signal metadata\n\nSome endpoints provide additional metadata for signals.\n\nTime Information: Details on available time frames and last update times.\nGeography Information: Details on available geography types.\n\nKey Endpoints for Metadata\n\npub_covidcast_meta(): Access metadata for the COVIDcast endpoint.\npub_fluview_meta(): Get metadata for the FluView endpoint.\npub_meta(): General metadata for the Delphi Epidata API."
  },
  {
    "objectID": "slides/day1-morning.html#panel-data-1",
    "href": "slides/day1-morning.html#panel-data-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Panel data",
    "text": "Panel data\n\n\nPanel data or longitudinal data, contain cross-sectional measurements of subjects over time.\nIn table form, panel data is a time index + one or more locations/keys.\nFor example: The estimated percentage of outpatient doctor visits that are COVID-related in WA from Dec. 2021 to Feb. 2022 (docs):\n\n\n\nAn `epi_df` object, 6 x 3 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-02-01\n\n# A tibble: 6 × 3\n  geo_value time_value percent_cli\n* &lt;chr&gt;     &lt;date&gt;           &lt;dbl&gt;\n1 wa        2021-12-01        4.70\n2 wa        2021-12-02        4.60\n3 wa        2021-12-03        4.56\n4 wa        2021-12-04        4.93\n5 wa        2021-12-05        4.17\n6 wa        2021-12-06        4.12"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---covid-19-cases",
    "href": "slides/day1-morning.html#examples-of-panel-data---covid-19-cases",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of panel data - COVID-19 cases",
    "text": "Examples of panel data - COVID-19 cases\nJHU CSSE COVID cases per 100k  estimates the daily number of new confirmed COVID-19 cases per 100,000 population, averaged over the past 7 days.\nAPI docs: https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/jhu-csse.html"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---chng-cli",
    "href": "slides/day1-morning.html#examples-of-panel-data---chng-cli",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of panel data - CHNG-CLI",
    "text": "Examples of panel data - CHNG-CLI\nChange Healthcare COVID-like illness (CHNG-CLI) reports the percentage of outpatient visits for COVID-related symptoms, based on deidentified Change Healthcare claims data.\nAPI docs: https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/chng.html"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---chng-covid",
    "href": "slides/day1-morning.html#examples-of-panel-data---chng-covid",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of panel data - CHNG-COVID",
    "text": "Examples of panel data - CHNG-COVID\nChange Healthcare COVID (CHNG-COVID) reports the percentage of outpatient visits with confirmed COVID-19, based on Change Healthcare claims data.\nAPI docs: https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/chng.html"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---hhs-admissions",
    "href": "slides/day1-morning.html#examples-of-panel-data---hhs-admissions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of panel data - HHS Admissions",
    "text": "Examples of panel data - HHS Admissions\nConfirmed COVID-19 Hospital Admissions per 100k estimates the daily sum of adult and pediatric confirmed COVID-19 hospital admissions, per 100,000 population, averaged over the past 7 days.\nAPI docs: https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/hhs.html"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---covid-19-cases-and-deaths-in-ca",
    "href": "slides/day1-morning.html#examples-of-panel-data---covid-19-cases-and-deaths-in-ca",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of panel data - COVID-19 cases and deaths in CA",
    "text": "Examples of panel data - COVID-19 cases and deaths in CA\n\nTakeaway: Cases appear to strongly correlate with deaths several weeks later.\nWe’ll see this again in an upcoming session…"
  },
  {
    "objectID": "slides/day1-morning.html#intro-to-versioned-data",
    "href": "slides/day1-morning.html#intro-to-versioned-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Intro to versioned data",
    "text": "Intro to versioned data\n\nIn panel data, we’ve seen that time is indicated by time_value.\nNow, we add a second time index to indicate the data version…\n\n\n\nKey: &lt;geo_value, time_value, version&gt;\n   geo_value time_value    version percent_cli\n      &lt;char&gt;     &lt;Date&gt;     &lt;Date&gt;       &lt;num&gt;\n1:        wa 2021-12-01 2021-12-05    0.884362\n2:        wa 2021-12-01 2021-12-06    0.917057\n3:        wa 2021-12-01 2021-12-07    0.896221\n4:        wa 2021-12-01 2021-12-08    0.984512\n5:        wa 2021-12-01 2021-12-09    1.027853\n6:        wa 2021-12-01 2021-12-10    0.999755\n\n\n\nNote that this feature can be indicated in different ways (ex. version, issue, release, as_of)."
  },
  {
    "objectID": "slides/day1-morning.html#versioned-panel-data",
    "href": "slides/day1-morning.html#versioned-panel-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Versioned panel data",
    "text": "Versioned panel data\nEstimated percentage of outpatient (DV-CLI) data across multiple issue dates, with updates and revisions to past data as new issue dates are released:\n\n\nFigure 5 from Reinhart et al. (2021)"
  },
  {
    "objectID": "slides/day1-morning.html#latency-and-revision-in-signals",
    "href": "slides/day1-morning.html#latency-and-revision-in-signals",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Latency and revision in signals",
    "text": "Latency and revision in signals\n\nLatency refers to the delay between data collection and availability.\n\nExample: A signal based on medical insurance claims may take several days to appear but is subject to delays as claims are processed over weeks.\n\nRevision occurs when data is updated or corrected after initial publication, often due to new information or late reporting.\n\nExample: COVID-19 case reports are revised frequently after initial publication as new data comes in or reporting backlogs are cleared."
  },
  {
    "objectID": "slides/day1-morning.html#latency-and-revision-in-signals---example",
    "href": "slides/day1-morning.html#latency-and-revision-in-signals---example",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Latency and revision in signals - Example",
    "text": "Latency and revision in signals - Example\n\n\nRecall the first example of panel & versioned data we’ve seen…\nThis signal is 4 days latent (min(version - time_value))\n\n\n\n# A tibble: 6 × 5\n# Groups:   time_value [6]\n  geo_value time_value version    percent_cli version_time_diff\n  &lt;chr&gt;     &lt;date&gt;     &lt;date&gt;           &lt;dbl&gt; &lt;drtn&gt;           \n1 wa        2021-12-01 2021-12-05       0.884 4 days           \n2 wa        2021-12-02 2021-12-06       0.737 4 days           \n3 wa        2021-12-03 2021-12-07       0.662 4 days           \n4 wa        2021-12-04 2021-12-08       0.663 4 days           \n5 wa        2021-12-05 2021-12-09       0.872 4 days           \n6 wa        2021-12-06 2021-12-10       0.642 4 days           \n\n\n\nAnd clearly undergoes revision over time (ex. consider Dec. 1’s percent_cli across version):\n\n\n\nKey: &lt;geo_value, time_value, version&gt;\n   geo_value time_value    version percent_cli version_time_diff\n      &lt;char&gt;     &lt;Date&gt;     &lt;Date&gt;       &lt;num&gt;        &lt;difftime&gt;\n1:        wa 2021-12-01 2021-12-05    0.884362            4 days\n2:        wa 2021-12-01 2021-12-06    0.917057            5 days\n3:        wa 2021-12-01 2021-12-07    0.896221            6 days\n4:        wa 2021-12-01 2021-12-08    0.984512            7 days\n5:        wa 2021-12-01 2021-12-09    1.027853            8 days\n6:        wa 2021-12-01 2021-12-10    0.999755            9 days"
  },
  {
    "objectID": "slides/day1-morning.html#revision-triangle-outpatient-visits-in-wa-2022",
    "href": "slides/day1-morning.html#revision-triangle-outpatient-visits-in-wa-2022",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Revision triangle, Outpatient visits in WA 2022",
    "text": "Revision triangle, Outpatient visits in WA 2022"
  },
  {
    "objectID": "slides/day1-morning.html#revisions",
    "href": "slides/day1-morning.html#revisions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Revisions",
    "text": "Revisions\nMany data sources are subject to revisions:\n\nCase and death counts are frequently corrected or adjusted by authorities.\nMedical claims data can take weeks to be submitted and processed.\n\n\n\nLab tests and medical records can be backlogged for a variety of reasons.\nSurveys are not always completed promptly.\nKey: An accurate revision log is crucial for researchers building forecasts.\n\nA forecast that is made today can should rely on information we have access to today."
  },
  {
    "objectID": "slides/day1-morning.html#three-types-of-revisions",
    "href": "slides/day1-morning.html#three-types-of-revisions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Three types of revisions",
    "text": "Three types of revisions\n\nSources that don’t revise - Ex. Facebook or Google symptoms (provisional and final are the same)\nPredictable revisions - Ex. Claims data (CHNG) and public health reports aligned by observation/test, hosp, or death date\nRevisions that are large and erratic to predict - Ex. COVID cases and deaths"
  },
  {
    "objectID": "slides/day1-morning.html#types-of-revisions---comparison-between-2.-and-3.",
    "href": "slides/day1-morning.html#types-of-revisions---comparison-between-2.-and-3.",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Types of revisions - Comparison between 2. and 3.",
    "text": "Types of revisions - Comparison between 2. and 3.\n\n\nRevision behavior for two indicators in the HRR containing Charlotte, NC.\n\n\n\nDV-CLI signal (left)  was regularly revised throughout the period, although effects fade farther back.\nJHU CSSE cases (right)  remain “as reported” on Sept. 28, with a spike toward the end of this period, until a major correction is made on Oct. 19, which brings this down & affects prior data.\n\n\n\n\nFigure 1 from McDonald et al. (2021)"
  },
  {
    "objectID": "slides/day1-morning.html#key-takeaways",
    "href": "slides/day1-morning.html#key-takeaways",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nMedical claims revisions: More systematic and predictable.\nCOVID-19 case report revisions: Erratic and often unpredictable.\nLarge spikes or anomalies can occur as:\n\nReporting backlogs are cleared.\nChanges in case definitions are implemented."
  },
  {
    "objectID": "slides/day1-morning.html#reporting-backlogs---example",
    "href": "slides/day1-morning.html#reporting-backlogs---example",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Reporting backlogs - Example",
    "text": "Reporting backlogs - Example\n\n\nLeft: Reported cases per day in Bexar County, Texas, during the summer of 2020. On July 16, 4,810 backlogged cases were reported, reflecting a 2-week delay. This caused a prolonged spike due to the 7-day trailing average applied to the counts.\nRight: CTIS estimates of CLI-in-community showed more stable underlying trends.\n\n\n\n\nFigure 4 from Reinhart et al. (2021)"
  },
  {
    "objectID": "slides/day1-morning.html#reporting-backlogs---key-takeaways",
    "href": "slides/day1-morning.html#reporting-backlogs---key-takeaways",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Reporting backlogs - Key takeaways",
    "text": "Reporting backlogs - Key takeaways\n\nReporting issues been common across U.S. jurisdictions.\nFor example, audits have regularly discovered misclassified or unreported cases and deaths.\nThis underscores the value of cross-checking data with external sources not part of the same reporting systems."
  },
  {
    "objectID": "slides/day1-morning.html#versioned-data-in-epidatr",
    "href": "slides/day1-morning.html#versioned-data-in-epidatr",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Versioned data in epidatr",
    "text": "Versioned data in epidatr\n\n\nEpidata API contains comprehensive data record, capturing each signal’s estimate, location, date, and update timeline.\nExample: Doctor Visits Signal (from the covidcast endpoint)\n\nEstimates the percentage of outpatient doctor visits that are COVID-related. To give a specific example, let’s consider the estimate for PA on May 1, 2020:\n\n\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-01\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  as_of = \"2020-05-07\"\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value)\n\n# A tibble: 1 × 7\n  geo_value signal           source        time_value issue        lag value\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-visits 2020-05-01 2020-05-07     6  2.58\n\n\n\nInitial estimate was issued on May 7, 2020 (due to delay from aggregation and ingestion by the API)."
  },
  {
    "objectID": "slides/day1-morning.html#understanding-data-as-of-a-specific-date",
    "href": "slides/day1-morning.html#understanding-data-as-of-a-specific-date",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Understanding data “as of” a specific date",
    "text": "Understanding data “as of” a specific date\n\n\nRequesting Specific Data Versions:\n\nUse as_of, issues, or lag arguments to specify data availability.\nOnly one argument can be used at a time; not all endpoints support all three.\n\nWe’ve already used the as_of argument, so let’s try lag\n\nExample for May 7, 2020 (we should get the same output as before):\n\n\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-01\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  lag = 6\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 1 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-07     6  2.58     NA\n\n\nResult: Estimate of &lt;3% issued on May 7, 2020."
  },
  {
    "objectID": "slides/day1-morning.html#understanding-data-as-of-a-specific-date-1",
    "href": "slides/day1-morning.html#understanding-data-as-of-a-specific-date-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Understanding data “as of” a specific date",
    "text": "Understanding data “as of” a specific date\n\nDefault behaviour: If we don’t specify as_of, we get the most recent estimate:\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-01\"),\n  geo_type = \"state\",\n  geo_values = \"pa\"\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 1 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-07-04    64  5.97     NA\n\n\n\nSubstantial Estimate Change:\n\nEstimate increased from &lt;3% to almost 6% after May 7, reflecting new data on visits from May 1.\n\nCritical for Forecasting:\n\nAccurate backtesting requires using data available at the time of model fitting, not later updates, to ensure valid forecasting results."
  },
  {
    "objectID": "slides/day1-morning.html#multiple-issues-of-observations",
    "href": "slides/day1-morning.html#multiple-issues-of-observations",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Multiple issues of observations",
    "text": "Multiple issues of observations\nBy using the issues argument, we can request all issues in a certain time period:\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-01\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  issues = epirange(\"2020-05-01\", \"2020-05-15\")\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 6 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-07     6  2.58     NA\n2 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-08     7  3.28     NA\n3 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-09     8  3.32     NA\n4 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-12    11  3.59     NA\n5 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-13    12  3.63     NA\n6 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-14    13  3.66     NA"
  },
  {
    "objectID": "slides/day1-morning.html#observations-issued-with-a-specific-lag",
    "href": "slides/day1-morning.html#observations-issued-with-a-specific-lag",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Observations issued with a specific lag",
    "text": "Observations issued with a specific lag\n\n\nWe can use the lag argument to request only data reported with a certain lag.\nExample: Request a lag of 7 days fetches only data issued exactly 7 days after the corresponding time_value:\n\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-01\", \"2020-05-07\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  lag = 7\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 5 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-01 2020-05-08     7  3.28     NA\n2 pa        smoothed_adj_cli doctor-vi… 2020-05-02 2020-05-09     7  3.23     NA\n3 pa        smoothed_adj_cli doctor-vi… 2020-05-05 2020-05-12     7  2.78     NA\n4 pa        smoothed_adj_cli doctor-vi… 2020-05-06 2020-05-13     7  2.56     NA\n5 pa        smoothed_adj_cli doctor-vi… 2020-05-07 2020-05-14     7  2.19     NA"
  },
  {
    "objectID": "slides/day1-morning.html#query-results-exclusion",
    "href": "slides/day1-morning.html#query-results-exclusion",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Query results exclusion",
    "text": "Query results exclusion\n\n\nAlthough the query we ran on the previous slide requested values from May 1 to May 7, May 3 and May 4 were not included due to a 7-day lag.\nResults for those dates appear only if updates are issued on the corresponding lag day (e.g., May 10).\n\n\nepidata &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = epirange(\"2020-05-03\", \"2020-05-03\"),\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  issues = epirange(\"2020-05-09\", \"2020-05-15\")\n)\nhead(epidata) %&gt;% select(geo_value, signal, source, time_value, issue, lag, value, stderr)\n\n# A tibble: 5 × 8\n  geo_value signal           source     time_value issue        lag value stderr\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-09     6  2.79     NA\n2 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-12     9  3.02     NA\n3 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-13    10  3.04     NA\n4 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-14    11  3.02     NA\n5 pa        smoothed_adj_cli doctor-vi… 2020-05-03 2020-05-15    12  3.05     NA"
  },
  {
    "objectID": "slides/day1-morning.html#main-takeaways",
    "href": "slides/day1-morning.html#main-takeaways",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Main takeaways",
    "text": "Main takeaways\n\n\nDelphi Epidata: A one-stop platform for real-time epidemic data, providing aggregated signals for disease tracking and forecasting from diverse sources like health records, mobility patterns, and more.\nEpidata API: Open-access API delivering up-to-date, granular epidemiological data + makes all historical versions available.\nEpidatr: Enables you to access Delphi’s epidemiological data through R and Python, offering easy installation, powerful API functions, and interactive tools for discovering and analyzing health signals.\nVersioned Data and Latency: Panel data captures time-series trends, which are often subject to revision. A standout feature of this API is its inclusion of two critical fields…\n\nissue: When the data was published\nlag: The delay between the event and when it was published\n\nto manage latency and revisions for transparency and more accurate analysis."
  },
  {
    "objectID": "slides/day1-morning.html#final-slide",
    "href": "slides/day1-morning.html#final-slide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Final slide",
    "text": "Final slide\nThanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nDay 1 Morning — cmu-delphi/insightnet-workshop-2024"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Schedule",
    "text": "Schedule\nShort description\n\nDay 1 Morning\nDay 1 Afternoon\nDay 2 Morning\nDay 2 Afternoon"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Instructors",
    "text": "Instructors\n\nRyan J. Tibshirani\nDaniel J. McDonald\nAlice Cima\nRachel Lobay"
  },
  {
    "objectID": "slides/day2-afternoon.html#section",
    "href": "slides/day2-afternoon.html#section",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Forecasting with {epipredict}",
    "text": "Forecasting with {epipredict}\n\n\n\n\nVenue – dd Somemonth yyyy"
  },
  {
    "objectID": "slides/day2-afternoon.html#outline",
    "href": "slides/day2-afternoon.html#outline",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Outline",
    "text": "Outline\n\n{epipredict}\nFit and Predict with arx_forecaster\nCustomizing arx_forecaster\nForecasting with Versioned Data\nBuilding a Forecaster"
  },
  {
    "objectID": "slides/day2-afternoon.html#epipredict-1",
    "href": "slides/day2-afternoon.html#epipredict-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "{epipredict}",
    "text": "{epipredict}\nhttps://cmu-delphi.github.io/epipredict\nInstallation\n\n# Stable version\npak::pkg_install(\"cmu-delphi/epipredict@main\")\n# Development version\n# pak::pkg_install(\"cmu-delphi/epipredict@dev\")"
  },
  {
    "objectID": "slides/day2-afternoon.html#what-epipredict-provides-i",
    "href": "slides/day2-afternoon.html#what-epipredict-provides-i",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What {epipredict} provides (i)",
    "text": "What {epipredict} provides (i)\nBasic and easy to use “canned” forecasters:\n\nBaseline flat forecaster\nAutoregressive forecaster (ARX)\nAutoregressive classifier\nCDC FluSight flatline forecaster"
  },
  {
    "objectID": "slides/day2-afternoon.html#what-epipredict-provides-ii",
    "href": "slides/day2-afternoon.html#what-epipredict-provides-ii",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "What {epipredict} provides (ii)",
    "text": "What {epipredict} provides (ii)\n\nA framework for creating custom forecasters out of modular components.\nThere are four types of components:\n\nPreprocessor: do things to the data before model training\nTrainer: train a model on data, resulting in a fitted model object\nPredictor: make predictions, using a fitted model object\nPostprocessor: do things to the predictions before returning"
  },
  {
    "objectID": "slides/day2-afternoon.html#fit-arx-on-training-set",
    "href": "slides/day2-afternoon.html#fit-arx-on-training-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fit ARX on training set",
    "text": "Fit ARX on training set\n\nBack to the ARX model for COVID deaths: \\(\\quad \\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}\\)\nUsing {epipredict}\n\n\nlibrary(epipredict)\n\n# split into train and test \nt0_date &lt;- as.Date('2021-04-01')\ntrain &lt;- ca |&gt; filter(time_value &lt;= t0_date)\ntest &lt;- ca |&gt; filter(time_value &gt; t0_date)\n\n# fit ARX\nepi_arx &lt;- arx_forecaster(epi_data = train |&gt; as_epi_df(), \n                          outcome = \"deaths\", \n                          predictors = c(\"cases\", \"deaths\"),\n                          trainer = linear_reg() |&gt; set_engine(\"lm\"),\n                          args_list = arx_args_list(lags = 0, ahead = 28,\n                                                    quantile_levels = c(0.025, 0.975)))"
  },
  {
    "objectID": "slides/day2-afternoon.html#arx_forecaster-output",
    "href": "slides/day2-afternoon.html#arx_forecaster-output",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "arx_forecaster output",
    "text": "arx_forecaster output\n\nA fitted model object which can be used any time in the future to create forecasts ($epi_workflows).\nA forecast (point prediction + interval) for 28 days after the last available time value in the data ($predictions)."
  },
  {
    "objectID": "slides/day2-afternoon.html#arx_forecaster-output-1",
    "href": "slides/day2-afternoon.html#arx_forecaster-output-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "arx_forecaster output",
    "text": "arx_forecaster output\n\nepi_arx \n\n══ A basic forecaster of type ARX Forecaster ═══════════════════════════════════\n\n\n\n\n\nThis forecaster was fit on 2024-11-12 08:50:39.\n\n\n\n\n\nTraining data was an &lt;epi_df&gt; with:\n\n\n• Geography: state,\n\n\n• Other keys: ,\n\n\n• Time type: day,\n\n\n• Using data up-to-date as of: 2024-11-06 08:50:44.\n\n\n• With the last data available on 2021-04-01\n\n\n\n\n\n── Predictions ─────────────────────────────────────────────────────────────────\n\n\n\n\n\nA total of 1 prediction is available for\n\n\n• 1 unique geographic region,\n\n\n• At forecast date: 2021-04-01,\n\n\n• For target date: 2021-04-29,"
  },
  {
    "objectID": "slides/day2-afternoon.html#extract-fitted-object",
    "href": "slides/day2-afternoon.html#extract-fitted-object",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Extract fitted object",
    "text": "Extract fitted object\n\n\nepi_arx$epi_workflow\n\n\n\n\n══ Epi Workflow [trained] ══════════════════════════════════════════════════════\n\n\nPreprocessor: Recipe\n\n\nModel: linear_reg()\n\n\nPostprocessor: Frosting\n\n\n\n\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n\n\n\n\n\n6 Recipe steps.\n\n\n1. step_epi_lag()\n\n\n2. step_epi_lag()\n\n\n3. step_epi_ahead()\n\n\n4. step_naomit()\n\n\n5. step_naomit()\n\n\n6. step_training_window()\n\n\n\n\n\n── Model ───────────────────────────────────────────────────────────────────────\n\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n (Intercept)   lag_0_cases  lag_0_deaths  \n    0.076884      0.009858      0.200700  \n\n\n\n\n\n── Postprocessor ───────────────────────────────────────────────────────────────\n\n\n\n\n\n5 Frosting layers.\n\n\n1. layer_predict()\n\n\n2. layer_residual_quantiles()\n\n\n3. layer_add_forecast_date()\n\n\n4. layer_add_target_date()\n\n\n5. layer_threshold()\n\n\n\n\n\n\nExtract predictions\n\nepi_arx$predictions\n\n# A tibble: 1 × 5\n  geo_value .pred        .pred_distn forecast_date target_date\n  &lt;chr&gt;     &lt;dbl&gt;             &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n1 ca        0.219 quantiles(0.22)[2] 2021-04-01    2021-04-29 \n\n\n\n\n\nNote\n\n\n.pred_dstn is actually a “distribution”, parameterized by its quantiles.\n\n\n\n\n\nExtract predictions\nWe can extract the distribution into a “long” epi_df:\n\none row per quantile\nvalues = value associated to that quantile\n\n\nepi_arx$predictions |&gt;\n  mutate(.pred_distn = nested_quantiles(.pred_distn)) |&gt;  # create a \"nested\" list-column\n  unnest(.pred_distn)                                     # then unnest it\n\n# A tibble: 2 × 6\n  geo_value .pred values quantile_levels forecast_date target_date\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt; &lt;date&gt;        &lt;date&gt;     \n1 ca        0.219 0.0133           0.025 2021-04-01    2021-04-29 \n2 ca        0.219 0.425            0.975 2021-04-01    2021-04-29 \n\n\n\n\nPredict with fitted ARX (split-sample)\n\narx_forecaster fits a model to the training set, and outputs only one prediction (for time \\(t_0 +h\\)).\nTo get predictions for the test set:\n\n\npredict(epi_arx$epi_workflow, test)\n\nAn `epi_df` object, 707 x 6 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-11-06 08:50:44.00687\n\n# A tibble: 707 × 6\n   geo_value time_value .pred        .pred_distn forecast_date target_date\n * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;             &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n 1 ca        2021-04-02 0.217 quantiles(0.22)[2] 2021-04-01    2021-04-29 \n 2 ca        2021-04-03 0.203  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 3 ca        2021-04-04 0.197  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 4 ca        2021-04-05 0.202  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 5 ca        2021-04-06 0.199  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 6 ca        2021-04-07 0.196  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 7 ca        2021-04-08 0.193 quantiles(0.19)[2] 2021-04-01    2021-04-29 \n 8 ca        2021-04-09 0.195  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 9 ca        2021-04-10 0.209 quantiles(0.21)[2] 2021-04-01    2021-04-29 \n10 ca        2021-04-11 0.214 quantiles(0.21)[2] 2021-04-01    2021-04-29 \n# ℹ 697 more rows\n\n\n\n\nPredict with ARX (when re-fitting)\nIn practice, if we want to re-train the forecasters as new data arrive, we fit and predict combining arx_forecaster with epi_slide.\n\n\nPredict with ARX (re-fitting on trailing window)\n\nh &lt;- 28         #horizon\nw &lt;- 120 + h    #trailing window length\nn &lt;- nrow(ca)   #time-series length\n\n# Specify the forecast dates\nfc_time_values &lt;- seq(from = t0_date, to = ca$time_value[n]-h, by = \"1 day\")\n\n# Slide the arx_forecaster \nepi_pred_trailing &lt;- ca |&gt;\n  epi_slide(\n    ~ arx_forecaster(epi_data = .x,\n                     outcome = \"deaths\", \n                     predictors = c(\"cases\", \"deaths\"), \n                     trainer = linear_reg() |&gt; set_engine(\"lm\"),\n                     args_list = arx_args_list(lags = 0, ahead = h,\n                                               quantile_levels = c(0.025, 0.975))\n                     )$predictions |&gt;\n        pivot_quantiles_wider(.pred_distn),\n  .window_size = w, \n  .ref_time_values = fc_time_values\n)\n\n\n\nPredict with ARX\n\n\n\nNote (window length)\n\n\nWe set \\(w = 120 + h\\) to match the window size of the ARX model we fitted manually. Previously, when considering a window from \\(t-w\\) to \\(t\\), we had access to all outcomes in that window, and to all predictors between \\(t-w-h\\) and \\(t-h\\). (That’s because we lagged \\(x\\) before applying the window.) So we were “cheating” by saying that the trailing window had length \\(w=120\\), as its actual size was \\(120+h\\)!\n\n\n\n\n\n\nNote (all past)\n\n\nThe method fitting on all past data up to the forecasting date can be implemented by setting:\n.window_size = Inf in epi_slide.\n\n\n\n\n\nPredict with ARX (re-fitting on trailing window)\n\n\nepi_pred_trailing \n\nAn `epi_df` object, 680 x 9 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-11-06 08:50:44.00687\n\n# A tibble: 680 × 9\n# Groups:   geo_value [1]\n   geo_value time_value cases deaths .pred forecast_date target_date `0.025`\n * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        &lt;date&gt;        &lt;dbl&gt;\n 1 ca        2021-04-01  6.77  0.375 0.337 2021-04-01    2021-04-29   0.0996\n 2 ca        2021-04-02  7.27  0.340 0.340 2021-04-02    2021-04-30   0.104 \n 3 ca        2021-04-03  6.79  0.293 0.331 2021-04-03    2021-05-01   0.0960\n 4 ca        2021-04-04  6.51  0.281 0.327 2021-04-04    2021-05-02   0.0919\n 5 ca        2021-04-05  6.87  0.284 0.330 2021-04-05    2021-05-03   0.0934\n 6 ca        2021-04-06  6.97  0.267 0.329 2021-04-06    2021-05-04   0.0900\n 7 ca        2021-04-07  6.92  0.253 0.326 2021-04-07    2021-05-05   0.0850\n 8 ca        2021-04-08  6.97  0.234 0.322 2021-04-08    2021-05-06   0.0795\n 9 ca        2021-04-09  7.07  0.243 0.322 2021-04-09    2021-05-07   0.0784\n10 ca        2021-04-10  8.33  0.249 0.332 2021-04-10    2021-05-08   0.0878\n# ℹ 670 more rows\n# ℹ 1 more variable: `0.975` &lt;dbl&gt;\n\n\n\nPredict with ARX (re-fitting on trailing window)\n\n\n\n                                 MAE     MASE\ntime series CV + trailing 0.07852942 731.6178\n\n\n\n\nCustomizing arx_forecaster\n\n\n\nSimple adjustments\n\narx_forecaster(epi_data = train |&gt; as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |&gt; set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.025, 0.975)))\n\n\n\nModify predictors to add/drop predictors\n\ne.g. drop deaths for regression with a lagged predictor, or drop cases to get AR model\ndefault: predictors = outcome\n\n\n\n\n\nSimple adjustments\n\narx_forecaster(epi_data = train |&gt; as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |&gt; set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.025, 0.975)))\n\n\nModify trainer to use a model that is not lm (default)\n\n e.g. trainer = quantile_reg()\ncan use any {parsnip} models, see list\n\n\n\n\nSimple adjustments\n\narx_forecaster(epi_data = train |&gt; as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |&gt; set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.025, 0.975)))\n\n\nModify arx_args_list to change lags, horizon, quantile levels, …\n\n\n\narx_args_list(\n  lags = c(0L, 7L, 14L),\n  ahead = 7L,\n  n_training = Inf,\n  forecast_date = NULL,\n  target_date = NULL,\n  adjust_latency = c(\"none\", \"extend_ahead\", \"extend_lags\", \"locf\"),\n  warn_latency = TRUE,\n  quantile_levels = c(0.05, 0.95),\n  symmetrize = TRUE,\n  nonneg = TRUE,\n  quantile_by_key = character(0L),\n  check_enough_data_n = NULL,\n  check_enough_data_epi_keys = NULL,\n  ...\n)\n\n\n\n\n\nForecasting with Versioned Data\n\n\n\nVersioned data\n\nus_data\n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-04-01 / 2023-01-31\nℹ First/last version with update: 2021-04-01 / 2023-02-01\nℹ Versions end: 2023-02-01\nℹ A preview of the table (92197 rows x 5 columns):\n          version time_value geo_value     cases     deaths\n    1: 2021-04-01 2020-04-01        ak        NA         NA\n    2: 2021-04-01 2020-04-02        ak        NA         NA\n    3: 2021-04-01 2020-04-03        ak        NA         NA\n    4: 2021-04-01 2020-04-04        ak        NA         NA\n    5: 2021-04-01 2020-04-05        ak        NA         NA\n   ---                                                     \n92193: 2023-02-01 2023-01-27        wy   4.61203 0.17172453\n92194: 2023-02-01 2023-01-28        wy   4.61203 0.17172453\n92195: 2023-02-01 2023-01-29        wy   4.61203 0.17172453\n92196: 2023-02-01 2023-01-30        wy   4.61203 0.17172453\n92197: 2023-02-01 2023-01-31        wy -14.74378 0.04906416\n\n\n\n\nVersion-aware forecasting with geo-pooling\n\nforecast_dates &lt;- seq(from = t0_date, to = as.Date(\"2023-02-01\"), by = \"1 month\")\nh &lt;- c(7, 14, 21, 28)\n\nforecast_k_days_ahead &lt;- function(epi_archive, forecast_dates, ahead = 7) {\n  epi_archive |&gt;\n    epix_slide(\n      ~ arx_forecaster(\n        .x, \n        outcome = \"deaths\", \n        predictors = c(\"cases\", \"deaths\"),\n        trainer = linear_reg() |&gt; set_engine(\"lm\"),\n        args_list = arx_args_list(lags = 0, ahead = ahead,\n                                  quantile_levels = c(0.025, 0.975))\n      )$predictions |&gt; pivot_quantiles_wider(.pred_distn),\n      .before = 120,\n      .versions = forecast_dates\n    )\n}\n\nforecasts &lt;- bind_rows(map(h, ~ forecast_k_days_ahead(us_data, forecast_dates, ahead = .x)))\n\n\n\nVersion-aware forecasting with geo-pooling\n\n\n\n\nBuilding a forecaster\n\n\n\nPhilosophy of forecasting\n\nWe should build up modular components\nBe able to add/remove layers of complexity sequentially\n\n\n\nPreprocessor: do things to the data before model training\nTrainer: train a model on data, resulting in a fitted model object\nPredictor: make predictions, using a fitted model object\nPostprocessor: do things to the predictions before returning\n\n\n\n\nExamples of preprocessing\n\n\nEDA type stuff\n\nMaking locations/signals commensurate (scaling)\nDealing with revisions\nDetecting and removing outliers\nImputing or removing missing data\n\n\n\n\nFeature engineering\n\nCreating lagged predictors\nDay of Week effects\nRolling averages for smoothing\nLagged differences\nGrowth rates instead of raw signals\nThe sky’s the limit\n\n\n\n\nExamples of postprocessing\n\nImpute missing features\nNowcast current values of features\nBootstrap to get predictive intervals\nInvert scaling or other transforms\nThreshold predictions, [0, max_population]\n\n\n\nFit a forecaster\n\n# A preprocessing \"recipe\" that turns raw data into features / response\nr &lt;- epi_recipe(ca) |&gt;\n  step_epi_lag(cases, lag = c(0, 7, 14)) |&gt;\n  step_epi_lag(deaths, lag = c(0, 7, 14)) |&gt;\n  step_epi_ahead(deaths, ahead = 28) |&gt;\n  step_epi_naomit()\n\n# Training engine\ne &lt;- quantile_reg(quantile_levels = c(.025, .5, .975))\n\n# A postprocessing routine describing what to do to the predictions\nf &lt;- frosting() |&gt;\n  layer_predict() |&gt;\n  layer_threshold(.pred, lower = 0) |&gt; # predictions / intervals should be non-negative\n  layer_add_target_date() |&gt;\n  layer_add_forecast_date()\n\n# Bundle up the preprocessor, training engine, and postprocessor\n# We use quantile regression\newf &lt;- epi_workflow(r, e, f)\n\n# Fit it to data (we could fit this to ANY data that has the same format)\ntrained_ewf &lt;- ewf |&gt; fit(ca)\n\n# examines the recipe to determine what we need to make the prediction\nlatest &lt;- get_test_data(r, ca)\n\n# we could make predictions using the same model on ANY test data\npreds &lt;- trained_ewf |&gt; predict(new_data = latest)\n\n\n\nThanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nForecasting with {epipredict} — cmu-delphi/insightnet-workshop-2024"
  },
  {
    "objectID": "slides/day2-afternoon.html#extract-predictions",
    "href": "slides/day2-afternoon.html#extract-predictions",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Extract predictions",
    "text": "Extract predictions\n\nepi_arx$predictions\n\n# A tibble: 1 × 5\n  geo_value .pred        .pred_distn forecast_date target_date\n  &lt;chr&gt;     &lt;dbl&gt;             &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n1 ca        0.219 quantiles(0.22)[2] 2021-04-01    2021-04-29 \n\n\n\n\n\nNote\n\n\n.pred_dstn is actually a “distribution”, parameterized by its quantiles."
  },
  {
    "objectID": "slides/day2-afternoon.html#extract-predictions-1",
    "href": "slides/day2-afternoon.html#extract-predictions-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Extract predictions",
    "text": "Extract predictions\nWe can extract the distribution into a “long” epi_df:\n\none row per quantile\nvalues = value associated to that quantile\n\n\nepi_arx$predictions |&gt;\n  mutate(.pred_distn = nested_quantiles(.pred_distn)) |&gt;  # create a \"nested\" list-column\n  unnest(.pred_distn)                                     # then unnest it\n\n# A tibble: 2 × 6\n  geo_value .pred values quantile_levels forecast_date target_date\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt; &lt;date&gt;        &lt;date&gt;     \n1 ca        0.219 0.0133           0.025 2021-04-01    2021-04-29 \n2 ca        0.219 0.425            0.975 2021-04-01    2021-04-29"
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-fitted-arx-split-sample",
    "href": "slides/day2-afternoon.html#predict-with-fitted-arx-split-sample",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predict with fitted ARX (split-sample)",
    "text": "Predict with fitted ARX (split-sample)\n\narx_forecaster fits a model to the training set, and outputs only one prediction (for time \\(t_0 +h\\)).\nTo get predictions for the test set:\n\n\npredict(epi_arx$epi_workflow, test)\n\nAn `epi_df` object, 707 x 6 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-11-06 08:50:44.00687\n\n# A tibble: 707 × 6\n   geo_value time_value .pred        .pred_distn forecast_date target_date\n * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;             &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n 1 ca        2021-04-02 0.217 quantiles(0.22)[2] 2021-04-01    2021-04-29 \n 2 ca        2021-04-03 0.203  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 3 ca        2021-04-04 0.197  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 4 ca        2021-04-05 0.202  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 5 ca        2021-04-06 0.199  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 6 ca        2021-04-07 0.196  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 7 ca        2021-04-08 0.193 quantiles(0.19)[2] 2021-04-01    2021-04-29 \n 8 ca        2021-04-09 0.195  quantiles(0.2)[2] 2021-04-01    2021-04-29 \n 9 ca        2021-04-10 0.209 quantiles(0.21)[2] 2021-04-01    2021-04-29 \n10 ca        2021-04-11 0.214 quantiles(0.21)[2] 2021-04-01    2021-04-29 \n# ℹ 697 more rows"
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-arx-when-re-fitting",
    "href": "slides/day2-afternoon.html#predict-with-arx-when-re-fitting",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predict with ARX (when re-fitting)",
    "text": "Predict with ARX (when re-fitting)\nIn practice, if we want to re-train the forecasters as new data arrive, we fit and predict combining arx_forecaster with epi_slide."
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window",
    "href": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predict with ARX (re-fitting on trailing window)",
    "text": "Predict with ARX (re-fitting on trailing window)\n\nh &lt;- 28         #horizon\nw &lt;- 120 + h    #trailing window length\nn &lt;- nrow(ca)   #time-series length\n\n# Specify the forecast dates\nfc_time_values &lt;- seq(from = t0_date, to = ca$time_value[n]-h, by = \"1 day\")\n\n# Slide the arx_forecaster \nepi_pred_trailing &lt;- ca |&gt;\n  epi_slide(\n    ~ arx_forecaster(epi_data = .x,\n                     outcome = \"deaths\", \n                     predictors = c(\"cases\", \"deaths\"), \n                     trainer = linear_reg() |&gt; set_engine(\"lm\"),\n                     args_list = arx_args_list(lags = 0, ahead = h,\n                                               quantile_levels = c(0.025, 0.975))\n                     )$predictions |&gt;\n        pivot_quantiles_wider(.pred_distn),\n  .window_size = w, \n  .ref_time_values = fc_time_values\n)"
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-arx",
    "href": "slides/day2-afternoon.html#predict-with-arx",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predict with ARX",
    "text": "Predict with ARX\n\n\n\nNote (window length)\n\n\nWe set \\(w = 120 + h\\) to match the window size of the ARX model we fitted manually. Previously, when considering a window from \\(t-w\\) to \\(t\\), we had access to all outcomes in that window, and to all predictors between \\(t-w-h\\) and \\(t-h\\). (That’s because we lagged \\(x\\) before applying the window.) So we were “cheating” by saying that the trailing window had length \\(w=120\\), as its actual size was \\(120+h\\)!\n\n\n\n\n\n\nNote (all past)\n\n\nThe method fitting on all past data up to the forecasting date can be implemented by setting:\n.window_size = Inf in epi_slide."
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window-1",
    "href": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predict with ARX (re-fitting on trailing window)",
    "text": "Predict with ARX (re-fitting on trailing window)\n\n\nepi_pred_trailing \n\nAn `epi_df` object, 680 x 9 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-11-06 08:50:44.00687\n\n# A tibble: 680 × 9\n# Groups:   geo_value [1]\n   geo_value time_value cases deaths .pred forecast_date target_date `0.025`\n * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        &lt;date&gt;        &lt;dbl&gt;\n 1 ca        2021-04-01  6.77  0.375 0.337 2021-04-01    2021-04-29   0.0996\n 2 ca        2021-04-02  7.27  0.340 0.340 2021-04-02    2021-04-30   0.104 \n 3 ca        2021-04-03  6.79  0.293 0.331 2021-04-03    2021-05-01   0.0960\n 4 ca        2021-04-04  6.51  0.281 0.327 2021-04-04    2021-05-02   0.0919\n 5 ca        2021-04-05  6.87  0.284 0.330 2021-04-05    2021-05-03   0.0934\n 6 ca        2021-04-06  6.97  0.267 0.329 2021-04-06    2021-05-04   0.0900\n 7 ca        2021-04-07  6.92  0.253 0.326 2021-04-07    2021-05-05   0.0850\n 8 ca        2021-04-08  6.97  0.234 0.322 2021-04-08    2021-05-06   0.0795\n 9 ca        2021-04-09  7.07  0.243 0.322 2021-04-09    2021-05-07   0.0784\n10 ca        2021-04-10  8.33  0.249 0.332 2021-04-10    2021-05-08   0.0878\n# ℹ 670 more rows\n# ℹ 1 more variable: `0.975` &lt;dbl&gt;\n\n\n\nPredict with ARX (re-fitting on trailing window)\n\n\n\n                                 MAE     MASE\ntime series CV + trailing 0.07852942 731.6178\n\n\n\n\nCustomizing arx_forecaster\n\n\n\nSimple adjustments\n\narx_forecaster(epi_data = train |&gt; as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |&gt; set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.025, 0.975)))\n\n\n\nModify predictors to add/drop predictors\n\ne.g. drop deaths for regression with a lagged predictor, or drop cases to get AR model\ndefault: predictors = outcome\n\n\n\n\n\nSimple adjustments\n\narx_forecaster(epi_data = train |&gt; as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |&gt; set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.025, 0.975)))\n\n\nModify trainer to use a model that is not lm (default)\n\n e.g. trainer = quantile_reg()\ncan use any {parsnip} models, see list\n\n\n\n\nSimple adjustments\n\narx_forecaster(epi_data = train |&gt; as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |&gt; set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.025, 0.975)))\n\n\nModify arx_args_list to change lags, horizon, quantile levels, …\n\n\n\narx_args_list(\n  lags = c(0L, 7L, 14L),\n  ahead = 7L,\n  n_training = Inf,\n  forecast_date = NULL,\n  target_date = NULL,\n  adjust_latency = c(\"none\", \"extend_ahead\", \"extend_lags\", \"locf\"),\n  warn_latency = TRUE,\n  quantile_levels = c(0.05, 0.95),\n  symmetrize = TRUE,\n  nonneg = TRUE,\n  quantile_by_key = character(0L),\n  check_enough_data_n = NULL,\n  check_enough_data_epi_keys = NULL,\n  ...\n)\n\n\n\n\n\nForecasting with Versioned Data\n\n\n\nVersioned data\n\nus_data\n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-04-01 / 2023-01-31\nℹ First/last version with update: 2021-04-01 / 2023-02-01\nℹ Versions end: 2023-02-01\nℹ A preview of the table (92197 rows x 5 columns):\n          version time_value geo_value     cases     deaths\n    1: 2021-04-01 2020-04-01        ak        NA         NA\n    2: 2021-04-01 2020-04-02        ak        NA         NA\n    3: 2021-04-01 2020-04-03        ak        NA         NA\n    4: 2021-04-01 2020-04-04        ak        NA         NA\n    5: 2021-04-01 2020-04-05        ak        NA         NA\n   ---                                                     \n92193: 2023-02-01 2023-01-27        wy   4.61203 0.17172453\n92194: 2023-02-01 2023-01-28        wy   4.61203 0.17172453\n92195: 2023-02-01 2023-01-29        wy   4.61203 0.17172453\n92196: 2023-02-01 2023-01-30        wy   4.61203 0.17172453\n92197: 2023-02-01 2023-01-31        wy -14.74378 0.04906416\n\n\n\n\nVersion-aware forecasting with geo-pooling\n\nforecast_dates &lt;- seq(from = t0_date, to = as.Date(\"2023-02-01\"), by = \"1 month\")\nh &lt;- c(7, 14, 21, 28)\n\nforecast_k_days_ahead &lt;- function(epi_archive, forecast_dates, ahead = 7) {\n  epi_archive |&gt;\n    epix_slide(\n      ~ arx_forecaster(\n        .x, \n        outcome = \"deaths\", \n        predictors = c(\"cases\", \"deaths\"),\n        trainer = linear_reg() |&gt; set_engine(\"lm\"),\n        args_list = arx_args_list(lags = 0, ahead = ahead,\n                                  quantile_levels = c(0.025, 0.975))\n      )$predictions |&gt; pivot_quantiles_wider(.pred_distn),\n      .before = 120,\n      .versions = forecast_dates\n    )\n}\n\nforecasts &lt;- bind_rows(map(h, ~ forecast_k_days_ahead(us_data, forecast_dates, ahead = .x)))\n\n\n\nVersion-aware forecasting with geo-pooling\n\n\n\n\nBuilding a forecaster\n\n\n\nPhilosophy of forecasting\n\nWe should build up modular components\nBe able to add/remove layers of complexity sequentially\n\n\n\nPreprocessor: do things to the data before model training\nTrainer: train a model on data, resulting in a fitted model object\nPredictor: make predictions, using a fitted model object\nPostprocessor: do things to the predictions before returning\n\n\n\n\nExamples of preprocessing\n\n\nEDA type stuff\n\nMaking locations/signals commensurate (scaling)\nDealing with revisions\nDetecting and removing outliers\nImputing or removing missing data\n\n\n\n\nFeature engineering\n\nCreating lagged predictors\nDay of Week effects\nRolling averages for smoothing\nLagged differences\nGrowth rates instead of raw signals\nThe sky’s the limit\n\n\n\n\nExamples of postprocessing\n\nImpute missing features\nNowcast current values of features\nBootstrap to get predictive intervals\nInvert scaling or other transforms\nThreshold predictions, [0, max_population]\n\n\n\nFit a forecaster\n\n# A preprocessing \"recipe\" that turns raw data into features / response\nr &lt;- epi_recipe(ca) |&gt;\n  step_epi_lag(cases, lag = c(0, 7, 14)) |&gt;\n  step_epi_lag(deaths, lag = c(0, 7, 14)) |&gt;\n  step_epi_ahead(deaths, ahead = 28) |&gt;\n  step_epi_naomit()\n\n# Training engine\ne &lt;- quantile_reg(quantile_levels = c(.025, .5, .975))\n\n# A postprocessing routine describing what to do to the predictions\nf &lt;- frosting() |&gt;\n  layer_predict() |&gt;\n  layer_threshold(.pred, lower = 0) |&gt; # predictions / intervals should be non-negative\n  layer_add_target_date() |&gt;\n  layer_add_forecast_date()\n\n# Bundle up the preprocessor, training engine, and postprocessor\n# We use quantile regression\newf &lt;- epi_workflow(r, e, f)\n\n# Fit it to data (we could fit this to ANY data that has the same format)\ntrained_ewf &lt;- ewf |&gt; fit(ca)\n\n# examines the recipe to determine what we need to make the prediction\nlatest &lt;- get_test_data(r, ca)\n\n# we could make predictions using the same model on ANY test data\npreds &lt;- trained_ewf |&gt; predict(new_data = latest)\n\n\n\nThanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nForecasting with {epipredict} — cmu-delphi/insightnet-workshop-2024"
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window-2",
    "href": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window-2",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predict with ARX (re-fitting on trailing window)",
    "text": "Predict with ARX (re-fitting on trailing window)\n\n\n\n                                 MAE     MASE\ntime series CV + trailing 0.07852942 731.6178"
  },
  {
    "objectID": "slides/day2-afternoon.html#simple-adjustments",
    "href": "slides/day2-afternoon.html#simple-adjustments",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Simple adjustments",
    "text": "Simple adjustments\n\narx_forecaster(epi_data = train |&gt; as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |&gt; set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.025, 0.975)))\n\n\n\nModify predictors to add/drop predictors\n\ne.g. drop deaths for regression with a lagged predictor, or drop cases to get AR model\ndefault: predictors = outcome"
  },
  {
    "objectID": "slides/day2-afternoon.html#simple-adjustments-1",
    "href": "slides/day2-afternoon.html#simple-adjustments-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Simple adjustments",
    "text": "Simple adjustments\n\narx_forecaster(epi_data = train |&gt; as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |&gt; set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.025, 0.975)))\n\n\nModify trainer to use a model that is not lm (default)\n\n e.g. trainer = quantile_reg()\ncan use any {parsnip} models, see list"
  },
  {
    "objectID": "slides/day2-afternoon.html#simple-adjustments-2",
    "href": "slides/day2-afternoon.html#simple-adjustments-2",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Simple adjustments",
    "text": "Simple adjustments\n\narx_forecaster(epi_data = train |&gt; as_epi_df(), \n               outcome = \"deaths\", \n               predictors = c(\"cases\", \"deaths\"),\n               trainer = linear_reg() |&gt; set_engine(\"lm\"),\n               args_list = arx_args_list(lags = 0, ahead = 28,\n                                         quantile_levels = c(0.025, 0.975)))\n\n\nModify arx_args_list to change lags, horizon, quantile levels, …\n\n\n\narx_args_list(\n  lags = c(0L, 7L, 14L),\n  ahead = 7L,\n  n_training = Inf,\n  forecast_date = NULL,\n  target_date = NULL,\n  adjust_latency = c(\"none\", \"extend_ahead\", \"extend_lags\", \"locf\"),\n  warn_latency = TRUE,\n  quantile_levels = c(0.05, 0.95),\n  symmetrize = TRUE,\n  nonneg = TRUE,\n  quantile_by_key = character(0L),\n  check_enough_data_n = NULL,\n  check_enough_data_epi_keys = NULL,\n  ...\n)"
  },
  {
    "objectID": "slides/day2-afternoon.html#versioned-data",
    "href": "slides/day2-afternoon.html#versioned-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Versioned data",
    "text": "Versioned data\n\nus_data\n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-04-01 / 2023-01-31\nℹ First/last version with update: 2021-04-01 / 2023-02-01\nℹ Versions end: 2023-02-01\nℹ A preview of the table (92197 rows x 5 columns):\n          version time_value geo_value     cases     deaths\n    1: 2021-04-01 2020-04-01        ak        NA         NA\n    2: 2021-04-01 2020-04-02        ak        NA         NA\n    3: 2021-04-01 2020-04-03        ak        NA         NA\n    4: 2021-04-01 2020-04-04        ak        NA         NA\n    5: 2021-04-01 2020-04-05        ak        NA         NA\n   ---                                                     \n92193: 2023-02-01 2023-01-27        wy   4.61203 0.17172453\n92194: 2023-02-01 2023-01-28        wy   4.61203 0.17172453\n92195: 2023-02-01 2023-01-29        wy   4.61203 0.17172453\n92196: 2023-02-01 2023-01-30        wy   4.61203 0.17172453\n92197: 2023-02-01 2023-01-31        wy -14.74378 0.04906416"
  },
  {
    "objectID": "slides/day2-afternoon.html#version-aware-forecasting-with-geo-pooling",
    "href": "slides/day2-afternoon.html#version-aware-forecasting-with-geo-pooling",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Version-aware forecasting with geo-pooling",
    "text": "Version-aware forecasting with geo-pooling\n\nforecast_dates &lt;- seq(from = t0_date, to = as.Date(\"2023-02-01\"), by = \"1 month\")\nh &lt;- c(7, 14, 21, 28)\n\nforecast_k_days_ahead &lt;- function(epi_archive, forecast_dates, ahead = 7) {\n  epi_archive |&gt;\n    epix_slide(\n      ~ arx_forecaster(\n        .x, \n        outcome = \"deaths\", \n        predictors = c(\"cases\", \"deaths\"),\n        trainer = linear_reg() |&gt; set_engine(\"lm\"),\n        args_list = arx_args_list(lags = 0, ahead = ahead,\n                                  quantile_levels = c(0.025, 0.975))\n      )$predictions |&gt; pivot_quantiles_wider(.pred_distn),\n      .before = 120,\n      .versions = forecast_dates\n    )\n}\n\nforecasts &lt;- bind_rows(map(h, ~ forecast_k_days_ahead(us_data, forecast_dates, ahead = .x)))"
  },
  {
    "objectID": "slides/day2-afternoon.html#version-aware-forecasting-with-geo-pooling-1",
    "href": "slides/day2-afternoon.html#version-aware-forecasting-with-geo-pooling-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Version-aware forecasting with geo-pooling",
    "text": "Version-aware forecasting with geo-pooling"
  },
  {
    "objectID": "slides/day2-afternoon.html#philosophy-of-forecasting",
    "href": "slides/day2-afternoon.html#philosophy-of-forecasting",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Philosophy of forecasting",
    "text": "Philosophy of forecasting\n\nWe should build up modular components\nBe able to add/remove layers of complexity sequentially\n\n\n\nPreprocessor: do things to the data before model training\nTrainer: train a model on data, resulting in a fitted model object\nPredictor: make predictions, using a fitted model object\nPostprocessor: do things to the predictions before returning"
  },
  {
    "objectID": "slides/day2-afternoon.html#examples-of-preprocessing",
    "href": "slides/day2-afternoon.html#examples-of-preprocessing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of preprocessing",
    "text": "Examples of preprocessing\n\n\nEDA type stuff\n\nMaking locations/signals commensurate (scaling)\nDealing with revisions\nDetecting and removing outliers\nImputing or removing missing data\n\n\n\n\nFeature engineering\n\nCreating lagged predictors\nDay of Week effects\nRolling averages for smoothing\nLagged differences\nGrowth rates instead of raw signals\nThe sky’s the limit"
  },
  {
    "objectID": "slides/day2-afternoon.html#examples-of-postprocessing",
    "href": "slides/day2-afternoon.html#examples-of-postprocessing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Examples of postprocessing",
    "text": "Examples of postprocessing\n\nImpute missing features\nNowcast current values of features\nBootstrap to get predictive intervals\nInvert scaling or other transforms\nThreshold predictions, [0, max_population]"
  },
  {
    "objectID": "slides/day2-afternoon.html#fit-a-forecaster",
    "href": "slides/day2-afternoon.html#fit-a-forecaster",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fit a forecaster",
    "text": "Fit a forecaster\n\n# A preprocessing \"recipe\" that turns raw data into features / response\nr &lt;- epi_recipe(ca) |&gt;\n  step_epi_lag(cases, lag = c(0, 7, 14)) |&gt;\n  step_epi_lag(deaths, lag = c(0, 7, 14)) |&gt;\n  step_epi_ahead(deaths, ahead = 28) |&gt;\n  step_epi_naomit()\n\n# Training engine\ne &lt;- quantile_reg(quantile_levels = c(.025, .5, .975))\n\n# A postprocessing routine describing what to do to the predictions\nf &lt;- frosting() |&gt;\n  layer_predict() |&gt;\n  layer_threshold(.pred, lower = 0) |&gt; # predictions / intervals should be non-negative\n  layer_add_target_date() |&gt;\n  layer_add_forecast_date()\n\n# Bundle up the preprocessor, training engine, and postprocessor\n# We use quantile regression\newf &lt;- epi_workflow(r, e, f)\n\n# Fit it to data (we could fit this to ANY data that has the same format)\ntrained_ewf &lt;- ewf |&gt; fit(ca)\n\n# examines the recipe to determine what we need to make the prediction\nlatest &lt;- get_test_data(r, ca)\n\n# we could make predictions using the same model on ANY test data\npreds &lt;- trained_ewf |&gt; predict(new_data = latest)"
  },
  {
    "objectID": "slides/day2-afternoon.html#thanks",
    "href": "slides/day2-afternoon.html#thanks",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Thanks:",
    "text": "Thanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nForecasting with {epipredict} — cmu-delphi/insightnet-workshop-2024"
  },
  {
    "objectID": "slides/day2-morning.html#section",
    "href": "slides/day2-morning.html#section",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Forecasting and Time-Series Models",
    "text": "Forecasting and Time-Series Models\n\n\n\n\nVenue – dd Somemonth yyyy"
  },
  {
    "objectID": "slides/day2-morning.html#outline",
    "href": "slides/day2-morning.html#outline",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Outline",
    "text": "Outline\n\nLinear Regression for Time Series Data\nEvaluation Methods\nARX Models\nOverfitting and Regularization\nPrediction Intervals\nForecasting with Versioned Data\nModeling Multiple Time Series"
  },
  {
    "objectID": "slides/day2-morning.html#basics-of-linear-regression",
    "href": "slides/day2-morning.html#basics-of-linear-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Basics of linear regression",
    "text": "Basics of linear regression\n\nAssume we observe a predictor \\(x_i\\) and an outcome \\(y_i\\) for \\(i = 1, \\dots, n\\).\nLinear regression seeks coefficients \\(\\beta_0\\) and \\(\\beta_1\\) such that\n\n\\[y_i \\approx \\beta_0 + \\beta_1 x_i\\]\nis a good approximation for every \\(i = 1, \\dots, n\\).\n\nIn R, the coefficients are found by running lm(y ~ x), where y is the vector of responses and x the vector of predictors."
  },
  {
    "objectID": "slides/day2-morning.html#multiple-linear-regression",
    "href": "slides/day2-morning.html#multiple-linear-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nGiven \\(p\\) different predictors, we seek \\((p+1)\\) coefficients such that\n\n\\[y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\\] is a good approximation for every \\(i = 1, \\dots, n\\)."
  },
  {
    "objectID": "slides/day2-morning.html#linear-regression-with-lagged-predictor",
    "href": "slides/day2-morning.html#linear-regression-with-lagged-predictor",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Linear regression with lagged predictor",
    "text": "Linear regression with lagged predictor\n\nIn time series, outcomes and predictors are usually indexed by time \\(t\\).\n\n\n\nGoal: predicting future \\(y\\), given present \\(x\\).\n\n\n\n\nModel: linear regression with lagged predictor\n\n\\[\\hat y_t = \\hat \\beta + \\hat \\beta_0 x_{t-k}\\]\ni.e. regress the outcome \\(y\\) at time \\(t\\) on the predictor \\(x\\) at time \\(t-k\\).\n\n\n\nEquivalent way to write the model:\n\n\\[\\hat y_{t+k} = \\hat \\beta + \\hat \\beta_0 x_t\\]"
  },
  {
    "objectID": "slides/day2-morning.html#example-predicting-covid-deaths",
    "href": "slides/day2-morning.html#example-predicting-covid-deaths",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example: predicting COVID deaths",
    "text": "Example: predicting COVID deaths\n\nDuring the pandemic, interest in predicting COVID deaths 7, 14, 21, 28 days ahead.\nCan we reasonably predict COVID deaths 28 days ahead by just using cases today?\n\n\n\nIf we let\n\n\\[y_{t+28} = \\text{deaths at time } t+28 \\quad\\quad x_{t} = \\text{cases at time } t\\] is the following a good model?\n\\[\\hat y_{t+28} = \\hat\\beta_0 + \\hat\\beta_1 x_{t}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#example-covid-cases-and-deaths-in-california",
    "href": "slides/day2-morning.html#example-covid-cases-and-deaths-in-california",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Example: COVID cases and deaths in California",
    "text": "Example: COVID cases and deaths in California\n\nLet’s focus on California.\nCases seem highly correlated with deaths several weeks later.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhead(ca)\n\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-11-06 08:50:44.00687\n\n# A tibble: 6 × 4\n# Groups:   geo_value [1]\n  geo_value time_value cases deaths\n* &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 ca        2020-04-01  3.17 0.0734\n2 ca        2020-04-02  3.48 0.0835\n3 ca        2020-04-03  3.44 0.0894\n4 ca        2020-04-04  3.05 0.0778\n5 ca        2020-04-05  3.28 0.0876\n6 ca        2020-04-06  3.37 0.0848"
  },
  {
    "objectID": "slides/day2-morning.html#checking-correlation",
    "href": "slides/day2-morning.html#checking-correlation",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Checking correlation",
    "text": "Checking correlation\n\nLet’s split the data into a training and a test set (before/after 2021-04-01).\nOn training set: large correlation between cases and deaths 28 days ahead (&gt; 0.95).\n\n\n\n\nLet’s use (base) R to prepare the data and fit\n\n\\[\\hat y_{t+28} = \\hat\\beta + \\hat\\beta_0 x_{t}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#preparing-the-data",
    "href": "slides/day2-morning.html#preparing-the-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Preparing the data",
    "text": "Preparing the data\n\n# Add column with cases lagged by k\nca$lagged_cases &lt;- dplyr::lag(ca$cases, n = k)\n\n# Split into train and test (before/after t0_date)\nt0_date &lt;- as.Date('2021-04-01')\ntrain &lt;- ca |&gt; filter(time_value &lt;= t0_date)\ntest &lt;- ca |&gt; filter(time_value &gt; t0_date)\n\nCheck if deaths is approximately linear in lagged_cases:"
  },
  {
    "objectID": "slides/day2-morning.html#fitting-lagged-linear-regression-in-r",
    "href": "slides/day2-morning.html#fitting-lagged-linear-regression-in-r",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fitting lagged linear regression in R",
    "text": "Fitting lagged linear regression in R\n\nreg_lagged = lm(deaths ~ lagged_cases, data = train)\ncoef(reg_lagged)\n\n (Intercept) lagged_cases \n   0.1171839    0.0112714"
  },
  {
    "objectID": "slides/day2-morning.html#error-metrics",
    "href": "slides/day2-morning.html#error-metrics",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Error metrics",
    "text": "Error metrics\n\nAssume we have predictions \\(\\hat y_{new, t}\\) for the unseen observations \\(y_{new,t}\\) over times \\(t = 1, \\dots, N\\).\nFour commonly used error metrics are:\n\nmean squared error (MSE)\nmean absolute error (MAE)\nmean absolute percentage error (MAPE)\nmean absolute scaled error (MASE)"
  },
  {
    "objectID": "slides/day2-morning.html#error-metrics-mse-and-mae",
    "href": "slides/day2-morning.html#error-metrics-mse-and-mae",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Error metrics: MSE and MAE",
    "text": "Error metrics: MSE and MAE\n\\[MSE = \\frac{1}{N} \\sum_{t=1}^N (y_{new, t}- \\hat y_{new, t})^2\\] \\[MAE = \\frac{1}{N} \\sum_{t=1}^N |y_{new, t}- \\hat y_{new, t}|\\]\n\nMAE gives less importance to extreme errors than MSE.\nDrawback: both metrics are scale-dependent, so they are not universally interpretable. (For example, if \\(y\\) captures height, MSE and MAE will vary depending on whether we measure in feet or meters.)"
  },
  {
    "objectID": "slides/day2-morning.html#error-metrics-mape",
    "href": "slides/day2-morning.html#error-metrics-mape",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Error metrics: MAPE",
    "text": "Error metrics: MAPE\n\nFixing scale-dependence:\n\n\\[MAPE = 100 \\times \\frac{1}{N} \\sum_{t=1}^N\n\\left|\\frac{y_{new, t}- \\hat y_{new, t}}{y_{new, t}}\\right|\\]\n\nDrawbacks:\n\nErratic behavior when \\(y_{new, t}\\) is close to zero\nIt assumes the unit of measurement has a meaningful zero (e.g. using Fahrenheit or Celsius to measure temperature will lead to different MAPE)"
  },
  {
    "objectID": "slides/day2-morning.html#comparing-mae-and-mape",
    "href": "slides/day2-morning.html#comparing-mae-and-mape",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Comparing MAE and MAPE",
    "text": "Comparing MAE and MAPE\n\n\n\nNote\n\n\nThere are situations when MAPE is problematic!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n           MAE     MAPE\nyhat1 2.873328 43.14008\nyhat2 5.382247 36.08279"
  },
  {
    "objectID": "slides/day2-morning.html#error-metrics-mase",
    "href": "slides/day2-morning.html#error-metrics-mase",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Error metrics: MASE",
    "text": "Error metrics: MASE\n\\[MASE = 100 \\times \\frac{\\frac{1}{N} \\sum_{t=1}^N\n|y_{new, t}- \\hat y_{new, t}|}\n{\\frac{1}{N-1} \\sum_{t=2}^N\n|y_{new, t}- y_{new, t-1}|}\\]\n\nAdvantages:\n\nis universally interpretable (not scale dependent)\navoids the zero-pitfall\n\nMASE in words: we normalize the error of our forecasts by that of a naive method which always predicts the last observation."
  },
  {
    "objectID": "slides/day2-morning.html#comparing-mae-mape-and-mase",
    "href": "slides/day2-morning.html#comparing-mae-mape-and-mase",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Comparing MAE, MAPE and MASE",
    "text": "Comparing MAE, MAPE and MASE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n           MAE     MAPE      MASE\nyhat1 2.873328 43.14008  66.10004\nyhat2 5.382247 36.08279 123.81696"
  },
  {
    "objectID": "slides/day2-morning.html#defining-the-error-metrics-in-r",
    "href": "slides/day2-morning.html#defining-the-error-metrics-in-r",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Defining the error metrics in R",
    "text": "Defining the error metrics in R\n\nMSE &lt;- function(truth, prediction) {\n  mean((truth - prediction)^2)}\n\nMAE &lt;- function(truth, prediction) {\n  mean(abs(truth - prediction))}\n\nMAPE &lt;- function(truth, prediction) {\n  100 * mean(abs(truth - prediction) / truth)}\n\nMASE &lt;- function(truth, prediction) {\n  100 * MAE(truth, prediction) / mean(abs(diff(truth)))}"
  },
  {
    "objectID": "slides/day2-morning.html#estimating-the-prediction-error",
    "href": "slides/day2-morning.html#estimating-the-prediction-error",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Estimating the prediction error",
    "text": "Estimating the prediction error\n\nGiven an error metric, we want to estimate the prediction error under that metric.\nThis can be accomplished in different ways, using the\n\nTraining error\nSplit-sample error\nTime series cross-validation error (using all past data or a trailing window)"
  },
  {
    "objectID": "slides/day2-morning.html#training-error",
    "href": "slides/day2-morning.html#training-error",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Training error",
    "text": "Training error\n\nThe easiest but worst approach to estimate the prediction error is to use the training error, i.e. the average error on the training set that was used to fit the model.\nThe training error is\n\ngenerally too optimistic as an estimate of prediction error\nmore optimistic the more complex the model!1\n\n\nMore on this when we talk about overfitting."
  },
  {
    "objectID": "slides/day2-morning.html#training-error-1",
    "href": "slides/day2-morning.html#training-error-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Training error",
    "text": "Training error\nLinear regression of COVID deaths on lagged cases\n\n# Getting the predictions for the training set\npred_train &lt;- predict(reg_lagged)\n\n\n\n\n               MAE     MASE\ntraining 0.0740177 380.9996"
  },
  {
    "objectID": "slides/day2-morning.html#split-sample-error",
    "href": "slides/day2-morning.html#split-sample-error",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Split-sample error",
    "text": "Split-sample error\nTo compute the split-sample error\n\nSplit data into training (up to time \\(t_0\\)), and test set (after \\(t_0\\))\nFit the model to the training data only\nMake predictions for the test set\nCompute the selected error metric on the test set only\n\n\n\n\nNote\n\n\nSplit-sample estimates of prediction error don’t mimic a situation where we would refit the model in the future. They are pessimistic if the relation between outcome and predictors changes over time."
  },
  {
    "objectID": "slides/day2-morning.html#split-sample-mse",
    "href": "slides/day2-morning.html#split-sample-mse",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Split-sample MSE",
    "text": "Split-sample MSE\nAssume we want to make \\(h\\)-step ahead predictions, i.e. at time \\(t\\) we want to make a forecast for \\(t+h\\). Then, the split-sample MSE is\n\\[\\text{SplitMSE} = \\frac{1}{n-h-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t_0} - y_{t+h})^2\\]\nwhere \\(\\hat y_{t+h|t_0}\\) indicates a prediction for \\(y\\) at time \\(t+h\\) that was made with a model that was fit on data up to time \\(t_0\\)."
  },
  {
    "objectID": "slides/day2-morning.html#split-sample-error-1",
    "href": "slides/day2-morning.html#split-sample-error-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Split-sample error",
    "text": "Split-sample error\nLinear regression of COVID deaths on lagged cases\n\n# Getting the h-step ahead predictions for the test set\nh &lt;- k\ntest_h &lt;- test[-(1:h-1), ] # drop first h-1 rows to avoid data leakage\npred_test &lt;- predict(reg_lagged, newdata = test_h)\n\n\n\n\n                   MAE      MASE\ntraining     0.0740177  380.9996\nsplit-sample 0.3116854 2914.4575\n\n\n\nNote that we are overestimating the peak due to the changed relationship between cases - deaths over time.\nTalk about data leakage."
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cross-validation-cv",
    "href": "slides/day2-morning.html#time-series-cross-validation-cv",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series cross-validation (CV)",
    "text": "Time-series cross-validation (CV)\n\\(h\\)-step ahead predictions\n\nIf we refit in the future once new data are available, a more appropriate way to estimate the prediction error is time-series cross-validation.\nTo get \\(h\\)-step ahead predictions, for each time \\(t = t_0, t_0+1, \\dots\\),\n\nFit the model using data up to time \\(t\\)\nMake a prediction for \\(t+h\\)\nRecord the prediction error\n\nThe cross-validation MSE is then\n\n\\[CVMSE = \\frac{1}{n-h-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t} - y_{t+h})^2\\]\nwhere \\(\\hat y_{t+h|t}\\) indicates a prediction for \\(y\\) at time \\(t+h\\) that was made with data available up to time \\(t\\)."
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cross-validation-cv-1",
    "href": "slides/day2-morning.html#time-series-cross-validation-cv-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series cross-validation (CV)",
    "text": "Time-series cross-validation (CV)\nLinear regression of COVID deaths on lagged cases\n\nn &lt;- nrow(ca)                               #length of time series\nh &lt;- k                                      #number of days ahead for which prediction is wanted\npred_all_past &lt;- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to all past data and make h-step ahead prediction\n  reg_all_past = lm(deaths ~ lagged_cases, data = ca, subset = (1:n) &lt;= t) \n  pred_all_past[t+h] = predict(reg_all_past, newdata = data.frame(ca[t+h, ]))\n}\n\n\n\n\nNote\n\n\nWith the current model, we can only predict \\(k\\) days ahead (where \\(k\\) = number of days by which predictor is lagged)!"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cross-validation-cv-2",
    "href": "slides/day2-morning.html#time-series-cross-validation-cv-2",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series cross-validation (CV)",
    "text": "Time-series cross-validation (CV)\nLinear regression of COVID deaths on lagged cases\n\n\n\n                     MAE      MASE\ntraining       0.0740177  380.9996\nsplit-sample   0.3116854 2914.4575\ntime series CV 0.2374931 2212.5992\n\n\n\nSome improvement wrt split-sample, but still overestimating peak."
  },
  {
    "objectID": "slides/day2-morning.html#regression-on-a-trailing-window",
    "href": "slides/day2-morning.html#regression-on-a-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Regression on a trailing window",
    "text": "Regression on a trailing window\n\nSo far, to get \\(h\\)-step ahead predictions for time \\(t+h\\), we have fitted the model on all data available up to time \\(t\\). We can instead use a trailing window, i.e. fit the model on a window of data of length \\(w\\), starting at \\(t-w\\) and ending at \\(t\\).\nAdvantage: if the predictors-outcome relation changes over time, training the forecaster on a window of recent data can better capture the recent relation which might be more relevant to predict the outcome in the near future.\nWindow length \\(w\\) considerations:\n\nif \\(w\\) is too big, the model can’t adapt to the recent predictors-outcome relation \ndefault: if \\(w\\) is too small, the fitted model may be too volatile (trained on too little data)"
  },
  {
    "objectID": "slides/day2-morning.html#trailing-window",
    "href": "slides/day2-morning.html#trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Trailing window",
    "text": "Trailing window\nLinear regression of COVID deaths on lagged cases\n\n# Getting the predictions through CV with trailing window\nw &lt;- 120                                    #trailing window size\nh &lt;- k                                      #number of days ahead for which prediction is wanted\npred_trailing &lt;- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to a trailing window of size w and make h-step ahead prediction\n  reg_trailing = lm(deaths ~ lagged_cases, data = ca, \n                    subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n  pred_trailing[t+h] = predict(reg_trailing, newdata = data.frame(ca[t+h, ]))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cv-all-past-vs-trailing-window",
    "href": "slides/day2-morning.html#time-series-cv-all-past-vs-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series CV: all past vs trailing window",
    "text": "Time-series CV: all past vs trailing window\nLinear regression of COVID deaths on lagged cases\n\n\n\n                                 MAE      MASE\ntraining                  0.07401770  380.9996\nsplit-sample              0.31168536 2914.4575\ntime series CV            0.23749306 2212.5992\ntime series CV + trailing 0.09932651  925.3734\n\n\n\nA lot of improvement: trailing window allows to adapt to the change in relationship between cases and deaths over time."
  },
  {
    "objectID": "slides/day2-morning.html#autoregressive-exogenous-input-arx-model",
    "href": "slides/day2-morning.html#autoregressive-exogenous-input-arx-model",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Autoregressive exogenous input (ARX) model",
    "text": "Autoregressive exogenous input (ARX) model\n\nIdea: predicting the outcome via a linear combination of its lags and a set of exogenous (i.e. external) input variables\nExample:\n\n\\[\\hat y_{t+h} = \\hat\\phi + \\sum_{i=0}^p \\hat\\phi_i y_{t-i} + \\sum_{j=0}^q \\hat\\beta_j x_{t-j}\\]\n\nNotice: we don’t need to include all contiguous lags, and we could fit e.g.\n\n\\[\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#arx-model-for-covid-deaths",
    "href": "slides/day2-morning.html#arx-model-for-covid-deaths",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX model for COVID deaths",
    "text": "ARX model for COVID deaths\n\nLet’s add lagged deaths as a predictor to our previous forecaster:\n\n\\[\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}\\]\n\n# Prepare data: add column with deaths lagged by 28\nca$lagged_deaths &lt;- dplyr::lag(ca$deaths, n = k)\n\n\nHow does it compare to the previous model in terms of time-series CV?"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cv-all-past-and-trailing-arx-model",
    "href": "slides/day2-morning.html#time-series-cv-all-past-and-trailing-arx-model",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-Series CV: all past and trailing (ARX model)",
    "text": "Time-Series CV: all past and trailing (ARX model)\n\n\n\n                                 MAE      MASE\ntime series CV            0.16204381 1509.6779\ntime series CV + trailing 0.07872895  733.4767\n\n\n\nErrors under both metrics are smaller than with previous model."
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h",
    "href": "slides/day2-morning.html#predictions-for-different-h",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions for different \\(h\\)",
    "text": "Predictions for different \\(h\\)\n\nSo far we only focused on COVID death predictions 28 days ahead.\nWe will now compare the first model\n\n\\[\\hat y_{t+h} = \\hat\\beta + \\hat\\beta_0 x_t\\]\nto the second model\n\\[\\hat y_{t+h} = \\hat\\phi + \\hat\\phi_0 y_t + \\hat\\beta_0 x_t\\]\nfor horizons \\(h = 7, 14, 21, 28\\).\n\nWe will only make forecasts on the \\(1^{st}\\) day of each month, and use a trailing window with \\(w = 120\\)."
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h-1",
    "href": "slides/day2-morning.html#predictions-for-different-h-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions for different \\(h\\)",
    "text": "Predictions for different \\(h\\)\n\nh_vals &lt;- c(7, 14, 21, 28)  #horizons \npred_m1 = pred_m2 &lt;- data.frame(matrix(NA, nrow = 0, ncol = 3))  #initialize df for predictions\ncolnames(pred_m1) = colnames(pred_m2) = c(\"forecast_date\", \"target_date\", \"prediction\")\nw &lt;- 120    #trailing window size\n\nca_lags &lt;- ca |&gt; select(!c(lagged_cases, lagged_deaths))\n\n# Create lagged predictors \nfor (i in seq_along(h_vals)) {\n  ca_lags[[paste0(\"lagged_deaths_\", h_vals[i])]] &lt;- dplyr::lag(ca_lags$deaths, n = h_vals[i])\n  ca_lags[[paste0(\"lagged_cases_\", h_vals[i])]] &lt;- dplyr::lag(ca_lags$cases, n = h_vals[i])\n}\n\n# Only forecast on 1st day of the months\nforecast_time &lt;- which(ca_lags$time_value &gt;= t0_date & \n                         ca_lags$time_value &lt; ca_lags$time_value[n-max(h_vals)] &\n                         day(ca_lags$time_value) == 1)\n\nfor (t in forecast_time) {\n  for (i in seq_along(h_vals)) {\n    h = h_vals[i]\n    # formulas including h-lagged variables\n    m1_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h))\n    m2_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h, \" + lagged_deaths_\", h))\n    # fit to trailing window of data\n    m1_fit = lm(m1_formula, data = ca_lags, subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n    m2_fit = lm(m2_formula, data = ca_lags, subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n    # make h-step ahead predictions\n    pred_m1 = rbind(pred_m1, \n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m1_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    pred_m2 = rbind(pred_m2, \n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m2_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    }\n}"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h-model-1",
    "href": "slides/day2-morning.html#predictions-for-different-h-model-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions for different \\(h\\) (Model 1)",
    "text": "Predictions for different \\(h\\) (Model 1)\n\n\n\n              MAE    MASE\nModel 1 0.1049742 304.007"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h-model-2",
    "href": "slides/day2-morning.html#predictions-for-different-h-model-2",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions for different \\(h\\) (Model 2)",
    "text": "Predictions for different \\(h\\) (Model 2)\n\n\n\n               MAE     MASE\nModel 2 0.04463132 129.2531"
  },
  {
    "objectID": "slides/day2-morning.html#arx-with-more-predictors",
    "href": "slides/day2-morning.html#arx-with-more-predictors",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX with more predictors",
    "text": "ARX with more predictors\n\nThe ARX model with only two predictors seems to forecast quite well for different \\(h\\).\nWe will try to improve it by adding some more lags. We will fit and compare\n\n\\[\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7}\\]\nand\n\\[\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-using-arx-with-2-lags",
    "href": "slides/day2-morning.html#predictions-using-arx-with-2-lags",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions using ARX with 2 lags",
    "text": "Predictions using ARX with 2 lags\n\n\n\n              MAE     MASE\nModel 3 0.0495936 143.6239"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-using-arx-with-3-lags",
    "href": "slides/day2-morning.html#predictions-using-arx-with-3-lags",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions using ARX with 3 lags",
    "text": "Predictions using ARX with 3 lags\n\n\n\n               MAE     MASE\nModel 4 0.05836984 169.0401"
  },
  {
    "objectID": "slides/day2-morning.html#too-many-predictors",
    "href": "slides/day2-morning.html#too-many-predictors",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Too many predictors",
    "text": "Too many predictors\n\nWhat if we try to incorporate past information extensively by fitting a model with a very large number of predictors?\n\nThe estimated coefficients will be chosen to mimic the observed data very closely on the training set, leading to small training error\nThe predictive performance on the test set might be very poor, producing large split-sample and CV error\n\n\n\n\n\nIssue\n\n\nOverfitting!"
  },
  {
    "objectID": "slides/day2-morning.html#arx-model-for-covid-deaths-with-many-predictors",
    "href": "slides/day2-morning.html#arx-model-for-covid-deaths-with-many-predictors",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX model for COVID deaths with many predictors",
    "text": "ARX model for COVID deaths with many predictors\n\nWhen predicting COVID deaths 28 days ahead, we can try to use more past information by fitting a model that includes the past two months of COVID deaths and cases as predictors\n\n\\[\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-1} + \\dots +\n\\hat\\phi_{59} y_{t-59} +\n\\hat\\beta_0 x_{t} + \\dots + \\hat\\beta_{t-59} x_{t-59}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#preparing-the-data-1",
    "href": "slides/day2-morning.html#preparing-the-data-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Preparing the data",
    "text": "Preparing the data\n\ny &lt;- ca$deaths  #outcome\nlags &lt;- 28:87   #lags used for predictors (deaths and cases)\nh &lt;- 28\n\n# Build predictor matrix with 60 columns\nX &lt;- data.frame(matrix(NA, nrow = length(y), ncol = 2*length(lags)))\ncolnames(X) &lt;- paste('X', 1:ncol(X), sep = '')\n\nfor (j in 1:length(lags)) {\n  # first 60 columns contain deaths lagged by 28, 29, ..., 87\n  X[, j] = dplyr::lag(ca$deaths, lags[j])\n  # last 60 columns contain cases lagged by 28, 29, ..., 87\n  X[, length(lags) + j] = dplyr::lag(ca$cases, lags[j])\n}"
  },
  {
    "objectID": "slides/day2-morning.html#fitting-the-arx-model",
    "href": "slides/day2-morning.html#fitting-the-arx-model",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fitting the ARX model",
    "text": "Fitting the ARX model\n\n# Train/test split\ny_train &lt;- y[1:t0]\nX_train &lt;- X[1:t0, ]\ny_test &lt;- y[(t0+h):length(y)]\nX_test &lt;- X[(t0+h):length(y), ]\n\n# Fitting the ARX model\nreg = lm(y_train ~ ., data = X_train)\ncoef(reg)\n\n  (Intercept)            X1            X2            X3            X4 \n 0.0775116437 -0.5593818462  0.7979659233 -0.4175920623 -0.2780809101 \n           X5            X6            X7            X8            X9 \n 0.3031742752 -0.0809244359  0.2548033065 -0.4860940499  0.1322930625 \n          X10           X11           X12           X13           X14 \n-0.2352041391 -0.1327834155  0.2155747658 -0.3380379255  0.4239820677 \n          X15           X16           X17           X18           X19 \n-0.2012041267 -0.3367851572 -0.2889890062  0.5328712849  0.5388650654 \n          X20           X21           X22           X23           X24 \n-0.3065835983  0.0724595436 -0.0168042757 -0.1171985635  0.2046513639 \n          X25           X26           X27           X28           X29 \n 0.1810480128  0.1213875691  0.0230516587  0.0196208441  0.0397085778 \n          X30           X31           X32           X33           X34 \n-0.3884271784  0.2088690345  0.1248242133  0.0706165553 -0.4882035875 \n          X35           X36           X37           X38           X39 \n 0.3609708771 -0.3169047917  0.4216666798  0.1891753615 -0.1106475626 \n          X40           X41           X42           X43           X44 \n 0.1498605000 -0.0692090064  0.1336287081 -0.1875462008 -0.2449003857 \n          X45           X46           X47           X48           X49 \n-0.0001337325 -0.5738823399  0.0695056705 -0.2460256934  1.0173509442 \n          X50           X51           X52           X53           X54 \n-0.1853591480 -0.5428279059  0.2678983608 -0.6935743948  0.3829408389 \n          X55           X56           X57           X58           X59 \n 0.1088530454  0.9466159031 -0.5618240450 -0.4660113206  0.6102916420 \n          X60           X61           X62           X63           X64 \n-0.2859449807  0.0283237204 -0.0099051792 -0.0070208086  0.0021192306 \n          X65           X66           X67           X68           X69 \n-0.0047341417 -0.0111532015  0.0038542335  0.0184565802 -0.0060684485 \n          X70           X71           X72           X73           X74 \n-0.0005801373  0.0048246180 -0.0061516656 -0.0066597399  0.0039021597 \n          X75           X76           X77           X78           X79 \n 0.0126296042 -0.0080708988 -0.0027091539  0.0052517573 -0.0052000323 \n          X80           X81           X82           X83           X84 \n 0.0029961750  0.0013593227  0.0083628716 -0.0063778828 -0.0018882435 \n          X85           X86           X87           X88           X89 \n-0.0097221295  0.0003314155 -0.0013911110  0.0066935430  0.0107961484 \n          X90           X91           X92           X93           X94 \n-0.0052473765 -0.0057177036  0.0023462634 -0.0112827594  0.0008517257 \n          X95           X96           X97           X98           X99 \n-0.0004388072  0.0231342674 -0.0056794349 -0.0046693142 -0.0061536587 \n         X100          X101          X102          X103          X104 \n-0.0094880392  0.0071605921  0.0021423255  0.0108738290 -0.0015420116 \n         X105          X106          X107          X108          X109 \n 0.0015155025  0.0022482275 -0.0148197121  0.0129113709  0.0009150566 \n         X110          X111          X112          X113          X114 \n 0.0021338029 -0.0019029077 -0.0040171812 -0.0025674957 -0.0069761237 \n         X115          X116          X117          X118          X119 \n 0.0226899068 -0.0022271856 -0.0060651747  0.0071536700 -0.0016426930 \n         X120 \n-0.0127949778"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-on-training-and-test-set",
    "href": "slides/day2-morning.html#predictions-on-training-and-test-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions on training and test set",
    "text": "Predictions on training and test set\n\n\n\n                    MAE      MASE\ntraining     0.04230235  189.9364\nsplit-sample 0.41684935 3883.5685\n\n\n\n\n\nNote\n\n\nSome predictions are negative, which doesn’t make sense for count data, so let’s truncate them at 0."
  },
  {
    "objectID": "slides/day2-morning.html#truncated-predictions-on-training-and-test-set",
    "href": "slides/day2-morning.html#truncated-predictions-on-training-and-test-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Truncated predictions on training and test set",
    "text": "Truncated predictions on training and test set\n\n\n\n                             MAE    MASE\nsplit-sample truncated 0.3978198 3706.28"
  },
  {
    "objectID": "slides/day2-morning.html#regularization",
    "href": "slides/day2-morning.html#regularization",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Regularization",
    "text": "Regularization\n\nIf we want to consider a large number of predictors, how can we avoid overfitting?\nIdea: introduce a regularization parameter \\(\\lambda\\) that shrinks or sets some of the estimated coefficients to zero, i.e. some predictors are estimated to have limited or no predictive power\nMost common regularization methods\n\nRidge: shrinks coefficients to zero\nLasso: sets some coefficients to zero"
  },
  {
    "objectID": "slides/day2-morning.html#arx-ridgelasso-for-covid-deaths",
    "href": "slides/day2-morning.html#arx-ridgelasso-for-covid-deaths",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX + ridge/lasso for COVID deaths",
    "text": "ARX + ridge/lasso for COVID deaths\nLet’s consider a model with lagged cases and deaths: 7 lags for each, spaced by one week.\n\nh &lt;- 28\nlags &lt;- h + 7*(0:6)   #lags used for predictors (deaths and cases)\n\n# Build predictor matrix \nX &lt;- data.frame(matrix(NA, nrow = length(y), ncol = 2*length(lags)))\ncolnames(X) &lt;- paste('X', 1:ncol(X), sep = '')\n\nfor (j in 1:length(lags)) {\n  # lagged deaths \n  X[, j] = dplyr::lag(ca$deaths, lags[j])\n  # lagged cases\n  X[, length(lags) + j] = dplyr::lag(ca$cases, lags[j])\n}\n\n# Train/test split\ny_train &lt;- y[1:t0]\nX_train &lt;- X[1:t0, ]\ny_test &lt;- y[(t0+h):length(y)]\nX_test &lt;- X[(t0+h):length(y), ]"
  },
  {
    "objectID": "slides/day2-morning.html#fit-arx-ridgelasso-for-covid-deaths",
    "href": "slides/day2-morning.html#fit-arx-ridgelasso-for-covid-deaths",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Fit ARX + ridge/lasso for COVID deaths",
    "text": "Fit ARX + ridge/lasso for COVID deaths\n\nlibrary(glmnet) # Implements ridge and lasso\n\n# We'll need to omit NA values explicitly, as otherwise glmnet will complain\nna_obs &lt;- 1:max(lags)\nX_train &lt;- X_train[-na_obs, ]\ny_train &lt;- y_train[-na_obs]\n\n# Ridge regression: set alpha = 0, lambda sequence will be chosen automatically\nridge &lt;- glmnet(X_train, y_train, alpha = 0)\nbeta_ridge &lt;- coef(ridge)       # matrix of estimated coefficients \nlambda_ridge &lt;- ridge$lambda    # sequence of lambdas used to fit ridge \n\n# Lasso regression: set alpha = 1, lambda sequence will be chosen automatically\nlasso &lt;- glmnet(X_train, y_train, alpha = 1)\nbeta_lasso &lt;- coef(lasso)       # matrix of estimated coefficients \nlambda_lasso &lt;- lasso$lambda    # sequence of lambdas used to fit lasso \n\ndim(beta_lasso)      # One row per coefficient, one column per lambda value\n\n[1] 15 92"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-on-test-set-and-best-lambda",
    "href": "slides/day2-morning.html#predictions-on-test-set-and-best-lambda",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions on test set and best \\(\\lambda\\)",
    "text": "Predictions on test set and best \\(\\lambda\\)\n\n# Predict values for second half of the time series\nyhat_ridge &lt;- predict(ridge, newx = as.matrix(X_test))\nyhat_lasso &lt;- predict(lasso, newx = as.matrix(X_test))\n\n# Compute MAE \nmae_ridge &lt;- colMeans(abs(yhat_ridge - y_test))\nmae_lasso &lt;- colMeans(abs(yhat_lasso - y_test))\n\n# Select index of lambda vector which gives lowest MAE\nmin_ridge &lt;- which.min(mae_ridge)\nmin_lasso &lt;- which.min(mae_lasso)\npaste('Best MAE ridge:', round(min(mae_ridge), 3),\n      '; Best MAE lasso:', round(min(mae_lasso), 3))\n\n[1] \"Best MAE ridge: 0.292 ; Best MAE lasso: 0.295\"\n\n# Get predictions for train and test sets\npred_train_ridge &lt;- predict(ridge, newx = as.matrix(X_train))[, min_ridge] \npred_test_ridge &lt;- yhat_ridge[, min_ridge]\npred_train_lasso &lt;- predict(lasso, newx = as.matrix(X_train))[, min_lasso] \npred_test_lasso &lt;- yhat_lasso[, min_lasso]"
  },
  {
    "objectID": "slides/day2-morning.html#estimated-coefficients-shrinkage-vs-sparsity",
    "href": "slides/day2-morning.html#estimated-coefficients-shrinkage-vs-sparsity",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Estimated coefficients: shrinkage vs sparsity",
    "text": "Estimated coefficients: shrinkage vs sparsity\n\n\n\n\n                    ridge       lasso\n(Intercept)  0.2978514823 0.124915807\nX1           0.0433892112 0.069300307\nX2           0.0292256563 0.000000000\nX3           0.0179647838 0.000000000\nX4           0.0083918680 0.000000000\nX5          -0.0015177021 0.000000000\nX6          -0.0125786976 0.000000000\nX7          -0.0191532557 0.000000000\nX8           0.0010586265 0.008320956\nX9           0.0009417383 0.000000000\nX10          0.0008208805 0.001690258\nX11          0.0006535137 0.000000000\nX12          0.0004488789 0.000000000\nX13          0.0002816751 0.000000000\nX14          0.0001839354 0.000000000"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-arx-ridgelasso-train-and-test-set",
    "href": "slides/day2-morning.html#predictions-arx-ridgelasso-train-and-test-set",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions: ARX + ridge/lasso (train and test set)",
    "text": "Predictions: ARX + ridge/lasso (train and test set)\n\n\n\n                         MAE      MASE\nridge training     0.1975073  924.4584\nridge split-sample 0.2923452 2723.6281\nlasso training     0.0790951  370.2149\nlasso split-sample 0.2945295 2743.9784"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cv-for-arx-ridgelasso-trailing",
    "href": "slides/day2-morning.html#time-series-cv-for-arx-ridgelasso-trailing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Time-series CV for ARX + ridge/lasso (trailing)",
    "text": "Time-series CV for ARX + ridge/lasso (trailing)\n\nh &lt;- 28  # number of days ahead \nw &lt;- 120 # window length\n\n# Initialize matrices for predictions (one column per lambda value)\nyhat_ridge &lt;- rep(NA, length = n) \nyhat_lasso &lt;- rep(NA, length = n) \n\nfor (t in t0:(n-h)) {\n  # Choose best lambda\n  cv_inds = max(lags) &lt; 1:n & 1:n &lt;= t-w\n  ridge_cv = cv.glmnet(as.matrix(X[cv_inds, ]), y[cv_inds], alpha = 0)\n  lasso_cv = cv.glmnet(as.matrix(X[cv_inds, ]), y[cv_inds], alpha = 1)\n  best_lambda_ridge = ridge_cv$lambda.1se\n  best_lambda_lasso = lasso_cv$lambda.1se\n  # Indices of data within window\n  inds = t-w &lt; 1:n & 1:n &lt;= t\n  # Fit ARX + ridge/lasso\n  ridge_trail = glmnet(X[inds, ], y[inds], alpha = 0, lambda = best_lambda_ridge)\n  lasso_trail = glmnet(X[inds, ], y[inds], alpha = 1, lambda = best_lambda_lasso)\n  # Predict\n  yhat_ridge[t+h] = predict(ridge_trail, newx = as.matrix(X[(t+h), ]))\n  yhat_lasso[t+h] = predict(lasso_trail, newx = as.matrix(X[(t+h), ]))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-time-series-cv-for-arx-ridgelasso-trailing",
    "href": "slides/day2-morning.html#predictions-time-series-cv-for-arx-ridgelasso-trailing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions: time-series CV for ARX + ridge/lasso (trailing)",
    "text": "Predictions: time-series CV for ARX + ridge/lasso (trailing)\n\n\n\n                          MAE     MASE\nridge CV + trailing 0.1068111  995.103\nlasso CV + trailing 0.1328789 1237.964"
  },
  {
    "objectID": "slides/day2-morning.html#point-predictions-vs-intervals",
    "href": "slides/day2-morning.html#point-predictions-vs-intervals",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Point predictions vs intervals",
    "text": "Point predictions vs intervals\n\nSo far, we have only considered point predictions, i.e.  we have fitted models to provide our best guess on the outcome at time \\(t+h\\).\n\n\n\n\nImportant\n\n\nWhat if we want to provide a measure of uncertainty around the point prediction or a likely range of values for the outcome at time \\(t+h\\)?\n\n\n\n\nFor each target time \\(t+h\\), we can construct prediction intervals, i.e. provide ranges of values that are expected to cover the true outcome value a fixed fraction of times."
  },
  {
    "objectID": "slides/day2-morning.html#prediction-intervals-for-lm-fits",
    "href": "slides/day2-morning.html#prediction-intervals-for-lm-fits",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Prediction intervals for lm fits",
    "text": "Prediction intervals for lm fits\n\nTo get prediction intervals for the models we previously fitted, we only need to tweak our call to predict by adding as an input:\ninterval = \"prediction\", level = p\nwhere \\(p \\in (0, 1)\\) is the desired coverage.\nThe output from predict will then be a matrix with\n\nfirst column a point estimate\nsecond column the lower limit of the interval\nthird column the upper limit of the interval"
  },
  {
    "objectID": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window",
    "href": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Prediction intervals for ARX (CV, trailing window)",
    "text": "Prediction intervals for ARX (CV, trailing window)\n\n# Initialize matrices to store predictions \n# 3 columns: point estimate, lower limit, and upper limit\npred_trailing &lt;- matrix(NA, nrow = n, ncol = 3)\ncolnames(pred_trailing) &lt;- c('prediction', 'lower', 'upper')\n\nfor (t in t0:(n-h)) {\n  # Fit ARX and predict\n  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, \n                    subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n  pred_trailing[t+h, ] = predict(arx_trailing, newdata = data.frame(ca[t+h, ]),\n                                 interval = \"prediction\", level = 0.95)\n}"
  },
  {
    "objectID": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window-1",
    "href": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Prediction intervals for ARX (CV, trailing window)",
    "text": "Prediction intervals for ARX (CV, trailing window)\n\n\n\n                 MAE     MASE\nlm.trailing 0.104397 972.6125"
  },
  {
    "objectID": "slides/day2-morning.html#quantile-regression",
    "href": "slides/day2-morning.html#quantile-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Quantile regression",
    "text": "Quantile regression\n\nSo far we only considered different ways to apply linear regression.\nQuantile regression is a different estimation method, and it directly targets conditional quantiles of the outcome over time.\n\n\n\n\n\n\n\nDefinition\n\n\nConditional quantile = value below which a given percentage (e.g. 25%, 50%, 75%) of observations fall, given specific values of the predictor variables.\n\n\n\n\nAdvantage: it provides a more complete picture of the outcome distribution."
  },
  {
    "objectID": "slides/day2-morning.html#arx-model-for-covid-deaths-via-quantile-regression",
    "href": "slides/day2-morning.html#arx-model-for-covid-deaths-via-quantile-regression",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "ARX model for COVID deaths via quantile regression",
    "text": "ARX model for COVID deaths via quantile regression\n\n#install.packages(\"quantreg\")\nlibrary(quantreg)  #library to perform quantile regression\n\n# Set quantiles of interest: we will focus on 2.5%, 50% (i.e. median), and 97.5% quantiles\nquantiles &lt;- c(0.025, 0.5, 0.975)  \n\n# Fit quantile regression to training set\nq_reg &lt;- rq(deaths ~ lagged_deaths + lagged_cases, data = train, tau = quantiles)\n\n# Estimated coefficients\ncoef(q_reg)\n\n               tau= 0.025 tau= 0.500 tau= 0.975\n(Intercept)   0.009351562 0.06896010 0.12257658\nlagged_deaths 0.229011485 0.19821254 0.28469573\nlagged_cases  0.007439881 0.01022547 0.01265167\n\n# Sort estimated coefficients \ncoefs_sorted &lt;- t(apply(coef(q_reg), 1, sort))\ncolnames(coefs_sorted) &lt;- colnames(coef(q_reg))\ncoefs_sorted\n\n               tau= 0.025 tau= 0.500 tau= 0.975\n(Intercept)   0.009351562 0.06896010 0.12257658\nlagged_deaths 0.198212543 0.22901149 0.28469573\nlagged_cases  0.007439881 0.01022547 0.01265167\n\nq_reg$coefficients &lt;- coefs_sorted"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-via-quantile-regression-cv-trailing",
    "href": "slides/day2-morning.html#predictions-via-quantile-regression-cv-trailing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions via quantile regression (CV, trailing)",
    "text": "Predictions via quantile regression (CV, trailing)\n\n# Initialize matrix to store predictions \n# 3 columns: lower limit, median, and upper limit\npred_trailing &lt;- matrix(NA, nrow = n, ncol = 3)\ncolnames(pred_trailing) &lt;- c('lower', 'median', 'upper')\n\nfor (t in t0:(n-h)) {\n  # Fit quantile regression\n  rq_trailing = rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles,\n                   data = ca, subset = (1:n) &lt;= t & (1:n) &gt; (t-w)) \n  # Sort estimated coefficients \n  coefs_sorted &lt;- t(apply(coef(rq_trailing), 1, sort))\n  rq_trailing$coefficients &lt;- coefs_sorted\n  # Predict\n  pred_trailing[t+h, ] = predict(rq_trailing, newdata = data.frame(ca[t+h, ]))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-via-quantile-regression-cv-trailing-1",
    "href": "slides/day2-morning.html#predictions-via-quantile-regression-cv-trailing-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Predictions via quantile regression (CV, trailing)",
    "text": "Predictions via quantile regression (CV, trailing)\n\n\n\n                  MAE     MASE\nrq.trailing 0.1244401 1159.343"
  },
  {
    "objectID": "slides/day2-morning.html#actual-coverage",
    "href": "slides/day2-morning.html#actual-coverage",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Actual Coverage",
    "text": "Actual Coverage\n\nWe would expect the ARX model fitted via lm and via rq to cover the truth about 95% of the times. Is this actually true in practice?\nThe actual coverage of each predictive interval is lower:\n\n\n\n         lm.trailing rq.trailing\nCoverage   0.8294118   0.8117647"
  },
  {
    "objectID": "slides/day2-morning.html#evaluation-1",
    "href": "slides/day2-morning.html#evaluation-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Evaluation",
    "text": "Evaluation\n\nPrediction intervals are “good” if they\n\ncover the truth most of the time\nare not too wide\n\nError metric that captures both desiderata: Weighted Interval Score (WIS)\n\\(F\\) = forecast composed of predicted quantiles \\(q_{\\tau}\\) for the set of quantile levels \\(\\tau\\). The WIS for target variable \\(Y\\) is represented as (McDonald et al., 2021):\n\n\\[WIS(F, Y) = 2\\sum_{\\tau} \\phi_{\\tau} (Y - q_{\\tau})\\]\nwhere \\(\\phi_{\\tau}(x) = \\tau |x|\\) for \\(x \\geq 0\\) and \\(\\phi_{\\tau}(x) = (1-\\tau) |x|\\) for \\(x &lt; 0\\)."
  },
  {
    "objectID": "slides/day2-morning.html#computing-the-wis",
    "href": "slides/day2-morning.html#computing-the-wis",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Computing the WIS",
    "text": "Computing the WIS\n\nWIS &lt;- function(truth, estimates, quantiles) {\n  2 * sum(pmax(\n    quantiles * (truth - estimates),\n    (1 - quantiles) * (estimates - truth),\n    na.rm = TRUE\n  ))\n}\n\n\n\n\nNote\n\n\nWIS tends to prioritize sharpness (how wide the interval is) relative to coverage (if the interval contains the truth)."
  },
  {
    "objectID": "slides/day2-morning.html#wis-for-arx-fitted-via-lm-and-rq",
    "href": "slides/day2-morning.html#wis-for-arx-fitted-via-lm-and-rq",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "WIS for ARX fitted via lm and rq",
    "text": "WIS for ARX fitted via lm and rq\n\nThe lowest mean WIS is attained by quantile regression.\nNotice: this method has coverage below 95% but is still preferred under WIS because its intervals are narrower than for linear regression.\n\n\n\n  Mean WIS lm Mean WIS rq\n1   0.1335326   0.1056215"
  },
  {
    "objectID": "slides/day2-morning.html#versioned-data",
    "href": "slides/day2-morning.html#versioned-data",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Versioned data",
    "text": "Versioned data\nSo far: data never revised (or simply ignored revisions, as_of today)\n\n\n\nImportant\n\n\nHow can we train forecasters when dealing with versioned data?\n\n\n\n\ndata_archive\n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-04-01 / 2023-03-09\nℹ First/last version with update: 2020-04-02 / 2023-03-10\nℹ Versions end: 2023-03-10\nℹ A preview of the table (148820 rows x 5 columns):\n        geo_value time_value    version case_rate death_rate\n     1:        ak 2020-04-01 2020-04-02  1.797489  0.0000000\n     2:        ak 2020-04-01 2020-05-07  1.777061  0.0000000\n     3:        ak 2020-04-01 2020-10-28  1.106147  0.0000000\n     4:        ak 2020-04-01 2020-10-29  1.797489  0.0000000\n     5:        ak 2020-04-01 2020-10-30  1.797489  0.0000000\n    ---                                                     \n148816:        wy 2023-03-05 2023-03-06  0.000000  0.0000000\n148817:        wy 2023-03-06 2023-03-07  0.000000  0.0000000\n148818:        wy 2023-03-07 2023-03-08 38.809743  0.3434491\n148819:        wy 2023-03-08 2023-03-09  0.000000  0.0000000\n148820:        wy 2023-03-09 2023-03-10  0.000000  0.0000000"
  },
  {
    "objectID": "slides/day2-morning.html#version-aware-forecasting",
    "href": "slides/day2-morning.html#version-aware-forecasting",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Version aware-forecasting",
    "text": "Version aware-forecasting\nImportant: when fitting and predicting, only use data in the latest version available at the forecast date!"
  },
  {
    "objectID": "slides/day2-morning.html#version-aware-forecasting-1",
    "href": "slides/day2-morning.html#version-aware-forecasting-1",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Version-aware forecasting",
    "text": "Version-aware forecasting\n\n# initialize dataframe for predictions\n# 5 columns: forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_trailing &lt;- data.frame(matrix(NA, ncol = 5, nrow = 0))\ncolnames(pred_trailing) &lt;- c(\"forecast_date\", \"target_date\", 'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw &lt;- 120         #trailing window size\nh &lt;- 28          #number of days ahead\n\n# dates when predictions are made (set to be 1 month apart)\nfc_time_values &lt;- seq(from = t0_date, to = as.Date(\"2023-02-01\"), by = \"1 month\")\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data &lt;- epix_as_of(ca_archive, max_version = as.Date(fc_date))\n  # create lagged predictors\n  data$lagged_deaths &lt;- dplyr::lag(data$deaths, h) \n  data$lagged_cases &lt;- dplyr::lag(data$cases, h)\n  # perform quantile regression\n  rq_trailing &lt;- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, \n                    # only consider window of data\n                    data = data |&gt; filter(time_value &gt; (max(time_value) - w))) \n  # sort estimated coefficients \n  coefs_sorted &lt;- t(apply(coef(rq_trailing), 1, sort))\n  colnames(coefs_sorted) &lt;- c('tau..0.025', 'tau..0.500', 'tau..0.975')\n  rq_trailing$coefficients &lt;- coefs_sorted\n  # construct data.frame with the right predictors for the target date\n  predictors &lt;- data.frame(lagged_deaths = tail(data$deaths, 1), \n                           lagged_cases = tail(data$cases, 1))\n  # make predictions for target date and add them to matrix of predictions\n  pred_trailing &lt;- rbind(pred_trailing, \n                         data.frame('forecast_date' = max(data$time_value),\n                                    'target_date' = max(data$time_value) + h, \n                                    predict(rq_trailing, newdata = predictors)))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#version-aware-predictions-cv-trailing",
    "href": "slides/day2-morning.html#version-aware-predictions-cv-trailing",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Version-aware predictions (CV, trailing)",
    "text": "Version-aware predictions (CV, trailing)"
  },
  {
    "objectID": "slides/day2-morning.html#using-geo-information",
    "href": "slides/day2-morning.html#using-geo-information",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Using geo information",
    "text": "Using geo information\n\nAssume we observe data over time from multiple locations (e.g. states or counties).\nWe could\n\nEstimate coefficients separately for each location (as we have done so far).\nFit one model using all locations together at each time point (geo-pooling). Estimated coefficients will not be location specific.\nEstimate coefficients separately for each location, but include predictors capturing averages across locations (partial geo-pooling)."
  },
  {
    "objectID": "slides/day2-morning.html#geo-pooling-trailing-window",
    "href": "slides/day2-morning.html#geo-pooling-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Geo-pooling (trailing window)",
    "text": "Geo-pooling (trailing window)\n\nusa_archive &lt;- data_archive$DT |&gt; \n  as_epi_archive()\n\n# initialize dataframe for predictions\n# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_trailing &lt;- data.frame(matrix(NA, ncol = 6, nrow = 0))\ncolnames(pred_trailing) &lt;- c('geo_value', 'forecast_date', 'target_date',\n                             'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw &lt;- 120         #trailing window size\nh &lt;- 28          #number of days ahead\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data &lt;- epix_as_of(usa_archive, max_version = as.Date(fc_date))\n  \n  # create lagged predictors for each state \n  data &lt;- data |&gt;\n    arrange(geo_value, time_value) |&gt;  \n    group_by(geo_value) |&gt;\n    mutate(lagged_deaths = dplyr::lag(deaths, h),\n           lagged_cases = dplyr::lag(cases, h)) |&gt;\n    ungroup()\n  \n  # perform quantile regression\n  rq_trailing &lt;- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, \n                    # only consider window of data\n                    data = data |&gt; filter(time_value &gt; (max(time_value) - w))) \n  # sort estimated coefficients \n  coefs_sorted &lt;- t(apply(coef(rq_trailing), 1, sort))\n  colnames(coefs_sorted) &lt;- c('tau..0.025', 'tau..0.500', 'tau..0.975')\n  rq_trailing$coefficients &lt;- coefs_sorted\n  \n  # construct dataframe with the right predictors for the target date\n  new_lagged_deaths &lt;- data |&gt; \n    filter(time_value == max(time_value)) |&gt;\n    select(geo_value, deaths)\n  \n  new_lagged_cases &lt;- data |&gt; \n    filter(time_value == max(time_value)) |&gt;\n    select(geo_value, cases)\n  \n  predictors &lt;- new_lagged_deaths |&gt;\n    inner_join(new_lagged_cases, join_by(geo_value)) |&gt;\n    rename(lagged_deaths = deaths,\n           lagged_cases = cases)\n  \n  # make predictions for target date and add them to matrix of predictions\n  pred_trailing &lt;- rbind(pred_trailing, \n                         data.frame(\n                           'geo_value' = predictors$geo_value,\n                           'forecast_date' = max(data$time_value),\n                           'target_date' = max(data$time_value) + h, \n                           predict(rq_trailing, newdata = predictors)))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#geo-pooled-predictions-for-california",
    "href": "slides/day2-morning.html#geo-pooled-predictions-for-california",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Geo-pooled predictions for California",
    "text": "Geo-pooled predictions for California"
  },
  {
    "objectID": "slides/day2-morning.html#partial-geo-pooling-trailing-window",
    "href": "slides/day2-morning.html#partial-geo-pooling-trailing-window",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Partial geo-pooling (trailing window)",
    "text": "Partial geo-pooling (trailing window)\n\n# initialize dataframe for predictions\n# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles\npred_trailing &lt;- data.frame(matrix(NA, ncol = 6, nrow = 0))\ncolnames(pred_trailing) &lt;- c('geo_value', 'forecast_date', 'target_date',\n                             'tau..0.025', 'tau..0.500', 'tau..0.975')\n\nw &lt;- 120         #trailing window size\nh &lt;- 28          #number of days ahead\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data &lt;- epix_as_of(usa_archive, max_version = as.Date(fc_date))\n  \n  # create lagged predictors \n  data &lt;- data |&gt;\n    arrange(geo_value, time_value) |&gt;  \n    group_by(geo_value) |&gt;\n    mutate(lagged_deaths = dplyr::lag(deaths, h),\n           lagged_cases = dplyr::lag(cases, h)) |&gt;\n    ungroup() |&gt;\n    group_by(time_value) |&gt;\n    mutate(avg_lagged_deaths = mean(lagged_deaths, na.rm = T),\n           avg_lagged_cases = mean(lagged_cases, na.rm = T)) |&gt;\n    ungroup() \n  \n  # perform quantile regression\n  rq_trailing &lt;- rq(deaths ~ lagged_deaths + lagged_cases + avg_lagged_deaths +\n                      avg_lagged_cases, tau = quantiles, \n                    data = (data |&gt; filter(geo_value == 'ca'))) \n  \n  # sort estimated coefficients \n  coefs_sorted &lt;- t(apply(coef(rq_trailing), 1, sort))\n  colnames(coefs_sorted) &lt;- c('tau..0.025', 'tau..0.500', 'tau..0.975')\n  rq_trailing$coefficients &lt;- coefs_sorted\n  \n  # construct data.frame with the right predictors for the target date\n  new_lagged_deaths &lt;- data |&gt; \n    filter(time_value == max(time_value)) |&gt;\n    select(geo_value, deaths) |&gt;\n    mutate(avg_lagged_deaths = mean(deaths, na.rm = T)) |&gt;\n    filter(geo_value == 'ca')\n  \n  new_lagged_cases &lt;- data |&gt; \n    filter(time_value == max(time_value)) |&gt;\n    select(geo_value, cases) |&gt;\n    mutate(avg_lagged_cases = mean(cases, na.rm = T)) |&gt;\n    filter(geo_value == 'ca')\n  \n  predictors &lt;- new_lagged_deaths |&gt;\n    inner_join(new_lagged_cases, join_by(geo_value)) |&gt;\n    rename(lagged_deaths = deaths,\n           lagged_cases = cases)\n  \n  # make predictions for target date and add them to matrix of predictions\n  pred_trailing &lt;- rbind(pred_trailing, \n                         data.frame(\n                           'geo_value' = predictors$geo_value,\n                           'forecast_date' = max(data$time_value),\n                           'target_date' = max(data$time_value) + h, \n                           predict(rq_trailing, newdata = predictors)))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#partially-geo-pooled-predictions-for-california",
    "href": "slides/day2-morning.html#partially-geo-pooled-predictions-for-california",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Partially geo-pooled predictions for California",
    "text": "Partially geo-pooled predictions for California"
  },
  {
    "objectID": "slides/day2-morning.html#final-slide",
    "href": "slides/day2-morning.html#final-slide",
    "title": "InsightNet EpiData Workshop 2024",
    "section": "Final slide",
    "text": "Final slide\nThanks:\n\nThe whole CMU Delphi Team (across many institutions)\nOptum/UnitedHealthcare, Change Healthcare.\nGoogle, Facebook, Amazon Web Services.\nQuidel, SafeGraph, Qualtrics.\nCenters for Disease Control and Prevention.\nCouncil of State and Territorial Epidemiologists\n\n\n    \n\n\n\n\n\nForecasting and Time-Series Models — cmu-delphi/insightnet-workshop-2024"
  }
]