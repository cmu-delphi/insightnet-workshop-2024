---
talk-title: "Forecasting with `{epipredict}`"
talk-short-title: "Forecasting"
talk-subtitle: "InsightNet Forecasting Workshop 2024"
talk-date: "12 December -- Afternoon"
format: revealjs
---


{{< include _titleslide.qmd >}}


```{r ggplot-theme}
#| cache: false
ggplot2::theme_set(ggplot2::theme_bw())
```


## Outline

1. `{epipredict}`

1. Customizing `arx_forecaster()`

1. Advanced Customizations

1. Building a Forecaster

1. A Flu Forecaster

1. Advanced Topics

# `{epipredict}` 

## `{epipredict}` 

<https://cmu-delphi.github.io/epipredict>

#### Installation 

```{r install, eval=FALSE}
#| echo: true
# Stable version
# pak::pkg_install("cmu-delphi/epipredict@main")
# Development version
pak::pkg_install("cmu-delphi/epipredict@dev")
```


## What `{epipredict}` provides (i)

Basic and easy to use ["canned" forecasters]{.primary}: 

  * Baseline flat forecaster
  
  * Autoregressive forecaster (ARX)
  
  * Autoregressive classifier
  
  * CDC FluSight flatline forecaster
  
## What `{epipredict}` provides (ii)

* A framework for creating [custom forecasters]{.primary} out of [modular]{.primary} components. 

* There are four types of components:

  1. [Preprocessor]{.primary}: do things to the data before model training
  
  1. [Trainer]{.primary}: train a model on data, resulting in a fitted model object

  1. [Predictor]{.primary}: make predictions, using a fitted model object

  1. [Postprocessor]{.primary}: do things to the predictions before returning
  
  


## Examples of pre-processing

::: {.fragment .fade-in-then-semi-out}

### EDA type stuff

1. Making locations/signals commensurate (scaling)
1. Dealing with revisions 
1. Detecting and removing outliers
1. Imputing or removing missing data

:::

::: {.fragment .fade-in-then-semi-out}

### Feature engineering

1. Creating lagged predictors
1. Day of Week effects
1. Rolling averages for smoothing 
1. Lagged differences
1. Growth rates instead of raw signals
1. The sky's the limit

:::

```{r load-data}
source(here::here("_code/cases_deaths.R"))
```

## Get the data

```{r get-data}
#| echo: true
#| eval: false
library(epidatr)
library(epiprocess)
library(epipredict)

cases <- pub_covidcast(
  source = "jhu-csse",
  signals = "confirmed_incidence_num",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20200401, 20230401),
  geo_values = "*") |>
  select(geo_value, time_value, cases = value)

deaths <- pub_covidcast(
  source = "jhu-csse",
  signals = "deaths_incidence_num",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20200401, 20230401),
  geo_values = "*") |>
  select(geo_value, time_value, deaths = value)
```


## Create an `epi_df`

```{r epi-df}
#| echo: true
df <- left_join(cases, deaths, by = c("time_value", "geo_value")) |>
  as_epi_df()

df
```

## Pre-processing: data scaling

Scale cases and deaths by population and multiply by 100K

```{r scale-data}
#| echo: true
df <- left_join(
  x = df,
  y = state_census |> select(pop, abbr),   # state_census is available in epipredict
  by = c("geo_value" = "abbr")) |>
    mutate(cases = cases / pop * 1e5, 
           deaths = deaths / pop * 1e5) |> 
    select(-pop)
```


## Scaled COVID cases and deaths 

```{r autoplot-deaths}
#| echo: true
#| code-fold: true
#| fig-width: 10
#| fig-height: 5
df |> 
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  autoplot(cases, deaths) 
```

## Pre-processing: smoothing

Smooth the data by computing 7-day averages of cases and deaths for each state

```{r 7dav-data}
#| echo: true
df <- df |>
  group_by(geo_value) |>
  epi_slide(cases_7dav = mean(cases, na.rm = T),
            deaths_7dav = mean(deaths, na.rm = T),
            .window_size = 7) |>
  ungroup() |>
  select(!c(cases, deaths)) |>
  rename(cases = cases_7dav, 
         deaths = deaths_7dav)
```

## Scaled and smoothed COVID cases deaths 

```{r autoplot-7dav-deaths}
#| echo: true
#| code-fold: true
#| fig-width: 10
#| fig-height: 5
df |> 
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  autoplot(cases, deaths) 
```

## Pre-processing: fix outliers and negative values

```{r outliers-deaths}
#| echo: true
#| code-fold: true
#| fig-width: 10
#| fig-height: 5
detection_methods <- dplyr::bind_rows(
  dplyr::tibble(method = "rm", args = list(list(detect_negatives = TRUE)), abbr = "rm")#,
  #dplyr::tibble(method = "stl", args = list(list(detect_negatives = TRUE, seasonal_period = 7)),
  #              abbr = "stl_seasonal")
  )

deaths_outlr <- df |> 
  group_by(geo_value) |>
  mutate(outlier_info = detect_outlr(x = time_value, 
                                     y = deaths, 
                                     methods = detection_methods, 
                                     combiner = "median"
                                     )) |>
  ungroup() |>
  unnest(outlier_info)

deaths_outlr |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |> 
  ggplot(aes(x = time_value)) +
  geom_line(aes(y = deaths), linetype = 2) +
  geom_line(aes(y = combined_replacement, col = geo_value)) +
  geom_hline(yintercept = 0, linetype = 3) +
  facet_wrap(vars(geo_value), scales = "free_y") +
  scale_color_viridis_d() +
  labs(x = "", y = "Covid-19 deaths per 100k people")
```

```{r outlier-cases, message=FALSE}
cases_outlr <- df |> 
  group_by(geo_value) |>
  mutate(outlier_info = detect_outlr(x = time_value, 
                                     y = cases, 
                                     methods = detection_methods, 
                                     combiner = "median"
                                     )) |>
  ungroup() |>
  unnest(outlier_info)

df$deaths <- deaths_outlr$combined_replacement
df$cases <- cases_outlr$combined_replacement
```


## Fit `arx_forecaster` on training set

* Back to the [ARX(1)]{.primary} model for COVID deaths:
$\quad \hat y_{t+28} = \hat\phi + \hat\phi_0 y_{t} + \hat\beta_0 x_{t}$

* Only focus on California (for now)

* Using `{epipredict}`

```{r epipredict-arx}
#| echo: true
#| code-line-numbers: "|7-13"
# split into train and test 
ca <- df |> filter(geo_value == "ca")
t0_date <- as.Date('2021-04-01')
train <- ca |> filter(time_value <= t0_date)
test <- ca |> filter(time_value > t0_date)

# fit ARX
epi_arx <- arx_forecaster(epi_data = train |> as_epi_df(), 
                          outcome = "deaths", 
                          predictors = c("cases", "deaths"),
                          trainer = linear_reg() |> set_engine("lm"),
                          args_list = arx_args_list(lags = 0, ahead = 28,
                                                    quantile_levels = c(0.1, 0.9)))
```

## `arx_forecaster` output

* A [fitted model]{.primary} object which can be used any time in the future to create forecasts (`$epi_workflow`).

* A [forecast]{.primary} (point prediction + interval) 
for 28 days after the last available time value in the data (`$predictions`).


## `arx_forecaster` output

```{r output-arx, message=TRUE}
#| echo: true
epi_arx 
```


## Extract fitted object

<div class="scrollable-output">

```{r epi-workflow-arx, message=TRUE}
#| echo: true
epi_arx$epi_workflow
```

</div>

## `$epi_workflow`

Contains information on 

* [Pre-processing]{.primary} steps automatically performed by `arx_forecaster` (e.g. compute lags of the predictors)

* [Fitted model]{.primary} 

* [Post-processing]{.primary} steps automatically performed by `arx_forecaster` (e.g. compute quantiles)

## Extract predictions

```{r epi-pred-arx}
#| echo: true
epi_arx$predictions
```

::: {.callout-important icon="false"}
## Note 

* `.pred_distn` is actually a “distribution”, parameterized by its quantiles

* `arx_forecaster` estimates the quantiles in a different way than `lm` 
:::


## Extract predictions

We can extract the distribution into a “long” `epi_df`

```{r epi-pred-quantile-longer}
#| echo: true
epi_arx$predictions |>
  pivot_quantiles_longer(.pred_distn)
```

or into a "wide" `epi_df`

```{r epi-pred-quantile-wider}
#| echo: true
epi_arx$predictions |>
  pivot_quantiles_wider(.pred_distn)
```


## Predict with fitted ARX (split-sample)

* `arx_forecaster` fits a model to the training set, and outputs only one prediction (for time $t_0+h$).

* To get [predictions]{.primary} for the [test]{.primary} set:

```{r arx-test-predict}
#| echo: true
predict(epi_arx$epi_workflow, test)
```

## Predict with ARX (when re-fitting)

* In practice, if we want to [re-train]{.primary} the forecasters as [new data]{.primary} arrive,
we fit and predict combining `arx_forecaster` with `epix_slide`

* From now on, we will only used [versioned data]{.primary}, and make predictions once a week

## Predict with ARX (re-fitting on trailing window)

```{r source-versioned-data}
source(here::here("_code/versioned_data.R"))
```

```{r ca-archive, warning=FALSE}
ca_archive <- covid_archive$DT |> 
  filter(geo_value == "ca") |>
  as_epi_archive()
```

```{r epipredict-cv-trailing}
#| echo: true
h <- 28         #horizon
w <- 120 + h    #trailing window length

# Specify the forecast dates
fc_time_values <- seq(from = t0_date, to = as.Date("2023-02-09"), by = "1 week")

# Slide the arx_forecaster over the epi_archive
pred_arx <- ca_archive |>
  epix_slide(
    ~ arx_forecaster(epi_data = .x,
                     outcome = "deaths", 
                     predictors = c("cases", "deaths"), 
                     trainer = linear_reg() |> set_engine("lm"),
                     args_list = arx_args_list(lags = 0, ahead = h,
                                               quantile_levels = c(0.1, 0.9))
                     )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = w, 
  .versions = fc_time_values
)
```

## Predict with ARX 

::: {.callout-important icon="false"}
## Note (window length)

We set $w = 120 + h$ to match the window size of the ARX model we fitted manually.
Previously, when considering a window from $t-w$ to $t$, 
we had access to all outcomes in that window, and to all predictors between 
$t-w-h$ and $t-h$. 
(That's because we lagged $x$ before applying the window.) 
So we were "cheating" by saying that 
the trailing window had length $w=120$, as its actual size was $120+h$! 
:::
  
::: {.callout-important icon="false"}
## Note (all past)

The method [fitting on all past data]{.primary} up to the forecasting date can be 
implemented by setting:

`.window_size = Inf` in `epi_slide`.
:::

```{r epipredict-cv, eval=FALSE}
# slide an arx_forecaster with appropriate outcome, predictions and lags
pred_arx <- ca_archive |>
  epix_slide(
    ~ arx_forecaster(epi_data = .x,
                     outcome = "deaths", 
                     predictors = c("cases", "deaths"), 
                     trainer = linear_reg() |> set_engine("lm"),
                     args_list = arx_args_list(lags = 0, ahead = h,
                                               quantile_levels = c(0.1, 0.9))
                     )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = Inf, 
  .versions = fc_time_values
)
```


## Predict with ARX (re-fitting on trailing window)

<div class="large-output">

```{r epipredict-cv-trailing-head}
#| echo: true
pred_arx 
```

</div>

## Predict with ARX (re-fitting on trailing window)

```{r arx-plot-cv-predictions}
#| fig-align: left
pred_arx |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca, aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) + 
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +    
  geom_vline(xintercept = t0_date + h, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```

```{r function errors}
MSE <- function(truth, prediction) {
  mean((truth - prediction)^2)}

MAE <- function(truth, prediction) {
  mean(abs(truth - prediction))}

MAPE <- function(truth, prediction) {
  100 * mean(abs(truth - prediction) / truth)}

MASE <- function(truth, prediction) {
  100 * MAE(truth, prediction) / mean(abs(diff(truth)))}

getErrors <- function(truth, prediction, type) {
  return(data.frame(#"MSE" = MSE(truth, prediction), 
                    "MAE"= MAE(truth, prediction), 
                    #"MAPE" = MAPE(truth, prediction), 
                    "MASE" = MASE(truth, prediction), 
                    row.names = type))
}

getAccuracy = function(finalized, predictions, row_name = "") {
  observed = (finalized |> 
                filter(time_value %in% predictions$target_date))$deaths
  return(cbind(
    getErrors(observed, predictions$.pred, ""),
    "Coverage" = mean(observed >= predictions$`0.1` & observed <= predictions$`0.9`), 
    row.names = row_name))
}
```

```{r error-arx}
getAccuracy(ca, pred_arx)
```


## Customizing `arx_forecaster`

```{r print-model-1}
#| echo: true
#| eval: false
#| code-line-numbers: "|3"
arx_forecaster(epi_data = train |> as_epi_df(), 
               outcome = "deaths", 
               predictors = c("cases", "deaths"),
               trainer = linear_reg() |> set_engine("lm"),
               args_list = arx_args_list(lags = 0, ahead = 28,
                                         quantile_levels = c(0.1, 0.9)))
```

::: {.fragment .fade-in}
* Modify `predictors` to add/drop predictors 

  * <span class="inner-list">e.g. drop `deaths` for regression with a 
  lagged predictor, or drop `cases` to get AR model</span>

  * <span class="inner-list">default: `predictors = outcome`</span>

:::  
  

## Customizing `arx_forecaster`

```{r print-model-3}
#| echo: true
#| eval: false
#| code-line-numbers: "5-6"
arx_forecaster(epi_data = train |> as_epi_df(), 
               outcome = "deaths", 
               predictors = c("cases", "deaths"),
               trainer = linear_reg() |> set_engine("lm"),
               args_list = arx_args_list(lags = 0, ahead = 28,
                                         quantile_levels = c(0.1, 0.9)))
```

* Modify `arx_args_list` to change lags, horizon, quantile levels, ...

::: {.fragment .fade-in}
```{r arx_args_list}
#| echo: true
#| eval: false
arx_args_list(
  lags = c(0L, 7L, 14L),
  ahead = 7L,
  n_training = Inf,
  forecast_date = NULL,
  target_date = NULL,
  adjust_latency = c("none", "extend_ahead", "extend_lags", "locf"),
  warn_latency = TRUE,
  quantile_levels = c(0.05, 0.95),
  symmetrize = TRUE,
  nonneg = TRUE,
  quantile_by_key = character(0L),
  check_enough_data_n = NULL,
  check_enough_data_epi_keys = NULL,
  ...
)
```
:::

## Customizing `arx_forecaster`

### Change predictors: doctor visits instead of cases

```{r get-doctor-visits-data}
#| echo: true
#| eval: false
dv_archive <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20200401, 20230401),
  geo_values = "*",
  issues = epirange(20200401, 20230401)) |>
  select(geo_value, time_value, version = issue, doctor_visits = value) |>
  arrange(geo_value, time_value) |>
  as_epi_archive(compactify = FALSE)
```

## Customizing `arx_forecaster`

### Change predictors: doctor visits instead of cases

```{r get-archives, warning=FALSE}
ca_archive_dv <- covid_archive_dv$DT |> 
  filter(geo_value == "ca") |>
  as_epi_archive()

usa_archive_dv <- covid_archive_dv$DT |> 
  as_epi_archive()

usa_archive <- covid_archive$DT |> 
  as_epi_archive()
```


```{r arx-with-dv}
#| echo: true
#| code-line-numbers: "5"
pred_arx_hosp <- ca_archive_dv |>
  epix_slide(
    ~ arx_forecaster(epi_data = .x,
                     outcome = "deaths", 
                     predictors = c("deaths", "doctor_visits"), 
                     trainer = linear_reg() |> set_engine("lm"),
                     args_list = arx_args_list(lags = 0, ahead = 28,
                                               quantile_levels = c(0.1, 0.9))
                     )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = w, 
  .versions = fc_time_values
)
```

## Predictions (doctor visits instead of cases in predictor set)

```{r arx-with-dv-plot}
#| fig-align: left
pred_arx_hosp |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca, aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) + 
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +    
  geom_vline(xintercept = t0_date + h, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```

```{r error-arx-with-dv}
getAccuracy(ca, pred_arx_hosp)
```


## Customizing `arx_forecaster`

### Add more lags

```{r arx-with-more-lags}
#| echo: true
#| code-line-numbers: "8"
pred_arx_more_lags <- ca_archive_dv |>
  epix_slide(
    ~ arx_forecaster(epi_data = .x,
                     outcome = "deaths", 
                     predictors = c("deaths", "doctor_visits"), 
                     trainer = linear_reg() |> set_engine("lm"),
                     args_list = arx_args_list(
                       lags = c(0, 7, 14),
                       ahead = 28,
                       quantile_levels = c(0.1, 0.9))
                     )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = w, 
  .versions = fc_time_values
)
```

## Predictions (more lags)

```{r arx-with-more-lags-plot}
#| fig-align: left
pred_arx_more_lags |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca, aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) + 
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +    
  geom_vline(xintercept = t0_date + h, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```

```{r error-arx-more-lags}
getAccuracy(ca, pred_arx_more_lags)
```



## Customizing `arx_forecaster`

### Multiple horizons

```{r arx-multiple-h}
#| echo: true
#| code-line-numbers: "1-2,11,18|19-20"
forecast_times <- seq(from = t0_date, to = as.Date("2023-02-23"), by = "1 month")
pred_h_days_ahead <- function(epi_archive, ahead = 7) {
  epi_archive |>
    epix_slide(
      ~ arx_forecaster(epi_data = .x,
                       outcome = "deaths", 
                       predictors = c("deaths", "doctor_visits"), 
                       trainer = linear_reg() |> set_engine("lm"),
                       args_list = arx_args_list(
                         lags = 0,  
                         ahead = ahead,
                         quantile_levels = c(0.1, 0.9))
      )$predictions |> 
        pivot_quantiles_wider(.pred_distn),
  .before = w, 
  .versions = forecast_times
  )
}
h <- c(7, 14, 21, 28)
forecasts <- bind_rows(map(h, ~ pred_h_days_ahead(ca_archive_dv, ahead = .x)))
```

## Predictions (multiple horizons)

```{r arx-multiple-h-plot}
#| fig-width: 10
#| fig-height: 5
ggplot(data = forecasts, aes(x = target_date, group = forecast_date)) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`, fill = factor(forecast_date)), 
              alpha = 0.4) +
  geom_vline(aes(xintercept = forecast_date, color = factor(forecast_date)), 
             lty = 2) +
  geom_line(data = ca,
    aes(x = time_value, y = deaths),
    inherit.aes = FALSE, na.rm = TRUE, alpha = .5
  ) +
  geom_line(aes(y = .pred, color = factor(forecast_date))) +
  geom_point(aes(y = .pred, color = factor(forecast_date))) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "none")
```


# Advanced Customizations

## Changing trainer

```{r print-model-2}
#| echo: true
#| eval: false
#| code-line-numbers: "4"
arx_forecaster(epi_data = train |> as_epi_df(), 
               outcome = "deaths", 
               predictors = c("cases", "deaths"),
               trainer = linear_reg() |> set_engine("lm"),
               args_list = arx_args_list(lags = 0, ahead = 28,
                                         quantile_levels = c(0.1, 0.9)))
```

* Modify `trainer` to use a model that is not `lm` (default)

  * <span class="inner-list"> e.g. `trainer = rand_forest()`</span>
  
  * <span class="inner-list">can use any `{parsnip}` models, 
  see [list](https://www.tidymodels.org/find/parsnip/)</span>
  
  

## Changing trainer

```{r arx-with-random-forests}
#| echo: true
#| code-line-numbers: "6"
pred_arx_rf <- ca_archive_dv |>
  epix_slide(
    ~ arx_forecaster(epi_data = .x,
                     outcome = "deaths", 
                     predictors = c("deaths", "doctor_visits"), 
                     trainer = parsnip::rand_forest(mode = "regression"), # defaults to ranger
                     args_list = arx_args_list(
                       lags = 0,
                       ahead = 28,
                       quantile_levels = c(0.1, 0.9))
                     )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = w, 
  .versions = fc_time_values
)
```

## Predictions (trained using random forest)

```{r arx-with-random-forests-plot}
#| fig-align: left
pred_arx_rf |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca, aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) + 
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +    
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```


```{r error-arx-random-forests}
getAccuracy(ca, pred_arx_rf)
```

## Warning!

Random forests has really [poor coverage]{.primary} here. 
Can [change engine]{.primary} to get better coverage: 

specify `engine = "grf_quantiles"` in the `rand_forest` call

```{r arx-with-grf}
#| eval: false
pred_arx_grf <- ca_archive_dv |>
  epix_slide(
    ~ arx_forecaster(epi_data = .x,
                     outcome = "deaths", 
                     predictors = c("deaths", "doctor_visits"), 
                     trainer = parsnip::rand_forest(mode = "regression",
                                                    engine = "grf_quantiles"),
                     args_list = arx_args_list(
                       lags = 0,
                       ahead = 28,
                       quantile_levels = c(0.1, 0.9))
                     )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = w, 
  .versions = fc_time_values
)
pred_arx_grf |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca, aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) + 
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +    
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```


```{r error-arx-grf, eval=FALSE}
getAccuracy(ca, pred_arx_grf)
```


## Geo-pooling

* Assume we observe data over time from [multiple locations]{.primary}
(e.g. states or counties).

* We could

  * Estimate coefficients [separately]{.primary} for each location (as we have done so far), or

  * Fit one model using all locations together at each time point ([geo-pooling]{.primary}).
Estimated coefficients will not be location specific.

* We will now pool data from [all US states]{.primary} to make predictions.

## Geo-pooling

```{r arx-geo-pooling}
#| echo: true
#| code-line-numbers: "1"
pred_arx_geo_pool <- usa_archive_dv |>
  epix_slide(
    ~ arx_forecaster(epi_data = .x,
                     outcome = "deaths", 
                     predictors = c("deaths", "doctor_visits"), 
                     trainer = linear_reg() |> set_engine("lm"),
                     args_list = arx_args_list(
                       lags = 0, #c(0, 7, 14),
                       ahead = 28,
                       quantile_levels = c(0.1, 0.9))
                     )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = w, 
  .versions = fc_time_values
)
```

## Predictions (geo-pooling): California

[Error is worse]{.primary} than without geo-pooling, 
but much [better coverage]{.primary} (close to nominal 80%)

```{r arx-geo-pooling-plot-ca}
#| fig-align: left
pred_arx_geo_pool |>
  filter(geo_value == "ca") |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca,
            aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) + 
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +    
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```

```{r error-arx-geo-pooling}
pred_ca_geo_pool <- pred_arx_geo_pool |> filter(geo_value == "ca")
getAccuracy(ca, pred_ca_geo_pool, "CA")
```


## Predictions (geo-pooling)

```{r arx-geo-pooling-plot}
#| fig-align: left
pred_arx_geo_pool |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = epix_as_of(usa_archive_dv, version = usa_archive_dv$versions_end) |> 
              filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
              mutate(deaths = pmax(0, deaths)), 
             aes(x = time_value, y = deaths),
    inherit.aes = FALSE, na.rm = TRUE, alpha = .5) +
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +    
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  facet_wrap(vars(geo_value), scales = 'free_y') +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.position = "none")

```

```{r error-geo-pooling-all-states}
final_data = epix_as_of(usa_archive_dv, version = usa_archive_dv$versions_end) |> 
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  mutate(deaths = pmax(0, deaths))

rbind(getAccuracy(final_data |> filter(geo_value == "ma"), 
                  pred_arx_geo_pool |> 
                    filter(geo_value == "ma" & 
                             target_date %in% (final_data |> filter(geo_value == "ma"))$time_value), 
                  "MA"),
      getAccuracy(final_data |> filter(geo_value == "ny"), 
                  pred_arx_geo_pool |> 
                    filter(geo_value == "ny" & 
                             target_date %in% (final_data |> filter(geo_value == "ny"))$time_value), 
                  "NY"),
      getAccuracy(final_data |> filter(geo_value == "tx"),
                  pred_arx_geo_pool |> 
                    filter(geo_value == "tx" & 
                             target_date %in% (final_data |> filter(geo_value == "tx"))$time_value), 
                  "TX"))
```


## Quantile regression

* Quantile regression is a different estimation method, which directly targets conditional 
quantiles of the outcome over time

* It [needs more data]{.primary} to estimate quantiles appropriately, so

  * unsuitable for settings with small training set (e.g. trailing window on one state)
  
  * can benefit by combination with geo-pooling (much more data to train on)

```{r qr-geo-pooling}
#| echo: true
#| code-line-numbers: "|8"
library(quantreg)

pred_qr_geo_pool <- usa_archive_dv |>
  epix_slide(
    ~ arx_forecaster(epi_data = .x,
                     outcome = "deaths", 
                     predictors = c("deaths", "doctor_visits"), 
                     trainer = quantile_reg(),
                     args_list = arx_args_list(
                       lags = 0, #c(0, 7, 14),
                       ahead = 28,
                       quantile_levels = c(0.1, 0.9))
                     )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = w, 
  .versions = fc_time_values
)
```

## Predictions (geo-pooling + quantile regression): California

```{r qr-geo-pooling-plot-ca}
#| fig-align: left
pred_qr_geo_pool |>
  filter(geo_value == "ca") |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca,
            aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) + 
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +    
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```

```{r error-qr-geo-pooling}
pred_ca_qr_geo_pool <- pred_qr_geo_pool |> filter(geo_value == "ca")
getAccuracy(ca, pred_ca_qr_geo_pool, "CA")
```


## Predictions (geo-pooling + quantile regression)

```{r qr-geo-pooling-plot}
#| fig-align: left
pred_qr_geo_pool |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = epix_as_of(usa_archive_dv, version = usa_archive_dv$versions_end) |> 
              filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
              mutate(deaths = pmax(0, deaths)), 
             aes(x = time_value, y = deaths),
    inherit.aes = FALSE, na.rm = TRUE, alpha = .5) +
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +    
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  facet_wrap(vars(geo_value), scales = 'free_y') +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.position = "none")

```

```{r error-qr-geo-pooling-all-states}
rbind(getAccuracy(final_data |> filter(geo_value == "ma"), 
                  pred_qr_geo_pool |> 
                    filter(geo_value == "ma" & 
                             target_date %in% (final_data |> filter(geo_value == "ma"))$time_value), 
                  "MA"),
      getAccuracy(final_data |> filter(geo_value == "ny"), 
                  pred_qr_geo_pool |> 
                    filter(geo_value == "ny" & 
                             target_date %in% (final_data |> filter(geo_value == "ny"))$time_value), 
                  "NY"),
      getAccuracy(final_data |> filter(geo_value == "tx"),
                  pred_qr_geo_pool |> 
                    filter(geo_value == "tx" & 
                             target_date %in% (final_data |> filter(geo_value == "tx"))$time_value), 
                  "TX"))
```


# Building a forecaster

## Philosophy of forecasting

::: {.fragment .fade-in-then-semi-out}

We should build up modular components

Be able to add/remove layers of complexity sequentially

:::

::: {.fragment .fade-in-then-semi-out}

  1. [Preprocessor]{.primary}: do things to the data before model training
  
  1. [Trainer]{.primary}: train a model on data, resulting in a fitted model object

  1. [Predictor]{.primary}: make predictions, using a fitted model object

  1. [Postprocessor]{.primary}: do things to the predictions before returning
  
:::

## Fit a forecaster from scratch

So far, we performed some [manual pre-processing]{.primary}, and then relied on 
a [canned forecaster]{.primary}
to automatically perform [more pre-processing]{.primary}, [training]{.primary}, [predicting]{.primary}, and [post-processing]{.primary}.


::: {.callout-important icon="false"}
## What if we want more direct control on each single step?

:::

## Fit a forecaster from scratch

```{r forecaster-from-scratch}
#| echo: true
#| eval: false
#| code-line-numbers: "1-6|8-9|11-16|18-20|21-29"
# A preprocessing "recipe" that turns raw data into features / response
r <- epi_recipe(ca) |>
  step_epi_lag(cases, lag = c(0, 7, 14)) |>
  step_epi_lag(deaths, lag = c(0, 7, 14)) |>
  step_epi_ahead(deaths, ahead = 28) |>
  step_epi_naomit()

# Training engine
e <- quantile_reg(quantile_levels = c(.1, .5, .9))

# A post-processing routine describing what to do to the predictions
f <- frosting() |>
  layer_predict() |>
  layer_threshold(.pred, lower = 0) |> # predictions / intervals should be non-negative
  layer_add_target_date() |>
  layer_add_forecast_date()

# Bundle up the preprocessor, training engine, and postprocessor
# We use quantile regression
ewf <- epi_workflow(r, e, f)

# Fit it to data (we could fit this to ANY data that has the same format)
trained_ewf <- ewf |> fit(ca)

# examines the recipe to determine what we need to make the prediction
latest <- get_test_data(r, ca)

# we could make predictions using the same model on ANY test data
preds <- trained_ewf |> predict(new_data = latest)
```


# A Flu Forecaster

## Flu data archive

```{r load-flu-data}
source(here::here("_code/weekly_hhs.R"))
```

```{r show-flu-data}
weekly_archive

flu <- weekly_archive$DT |> 
  filter(version == "2023-11-15") |>
  as_epi_df()
```

## Build forecaster

```{r forecaster-flu}
#| echo: true
# A preprocessing "recipe" that turns raw data into features / response
r <- epi_recipe(flu) |>
  #drop_non_seasons() |>
  step_population_scaling(
    hhs,
    df = epidatasets::state_census,
    df_pop_col = "pop",
    create_new = FALSE,
    rate_rescaling = 1e5,
    by = c("geo_value" = "abbr")) |>
  step_epi_lag(hhs, lag = c(0, 7, 14)) |>
  step_epi_ahead(hhs, ahead = 14) |>
  step_epi_naomit()

# Training engine
e <- quantile_reg(quantile_levels = c(0.01, 0.025, 1:19 / 20, 0.975, 0.99)) # 23 ForecastHub quantiles

# A post-processing routine describing what to do to the predictions
f <- frosting() |>
  layer_predict() |>
  layer_threshold(.pred, lower = 0) |> # predictions / intervals should be non-negative
  layer_population_scaling(
    .pred, 
    df = epidatasets::state_census,
    df_pop_col = "pop",
    create_new = FALSE,
    rate_rescaling = 1e5,
    by = c("geo_value" = "abbr")) |>
  layer_add_target_date() |>
  layer_add_forecast_date()

# Bundle up the preprocessor, training engine, and postprocessor
# We use quantile regression
ewf <- epi_workflow(r, e, f)

# Fit it to data (we could fit this to ANY data that has the same format)
trained_ewf <- ewf |> fit(flu)

# examines the recipe to determine what we need to make the prediction
latest <- get_test_data(r, flu)

# we could make predictions using the same model on ANY test data
preds <- trained_ewf |> predict(new_data = latest)
```

## Predictions at one forecast date

```{r pred-one-forecast-date}
preds |> pivot_quantiles_wider(.pred)
```

## Slide forecaster

```{r slide-flu-forecaster}
#| echo: true
flu_forecast <- function(epi_archive, forecast_date, ahead = 14) {
  flu <- epi_archive$DT |> 
    filter(version == forecast_date) |>
    as_epi_df()

  r <- epi_recipe(flu) |>
    #drop_non_seasons() |>
    step_population_scaling(
      hhs,
      df = epidatasets::state_census,
      df_pop_col = "pop",
      create_new = FALSE,
      rate_rescaling = 1e5,
      by = c("geo_value" = "abbr")) |>
    step_epi_lag(hhs, lag = c(0, 7, 14)) |>
    step_epi_ahead(hhs, ahead = ahead) |>
    step_epi_naomit()
  
  ewf <- epi_workflow(r, e, f)
  trained_ewf <- ewf |> fit(flu)
  latest <- get_test_data(r, flu)
  preds <- trained_ewf |> predict(new_data = latest)
  return(preds)
}

forecast_dates <- seq.Date(as.Date("2023-10-04"), as.Date("2024-03-27"), by = 7L)
forecasts <- bind_rows(map(forecast_dates, 
                           ~ flu_forecast(weekly_archive, forecast_date = .x, ahead = 14)))
```

## Version-aware predictions

```{r plot-flu-predictions}
forecasts |>
  pivot_quantiles_wider(.pred) |>
  filter(geo_value %in% c("ca", "fl", "ny", "tx")) |>
  ggplot(aes(x = target_date)) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = 0.2) +
  #geom_vline(data = data_at_fc, aes(color = factor(version), xintercept = version), lty = 2) +
  geom_line(
    data = weekly_archive$DT |> 
      filter(geo_value %in% c("ca", "fl", "ny", "tx") &
               time_value >= min(forecasts$forecast_date)),
    aes(x = time_value, y = hhs, color = factor(version)),
    inherit.aes = FALSE, na.rm = TRUE, alpha = .5
  ) +
  geom_line(aes(y = `0.5`, color = factor(time_value))) +
  geom_point(aes(y = `0.5`, color = factor(time_value)), size = 1) +
  facet_wrap(vars(geo_value)) +
  labs(x = "", y = "Hospitalizations") +
  theme(legend.position = "none")
```


# Advanced Topics

## Ensembling

* Instead of choosing one model, [combine]{.primary} the predictions from multiple constituent models

* Ensemble types

  * [untrained]{.primary}: combines multiple constituent models without considering their past performance 
  
  * [trained]{.primary}: weights different constituent models based on their accuracy
  
* Goals

  * [compete-with-best]{.primary}: ensemble should have accuracy competitive with best individual constituent model
  
  * [robustness-over-all]{.primary}: ensemble should have greater robustness than any individual constituent model

* Typically: untrained ensembles are robust but less accurate, 
and trained ensembles are accurate but less robust.

## Calibration

* We have seen that prediction intervals often have [nominal coverage << actual empirical coverage]{.primary},
e.g. 80% predictive intervals that in practice cover the truth $\approx$ 60% of the times

* Calibration aims at adjusting the intervals so that 
nominal coverage $\approx$ empirical coverage


## Calibration

### Quantile tracking

* Let $\hat q_{t}^{1-\alpha}$ = predicted level $1-\alpha$ quantile of the 
distribution of $e_t$, the absolute forecast error at time $t$:

$$e_t = |y_t - \hat y_{t|t-1}|.$$

* Given a learning rate $\eta>0$, update the quantiles as

$$\hat q_{t+1}^{1-\alpha} = \begin{cases} 
\hat q_{t}^{1-\alpha} + \eta(1-\alpha) \quad \text{if } x_t\notin I_{t|t-1}^{1-\alpha} \\
\hat q_{t}^{1-\alpha} - \eta\alpha \quad \quad \quad \,\,\, \text{if } x_t\in I_{t|t-1}^{1-\alpha}
\end{cases}$$

where $I_{t|t-1}^{1-\alpha}$ is the 
$1-\alpha$ prediction interval made by a forecaster at time $t$ for time $t+1$.

* Quantile tracking in words: if the latest interval does not cover, increase the quantile by 
$\eta(1-\alpha)$ (make the next interval wider), otherwise decrease the quantile by $\eta\alpha$
(make the next interval narrower).

## Multi-horizon smoothing
