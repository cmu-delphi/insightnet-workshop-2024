---
talk-title: "Forecasting and Time-Series Models"
talk-short-title: "{{< meta talk-title >}}"
talk-subtitle: ""
author: ""
other-authors: ""
repo-address: "cmu-delphi/insightnet-workshop-2024"
talk-date: "Venue -- dd Somemonth yyyy"
format: revealjs
execute:
  cache: false
---


<!-- Set any of the above to "" to omit them -->

<!-- Or adjust the formatting in _titleslide.qmd -->
---
---

\DeclareMathOperator*{\minimize}{minimize}

```{r setup}
#| include: false
primary <- "#a8201a"
secondary <- "#f9c80e"
tertiary <- "#2a76dd"
fourth_colour <- "#311847"
library(epiprocess)
suppressMessages(library(tidyverse))
x <- archive_cases_dv_subset
x_latest <- epix_as_of(x, max_version = max(x$DT$version))
self_max = max(x$DT$version)
versions = seq(as.Date("2020-06-01"), self_max - 1, by = "1 month")
snapshots_all <- map_dfr(versions, function(v) { 
  epix_as_of(x, max_version = v) %>% mutate(version = v)}) %>%
  bind_rows(x_latest %>% mutate(version = self_max)) %>%
  mutate(latest = version == self_max)
snapshots <- snapshots_all %>% 
  filter(geo_value %in% c("ca", "fl"))
```

```{r}
#| include: false
#| label: cover-art
snapshots_all |>
  arrange(geo_value, version, time_value) |>
  # filter(!latest) |>
  ggplot(aes(x = time_value, y = percent_cli)) +  
  geom_line(
    aes(color = factor(version), group = interaction(geo_value, version))
  ) + 
  # geom_vline(aes(color = factor(version), xintercept = version), lty = 3, 
  #           size = 0.5) +
  scale_x_date(minor_breaks = "month", labels = NULL) +
  labs(x = "", y = "") + 
  theme_void() +
  coord_cartesian(xlim = as.Date(c("2020-10-01", NA)), ylim = c(-5, NA)) +
  scale_color_viridis_d(option = "B", end = .8) +
  theme(legend.position = "none", panel.background = element_blank()) +
  geom_line(
    data = snapshots %>% filter(latest),
    aes(x = time_value, y = percent_cli, group = geo_value), 
    inherit.aes = FALSE, color = primary)
```



::: flex
::: w-20

:::
::: w-80
## {{< meta talk-title >}} {background-image="gfx/cover-art-1.svg" background-position="bottom"}

### {{< meta talk-subtitle >}}

<br>

#### {{< meta author >}} 
{{< meta other-authors >}}

{{< meta talk-date >}}
:::
:::



```{r theme}
library(tidyverse)
theme_set(theme_bw())
```


## Outline

1. Linear Regression for Time Series Data

1. Evaluation Methods

1. ARX Models

1. Considerations for Different Horizons 

1. Overfitting and Regularization

1. Prediction Intervals

1. Forecasting with Versioned Data

1. Modeling Multiple Time Series


# Linear Regression for Time Series Data

## Basics of linear regression 

* Assume we observe a predictor $x_i$ and an outcome $y_i$ for $i = 1, \dots, n$.

* Linear regression seeks coefficients $\beta_0$ and $\beta_1$ such that


$$y_i \approx \beta_0 + \beta_1 x_i$$


is a good approximation for every $i = 1, \dots, n$.

* In R, the coefficients are found by running `lm(y ~ x)`, where `y` is the vector 
of responses and `x` the vector of predictors.


## Multiple linear regression 

* Given $p$ different predictors, we seek $(p+1)$ coefficients such that


$$y_i \approx \beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip}$$

is a good approximation for every $i = 1, \dots, n$.


## Linear regression with lagged predictor

* In time series, outcomes and predictors are usually indexed by time $t$. 

* [Goal]{.primary}: predicting future $y$, given present $x$. 

* [Model]{.primary}: linear regression with lagged predictor


$$\hat y_t = \hat \beta + \hat \beta_0 x_{t-k}$$


i.e. regress the outcome $y$ at time $t$ on the predictor $x$ at time $t-k$.

* [Equivalent]{.primary} way to write the model: 


$$\hat y_{t+k} = \hat \beta + \hat \beta_0 x_t$$



## Example: predicting COVID deaths  

* During the pandemic, interest in predicting COVID deaths 7, 14, 21, 28 days ahead.

* Can we reasonably [predict COVID deaths 28 days ahead]{.primary} by just using cases today?

* If we let


$$y_{t+28} = \text{deaths at time } t+28 \quad\quad x_{t} = \text{cases at time } t$$

  is the following a good model?


  $$\hat y_{t+28} = \hat\beta_0 + \hat\beta_1 x_{t}$$



## Example: COVID cases and deaths in California 

* Let's focus on California.

* Cases seem highly correlated with deaths several weeks later.

::: flex
::: w-50


```{r plot-ca-cases-deaths}
#| fig-width: 8
#| fig-height: 5
source(here::here("_code/ca_cases_deaths.R"))
ggplot(ca %>% 
         mutate(deaths = trans21(deaths)) %>%
         pivot_longer(cols = c(cases, deaths), names_to = 'name'),
       aes(x = time_value, y = value)) + 
  geom_line(aes(color = name)) +
  scale_color_manual(values = palette()[c(2,4)]) +
  scale_y_continuous(
    name = "Reported Covid-19 cases per 100k people", 
    limits = range1,
    sec.axis = sec_axis(
      trans = trans12, 
      name = "Reported Deaths per 100k people")) +
  labs(title = "Covid-19 cases and deaths in California", x = "Date") +
  theme(legend.position = "bottom", legend.title = element_blank()) 
```


:::
::: w-50


```{r ca data}
#| echo: true
head(ca)
```

:::
:::


## Checking correlation

* Let’s split the data into a training and a test set (before/after 2021-03-01).

* On training set: [large correlation]{.primary}
between cases and deaths 28 days ahead (> 0.95).


```{r correlation-cases-deaths}
t0_date <- as.Date('2021-03-01') #split date

train <- ca %>% filter(time_value <= t0_date)
test <- ca %>% filter(time_value > t0_date)

t0 <- nrow(train)  #split index

# look at cases and deaths (where we move deaths forward by 1, 2, ..., 35 days)
lags = 1:35
cor_deaths_cases <- lapply(lags, 
                             function(x) epi_cor(train, deaths, cases, 
                                                 cor_by = geo_value, dt1 = x))

cor_deaths_cases <- list_rbind(cor_deaths_cases, names_to = 'Lag') 

#k <- which.max(cor_deaths_cases$cor)
k <- 28 # selected lag

cor_deaths_cases %>%
  ggplot(aes(Lag, cor)) +
  geom_point() +
  geom_line() +
  labs(x = "Lag", y = "Correlation") +
  geom_vline(xintercept = k) +
  ggtitle('Correlation between cases and deaths by lag')
```



* Let's use (base) R to prepare the data and fit 


$$\hat y_{t+28} = \hat\beta + \hat\beta_0 x_{t}$$



## Preparing the data


```{r lag-cases}
#| echo: true
# Add column with cases lagged by k
ca$lagged_cases <- dplyr::lag(ca$cases, n = k)

# Split into train and test (before/after t0_date)
t0_date <- as.Date('2021-03-01')
train <- ca %>% filter(time_value <= t0_date)
test <- ca %>% filter(time_value > t0_date)
```


* Check if `deaths` is approximately linear in `lagged_cases`:


```{r plot-lag-cases}
ggplot(train, aes(lagged_cases, deaths)) + 
  geom_point(alpha = .5) +
  labs(x = "Lagged cases", y = "Deaths") + 
  ggtitle("Deaths vs cases lagged by 28 days")
```


## Fitting lagged linear regression in R


```{r lagged-lm}
#| echo: true
reg_lagged = lm(deaths ~ lagged_cases, data = train)
coef(reg_lagged)
```

```{r plot-linear-fit}
ggplot(train, aes(lagged_cases, deaths)) +
  geom_point(alpha = .5) +
  geom_abline(intercept = coef(reg_lagged)[1], slope = coef(reg_lagged)[2],
              col = 'red') +
  labs(x = "Lagged cases", y = "Deaths") +
  ggtitle("Deaths vs cases lagged by 28 days with regression line")

```


# Evaluation

## Error metrics

* Assume we have predictions $\hat y_{new, t}$ for the unseen observations 
$y_{new,t}$ over times $t = 1, \dots, N$.

* Four commonly used error metrics are:

  * mean squared error (MSE)

  * mean absolute error (MAE)

  * mean absolute percentage error (MAPE)

  * mean absolute scaled error (MASE)

## Error metrics: MSE and MAE


$$MSE = \frac{1}{N} \sum_{t=1}^N (y_{new, t}- \hat y_{new, t})^2$$

$$MAE = \frac{1}{N} \sum_{t=1}^N |y_{new, t}- \hat y_{new, t}|$$


* MAE gives less importance to extreme errors than MSE.

* [Drawback]{.primary}: both metrics are scale-dependent, so they are not universally 
interpretable.
(For example, if $y$ captures height, MSE and MAE will vary depending on whether we measure in feet or meters.)

## Error metrics: MAPE

* Fixing scale-dependence:


$$MAPE = 100 \times \frac{1}{N} \sum_{t=1}^N 
\left|\frac{y_{new, t}- \hat y_{new, t}}{y_{new, t}}\right|$$

* [Drawbacks]{.primary}:

  * Erratic behavior when $y_{new, t}$ is close to zero

  * It assumes the unit of measurement has a meaningful zero (e.g. using 
Fahrenheit or Celsius to measure temperature will lead to different MAPE)

## Comparing MAE and MAPE

::: {.callout-important icon="false"}
## Note

There are situations when MAPE is problematic!
:::

::: flex
::: w-70

```{r mae-mape-example}
#| fig-width: 10
#| fig-height: 6
set.seed(0)
x = 1:50
y = rpois(n = 50, lambda = c(rep(5, 25), 5 + exp(0:24 * 0.2)))
yhat1 = c(rep(6.5, 25), 6.5 + exp(0:24 * 0.2))
yhat2 = c(rep(5, 25), 5 + exp(0:24 * 0.18))

plot(x, y, xlab = '')
lines(x, yhat1, col = 2)
lines(x, yhat2, col = 4)
legend("topleft", c("yhat1", "yhat2"), lty = 1, col = c(2, 4))
```

:::

::: {.w-30 .align-end}

```{r mae-mape-error}
data.frame("MAE"= c(mean(abs(y - yhat1)), mean(abs(y - yhat2))), 
           "MAPE" = c(mean(abs(y - yhat1) / y), mean(abs(y - yhat2) / y)) * 100, 
           row.names = c('yhat1', 'yhat2'))
```

:::
:::


## Error metrics: MASE

$$MASE = 100 \times \frac{\frac{1}{N} \sum_{t=1}^N 

|y_{new, t}- \hat y_{new, t}|}
{\frac{1}{N-1} \sum_{t=2}^N 
|y_{new, t}- y_{new, t-1}|}$$

* [Advantages]{.primary}:

  * is universally interpretable (not scale dependent)

  * avoids the zero-pitfall

* MASE in words: we normalize the error of our forecasts by that of a naive method 
which always predicts the last observation.


## Comparing MAE, MAPE and MASE

::: flex
::: w-65


```{r mae-mape-mase-example}
#| fig-width: 10
#| fig-height: 7
#| fig-align: 'left'
plot(x, y, xlab = '')
lines(x, yhat1, col = 2)
lines(x, yhat2, col = 4)
legend("topleft", c("yhat1", "yhat2"), lty = 1, col = c(2, 4))
```

:::

::: w-35


```{r mae-mape-mase-error}
data.frame("MAE"= c(mean(abs(y - yhat1)), mean(abs(y - yhat2))), 
           "MAPE" = c(mean(abs(y - yhat1) / y), mean(abs(y - yhat2) / y)) * 100, 
           "MASE" = c(mean(abs(y - yhat1)), 
                      mean(abs(y - yhat2))) / mean(abs(diff(y))) * 100,
           row.names = c('yhat1', 'yhat2'))
```


:::
:::

## Defining the error metrics in R


```{r error functions}
#| echo: true
MSE <- function(truth, prediction) {
  mean((truth - prediction)^2)}

MAE <- function(truth, prediction) {
  mean(abs(truth - prediction))}

MAPE <- function(truth, prediction) {
  100 * mean(abs(truth - prediction) / truth)}

MASE <- function(truth, prediction) {
  100 * MAE(truth, prediction) / mean(abs(diff(truth)))}
```


## Estimating the prediction error

* Given an error metric, we want to estimate the prediction error under that metric. 

* This can be accomplished in different ways, using the

  * Training error

  * Split-sample error

  * Time series cross-validation error (using all past data or a trailing window)


## Training error

* The easiest but [worst]{.primary} approach to estimate the prediction error is 
to use the training error, i.e. the average error on the training set that was 
used to fit the model.

* The training error is

  * generally too optimistic as an estimate of prediction error

  * [more optimistic the more complex the model!]{.primary}^[More on this when we talk about overfitting.]


## Training error
#### Linear regression of COVID deaths on lagged cases


```{r pred-train}
#| echo: true
# Getting the predictions for the training set
pred_train <- predict(reg_lagged)
```

```{r plot-train-predictions}
#| fig-align: left
ca %>%
  mutate(observed = deaths, 
         predicted = c(rep(NA, k), pred_train, rep(NA, nrow(test)))) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) + 
  geom_line(aes(alpha = ifelse(time_value < t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) + 
  labs(x = "", y = "Deaths per 100k people") + 
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()

```

```{r function errors}
getErrors <- function(truth, prediction, type) {
  return(data.frame(#"MSE" = MSE(truth, prediction), 
                    "MAE"= MAE(truth, prediction), 
                    #"MAPE" = MAPE(truth, prediction), 
                    "MASE" = MASE(truth, prediction), 
                    row.names = type))
}
```

```{r training-error}
errors <- getErrors(train$deaths[-(1:k)], pred_train, "training")
errors
```



## Split-sample error {.smaller}

* To compute the split-sample error  

  1. Split data into training (up to time $t_0$), and test set (after $t_0$)

  1. Fit the model to the training data only

  1. Make predictions for the test set

  1. Compute the selected error metric on the test set only

* Formally, the split-sample MSE is


$$\text{SplitMSE} = \frac{1}{n-t_0} \sum_{t = t_0 +1}^n (\hat y_t - y_t)^2$$


* Split-sample estimates of prediction error don't mimic a situation where we 
would refit the model in the future. 
They are [pessimistic]{.primary} if the relation between outcome and predictors 
changes over time.

## Split-sample error
#### Linear regression of COVID deaths on lagged cases


```{r test-pred}
#| echo: true
# Getting the predictions for the test set
pred_test <- predict(reg_lagged, newdata = test)
```

```{r plot-test-predictions}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, predicted = c(rep(NA, k), pred_train, pred_test)) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()
```

```{r test-error}
errors <- rbind(errors, getErrors(test$deaths, pred_test, "split-sample"))
errors
```


## Time-series cross-validation (CV) {.smaller}
#### 1-step ahead predictions

* If we refit in the future once new data are available, a more 
appropriate way to estimate the prediction error is time-series cross-validation.

* To get 1-step ahead predictions (i.e. at time $t$ we forecast for $t+1$) we proceed as follows,
for $t = t_0, t_0+1, \dots$

  1. Fit the model using data up to time $t$

  1. Make a prediction for $t+1$ 

  1. Record the prediction error

The cross-validation MSE is then


$$CVMSE = \frac{1}{n-t_0} \sum_{t = t_0}^{n-1} (\hat y_{t+1|t} - y_{t+1})^2$$


where	$\hat y_{t+1|t}$ indicates a prediction for $y$ at time $t+1$ that was made 
with data available up to time $t$.

## Time-series cross-validation (CV) {.smaller}
#### $h$-step ahead predictions

* In general, if we want to make $h$-step ahead predictions (i.e. at time 
$t$ we forecast for $t+h$), we proceed as follows 
for $t = t_0, t_0+1, \dots$

  * Fit the model using data up to time $t$

  * Make a prediction for $t+h$ 

  * Record the prediction error

* The cross-validation MSE is then


$$CVMSE = \frac{1}{n-t_0} \sum_{t = t_0}^{n-h} (\hat y_{t+h|t} - y_{t+h})^2$$


where	$\hat y_{t+h|t}$ indicates a prediction for $y$ at time $t+h$ that was made 
with data available up to time $t$.

## Time-series cross-validation (CV) 
#### Linear regression of COVID deaths on lagged cases

Getting the predictions requires slightly more code:


```{r time-series-cv}
#| echo: true
n <- nrow(ca)                               #length of time series
h <- k                                      #number of days ahead for which prediction is wanted
pred_all_past <- rep(NA, length = n-h-t0+1) #initialize vector of predictions

for (t in t0:(n-h)) {
  # fit to all past data and make 1-step ahead prediction
  reg_all_past = lm(deaths ~ lagged_cases, data = ca, subset = (1:n) <= t) 
  pred_all_past[t-t0+1] = predict(reg_all_past, newdata = data.frame(ca[t+h, ]))
}

```


::: {.callout-important icon="false"}
## Note

With the current model, we can only predict $k$ days ahead (where $k$ = number of days by which predictor is lagged)!
:::


## Time-series cross-validation (CV)
#### Linear regression of COVID deaths on lagged cases


```{r plot-cv-predictions}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         `predicted (CV)` = c(rep(NA, nrow(train)+k-1), pred_all_past)) %>%
  pivot_longer(cols = c(observed, `predicted (CV)`), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) +
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()
```

```{r cv-error}
test_deaths <- test$deaths[-(1:(k-1))]
errors <- rbind(errors, getErrors(test_deaths, pred_all_past, 
                                  "time series CV"))
errors
```


## Regression on a trailing window {.smaller}

* So far, to get $h$-step ahead predictions for time $t+h$, we have fitted the 
model on all data available up to time $t$. We can instead use a trailing 
window, i.e. fit the model on a window of data of length $w$, starting at 
time $t-w$ and ending at $t$.

* [Advantage]{.primary}: if the predictors-outcome relation changes over time,
training the forecaster on a window of recent data can better capture the recent 
relation which might be more relevant to predict the outcome in the near future.

* Window length [$w$]{.primary} considerations: 

  * if $w$ is too [big]{.primary}, the model [can't adapt]{.primary} to the 
  recent predictors-outcome relation 

  * if $w$ is too [small]{.primary}, the fitted model may be [too volatile]{.primary} 
  (trained on too little data)

## Trailing window
#### Linear regression of COVID deaths on lagged cases


```{r time-series-cv-trailing}
#| echo: true
# Getting the predictions through CV with trailing window
w <- 200                                    #trailing window size
h <- k                                      #number of days ahead for which prediction is wanted
pred_trailing <- rep(NA, length = n-h-t0+1) #initialize vector of predictions

for (t in t0:(n-h)) {
  # fit to a trailing window of size w and make 1-step ahead prediction
  reg_trailing = lm(deaths ~ lagged_cases, data = ca, 
                    subset = (1:n) <= t & (1:n) > (t-w)) 
  pred_trailing[t-t0+1] = predict(reg_trailing, newdata = data.frame(ca[t+h, ]))
}
```


## Time-series CV: all past vs trailing window
#### Linear regression of COVID deaths on lagged cases


```{r plot-cv-predictions-trailing}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         `predicted (CV)` = c(rep(NA, nrow(train)+k-1), pred_all_past), 
         `predicted (trailing + CV)` = c(rep(NA, nrow(train)+k-1), pred_trailing)) %>%
  pivot_longer(cols = c(observed, `predicted (CV)`, `predicted (trailing + CV)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
    scale_alpha_identity()
```

```{r cv-trailing-error}
errors <- rbind(errors, getErrors(test_deaths, pred_trailing, 
                                  "time series CV + trailing"))
errors
```






# ARX Models

## Autoregressive (AR) model

* [Idea]{.primary}: predicting the outcome via a linear combination of (some of) its lags 


$$\hat y_{t+h} = \hat \phi + \hat\phi_0 y_{t} + \hat\phi_1 y_{t-1} + \dots + \hat\phi_p y_{t-p}$$


* [Notice]{.primary}: we don't need to include all contiguous lags^[Here we 
depart from traditional AR models, which do include all contiguous lags.].

* For example, we could fit


$$\hat y_{t+h} = \hat \phi + \hat\phi_0 y_{t} + \hat\phi_1 y_{t-7} + \hat\phi_2 y_{t-14}$$



## AR model for COVID deaths

* Let's disregard cases, and only use COVID deaths to predict deaths 28 days ahead. 

* We will fit the model: 


$$\hat y_{t+28} = \hat\phi + \hat\phi_0 y_{t}$$


* Would this be a good forecaster?

## Preparing the data and checking correlation


```{r lag-deaths}
#| echo: true
# Add column with deaths lagged by 28
ca$lagged_deaths <- dplyr::lag(ca$deaths, n = k)

# Split into train and test (before/after t0_date)
train <- ca %>% filter(time_value <= t0_date)
test <- ca %>% filter(time_value > t0_date)
```


::: flex

::: w-50


```{r ar-plot-deaths-and-lagged-cases}
#| fig-width: 8
#| fig-height: 5
ggplot(train, aes(lagged_deaths, deaths)) + 
  geom_point(alpha = .5) +
  labs(x = "Lagged deaths (by 28)", y = "Deaths")
```


:::

::: w-50


```{r auto-cor-deaths}
#| fig-width: 8
#| fig-height: 5
lags <- 1:30
auto_cor_deaths <- lapply(lags, 
                          function(x) epi_cor(train, deaths, deaths, 
                                              cor_by = geo_value, dt1 = x))

auto_cor_deaths <- list_rbind(auto_cor_deaths, names_to = 'Lag') 

auto_cor_deaths %>%
  ggplot(aes(Lag, cor)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 28) +
  labs(x = "Lag", y = "Correlation") +
  ggtitle('Auto-correlation for deaths by lag')
```


:::
:::

## Fitting the AR model for COVID deaths


```{r ar-lm}
#| echo: true
ar_fit = lm(deaths ~ lagged_deaths, data = train)
coef(ar_fit)
```


::: {.callout-important icon="false"}
## Note

The intercept is close to 0, and the coefficient is close to 1. 
This means that we are naively predicting the number of deaths in 28 days 
with (approximately) the number of deaths observed today.
:::

## Predictions on training and test sets (AR model)


```{r ar-pred}
pred_train <- predict(ar_fit)                 #get training predictions
pred_test <- predict(ar_fit, newdata = test)  #get test predictions
```

```{r ar-plot-train-predictions}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         predicted = c(rep(NA, k), pred_train, pred_test)) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r ar-train-test-error}
errors_ar <- rbind(getErrors(train$deaths[-1], pred_train, "training"),
                   getErrors(test$deaths, pred_test, "split-sample"))
errors_ar
```



## Time-Series CV: all past and trailing (AR model)


```{r ar-time-series-cv}
# Getting 28-step ahead predictions through CV 
h <- k                                                      #number of days ahead 
pred_all_past = pred_trailing <- rep(NA, length = n-h-t0+1) #initialize vectors of predictions
w <- 200                                                    #trailing window size

for (t in t0:(n-h)) {
  # fit to all past data 
  ar_all_past = lm(deaths ~ lagged_deaths, data = ca, subset = (1:n) <= t) 
  # fit to trailing window of data
  ar_trailing = lm(deaths ~ lagged_deaths, data = ca, subset = (1:n) <= t & (1:n) > (t-w)) 
  # make h-step ahead predictions
  pred_all_past[t-t0+1] = predict(ar_all_past, newdata = data.frame(ca[t+h, ]))
  pred_trailing[t-t0+1] = predict(ar_trailing, newdata = data.frame(ca[t+h, ]))
}
```

```{r ar-plot-cv-predictions}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         `predicted (CV)` = c(rep(NA, nrow(train)+k-1), pred_all_past), 
         `predicted (trailing + CV)` = c(rep(NA, nrow(train)+k-1), pred_trailing)) %>%
  pivot_longer(cols = c(observed, `predicted (CV)`, `predicted (trailing + CV)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()
```

```{r ar-cv-error}
errors_ar <- rbind(errors_ar, 
                getErrors(test_deaths, pred_all_past, "time series CV")) %>%
  rbind(getErrors(test_deaths, pred_trailing, "time series CV + trailing"))

errors_ar
```


## Autoregressive exogenous input (ARX) model

* [Idea]{.primary}: predicting the outcome via a linear combination of its lags and a set of exogenous (i.e. external) input variables

* Example:


$$\hat y_{t+h} = \hat\phi + \sum_{i=0}^p \hat\phi_i y_{t-i} + \sum_{j=0}^q \hat\beta_j x_{t-j}$$


* We can construct more complex ARX models with multiple lags of several exogenous 
variables

## ARX model for COVID deaths

* To improve our predictions for COVID deaths 28 days ahead, we could merge the two models 
considered so far.

* This leads us to the ARX model


$$\hat y_{t+28} = \hat\phi + \hat\phi_0 y_{t} + \hat\beta_0 x_{t}$$


* We can fit it on the training set by running


```{r arx-lm}
#| echo: true
arx_fit = lm(deaths ~ lagged_deaths + lagged_cases, data = train)
coef(arx_fit)
```


## Predictions on training and test sets (ARX model)


```{r arx-training-error}
pred_train <- predict(arx_fit)                  #get training predictions
pred_test <- predict(arx_fit, newdata = test)   #get test predictions
```

```{r arx-plot-train-test-predictions}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         predicted = c(rep(NA, k), pred_train, pred_test)) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r arx-train-test-errors}
errors_arx <- rbind(getErrors(train$deaths[-(1:k)], pred_train, "training"),
                    getErrors(test$deaths, pred_test, "split-sample"))
errors_arx
```


## Time-Series CV: all past and trailing (ARX model)


```{r arx-time-series-cv}
# Getting 28-step ahead predictions through CV 
h <- k                                                      #number of days ahead 
pred_all_past = pred_trailing <- rep(NA, length = n-h-t0+1) #initialize vector of predictions
w <- 200                                                    #trailing window size

for (t in t0:(n-h)) {
  # fit to all past data
  arx_all_past = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, subset = (1:n) <= t) 
  # fit to trailing window of data
  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, 
                    subset = (1:n) <= t & (1:n) > (t-w)) 
  # make h-step ahead prediction
  pred_all_past[t-t0+1] = predict(arx_all_past, newdata = data.frame(ca[t+h, ]))
  pred_trailing[t-t0+1] = predict(arx_trailing, newdata = data.frame(ca[t+h, ]))
}
```

```{r arx-plot-cv-predictions}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         `predicted (CV)` = c(rep(NA, nrow(train)+k-1), pred_all_past), 
         `predicted (trailing + CV)` = c(rep(NA, nrow(train)+k-1), pred_trailing)) %>%
  pivot_longer(cols = c(observed, `predicted (CV)`, `predicted (trailing + CV)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()
```

```{r arx-cv-error}
errors_arx <- rbind(errors_arx, 
                    getErrors(test_deaths, pred_all_past, "time series CV")) %>%
  rbind(getErrors(test_deaths, pred_trailing, "time series CV + trailing"))
errors_arx
```


# Considerations for Different Horizons 

## Changing $h$

* So far we focused on COVID deaths prediction 28 days ahead.

* The ARX model we fitted had much better performance than the AR model.

* We will next compare the AR model 


$$\hat y_{t+h} = \hat\phi + \hat\phi_0 y_t$$


to the ARX model


$$\hat y_{t+h} = \hat\phi + \hat\phi_0 y_t + \hat\beta_0 x_t$$


for horizons $h = 7, 14, 21, 28$ (using a trailing window of size 200).

## Predicting 7 days ahead


```{r ar-arx-7}
# Getting 7-step ahead predictions 
h <- 7                                                 #number of days ahead 
pred_7_ar = pred_7_arx <- rep(NA, length = n-h-t0+1)   #initialize vectors of predictions
w <- 200                                               #trailing window size

ca_7 <- ca
ca_7$lagged_deaths <- dplyr::lag(ca_7$deaths, n = h)
ca_7$lagged_cases <- dplyr::lag(ca_7$cases, n = h)

for (t in t0:(n-h)) {
  # fit to trailing window of data
  ar_trailing = lm(deaths ~ lagged_deaths, data = ca_7, 
                   subset = (1:n) <= t & (1:n) > (t-w)) 
  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca_7, 
                    subset = (1:n) <= t & (1:n) > (t-w)) 
  # make h-step ahead predictions
  pred_7_ar[t-t0+1] = predict(ar_trailing, newdata = data.frame(ca[t+h, ]))
  pred_7_arx[t-t0+1] = predict(arx_trailing, newdata = data.frame(ca[t+h, ]))
}

```

```{r ar-arx-plot-7}
#| fig-align: left
ca_7 %>% 
  mutate(observed = deaths, 
         `predicted (AR)` = c(rep(NA, nrow(train)+h-1), pred_7_ar), 
         `predicted (ARX)` = c(rep(NA, nrow(train)+h-1), pred_7_arx)) %>%
  pivot_longer(cols = c(observed, `predicted (AR)`, `predicted (ARX)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()

rbind(getErrors(test$deaths[-c(1:(h-1))], pred_7_ar, "AR"),
      getErrors(test$deaths[-c(1:(h-1))], pred_7_arx, "ARX"))
```


## Predicting 14 days ahead


```{r ar-arx-14}
# Getting 14-step ahead predictions 
h <- 14  
pred_14_ar = pred_14_arx <- rep(NA, length = n-h-t0+1) #initialize vectors of predictions
w <- 200                                               #trailing window size

ca_14 <- ca
ca_14$lagged_deaths <- dplyr::lag(ca_14$deaths, n = h)
ca_14$lagged_cases <- dplyr::lag(ca_14$cases, n = h)

for (t in t0:(n-h)) {
  # fit to trailing window of data
  ar_trailing = lm(deaths ~ lagged_deaths, data = ca_14, 
                   subset = (1:n) <= t & (1:n) > (t-w)) 
  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca_14, 
                    subset = (1:n) <= t & (1:n) > (t-w)) 
  # make h-step ahead predictions
  pred_14_ar[t-t0+1] = predict(ar_trailing, newdata = data.frame(ca[t+h, ]))
  pred_14_arx[t-t0+1] = predict(arx_trailing, newdata = data.frame(ca[t+h, ]))
}
```

```{r ar-arx-plot-14}
#| fig-align: left
ca_14 %>% 
  mutate(observed = deaths, 
         `predicted (AR)` = c(rep(NA, nrow(train)+h-1), pred_14_ar), 
         `predicted (ARX)` = c(rep(NA, nrow(train)+h-1), pred_14_arx)) %>%
  pivot_longer(cols = c(observed, `predicted (AR)`, `predicted (ARX)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()

rbind(getErrors(test$deaths[-c(1:(h-1))], pred_14_ar, "AR"),
      getErrors(test$deaths[-c(1:(h-1))], pred_14_arx, "ARX"))
```


## Predicting 21 days ahead


```{r ar-arx-21}
# Getting 21-step ahead predictions 
h <- 21  
pred_21_ar = pred_21_arx <- rep(NA, length = n-h-t0+1) #initialize vectors of predictions
w <- 200                                               #trailing window size

for (t in t0:(n-h)) {
  # fit to trailing window of data
  ar_trailing = lm(deaths ~ lagged_deaths, data = ca, 
                   subset = (1:n) <= t & (1:n) > (t-w)) 
  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, 
                    subset = (1:n) <= t & (1:n) > (t-w)) 
  # make h-step ahead predictions
  pred_21_ar[t-t0+1] = predict(ar_trailing, newdata = data.frame(ca[t+h, ]))
  pred_21_arx[t-t0+1] = predict(arx_trailing, newdata = data.frame(ca[t+h, ]))
}
```

```{r ar-arx-plot-21}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         `predicted (AR)` = c(rep(NA, nrow(train)+h-1), pred_21_ar), 
         `predicted (ARX)` = c(rep(NA, nrow(train)+h-1), pred_21_arx)) %>%
  pivot_longer(cols = c(observed, `predicted (AR)`, `predicted (ARX)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()

rbind(getErrors(test$deaths[-c(1:(h-1))], pred_21_ar, "AR"),
      getErrors(test$deaths[-c(1:(h-1))], pred_21_arx, "ARX"))
```


## Predicting 28 days ahead


```{r ar-arx-28}
# Getting 28-step ahead predictions 
h <- 28  
pred_28_ar = pred_28_arx <- rep(NA, length = n-h-t0+1) #initialize vectors of predictions
w <- 200                                               #trailing window size

for (t in t0:(n-h)) {
  # fit to trailing window of data
  ar_trailing = lm(deaths ~ lagged_deaths, data = ca, 
                   subset = (1:n) <= t & (1:n) > (t-w)) 
  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, 
                    subset = (1:n) <= t & (1:n) > (t-w)) 
  # make h-step ahead predictions
  pred_28_ar[t-t0+1] = predict(ar_trailing, newdata = data.frame(ca[t+h, ]))
  pred_28_arx[t-t0+1] = predict(arx_trailing, newdata = data.frame(ca[t+h, ]))
}
```

```{r ar-arx-plot-28}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         `predicted (AR)` = c(rep(NA, nrow(train)+h-1), pred_28_ar), 
         `predicted (ARX)` = c(rep(NA, nrow(train)+h-1), pred_28_arx)) %>%
  pivot_longer(cols = c(observed, `predicted (AR)`, `predicted (ARX)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()

rbind(getErrors(test$deaths[-c(1:(h-1))], pred_28_ar, "AR"),
      getErrors(test$deaths[-c(1:(h-1))], pred_28_arx, "ARX"))
```


## Observations

* For each horizon $h$, the ARX model has always smaller error than the AR model.

* The benefit of including cases as a predictor increases with $h$.

* The error of the AR model increases with $h$, while the error of the ARX model
decreases with $h$.

# Overfitting and Regularization

## Too many predictors

* What if we try to incorporate past information extensively by fitting a model 
with a very large number of predictors?

  * The estimated coefficients will be chosen to mimic the observed data very 
  closely on the training set, leading to [small training error]{.primary}

  * The predictive performance on the test set might be very poor, 
  producing [large split-sample and CV error]{.primary}

::: {.callout-important icon="false"}
## Issue
Overfitting!
:::

## ARX model for COVID deaths with many predictors

* When predicting COVID deaths 28 days ahead, we can try to use more past 
information by fitting a model that includes the past two months of COVID deaths 
and cases as predictors


$$\hat y_{t+28} = \hat\phi + \hat\phi_0 y_{t} + \hat\phi_1 y_{t-1} + \dots + 
\hat\phi_{59} y_{t-59} + 
\hat\beta_0 x_{t} + \dots + \hat\beta_{t-59} x_{t-59}$$

## Preparing the data

```{r overfit-data}
#| echo: true
y <- ca$deaths  #outcome
lags <- 28:87   #lags used for predictors (deaths and cases)

# Build predictor matrix with 60 columns
X <- data.frame(matrix(NA, nrow = length(y), ncol = 2*length(lags)))
colnames(X) <- paste('X', 1:ncol(X), sep = '')

for (j in 1:length(lags)) {
  # first 60 columns contain deaths lagged by 28, 29, ..., 87
  X[, j] = dplyr::lag(ca$deaths, lags[j])
  # last 60 columns contain cases lagged by 28, 29, ..., 87
  X[, length(lags) + j] = dplyr::lag(ca$cases, lags[j])
}
```

## Fitting the ARX model

```{r overfit-lm}
#| echo: true
# Train/test split
y_train <- y[1:t0]
X_train <- X[1:t0, ]
y_test <- y[(t0+1):length(y)]
X_test <- X[(t0+1):length(y), ]

# Fitting the ARX model
reg = lm(y_train ~ ., data = X_train)
coef(reg)
```

## Predictions on training and test set 

```{r overfit-pred}
#| echo: true
pred_train <- predict(reg)                    #get training predictions
pred_test <- predict(reg, newdata = X_test)   #get test predictions
```

```{r overfit-plot-train-test}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         predicted = c(rep(NA, max(lags)), pred_train, pred_test)) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())

```

```{r overfit-train-test-error}
errors_ar <- rbind(getErrors(y_train[-(1:max(lags))], pred_train, "training"),
                   getErrors(y_test, pred_test, "split-sample"))
errors_ar
```


## Regularization

* If we want to consider a large number of predictors, 
how can we avoid overfitting?

* [Idea]{.primary}: introduce a regularization parameter $\lambda$ that [shrinks or sets]{.primary} some 
of the estimated coefficients to zero, i.e. some predictors are estimated to 
have limited or no predictive power

* Most common regularization methods

  * [Ridge]{.primary}: shrinks coefficients to zero
  
  * [Lasso]{.primary}: sets some coefficients to zero

## Choosing $\lambda$

* The regularization parameter $\lambda$ can be selected by cross-validation:

  1. Select a sequence of $\lambda$'s
  
  1. Fit and predict for each such $\lambda$
  
  1. Select the $\lambda$ that leads to smaller error
  
* The R library `glmnet` implements ridge and lasso regression, 
and can perform step 1. automatically.



## Fit ARX + ridge/lasso for COVID deaths

```{r ridge-lasso-fit}
#| echo: true
library(glmnet) # Implements ridge and lasso

# We'll need to omit NA values explicitly, as otherwise glmnet will complain
na_obs <- 1:max(lags)
X_train <- X_train[-na_obs, ]
y_train <- y_train[-na_obs]

# Ridge regression: set alpha = 0, lambda sequence will be chosen automatically
ridge <- glmnet(X_train, y_train, alpha = 0)
beta_ridge <- coef(ridge)       # matrix of estimated coefficients 
lambda_ridge <- ridge$lambda    # sequence of lambdas used to fit ridge 

# Lasso regression: set alpha = 1, lambda sequence will be chosen automatically
lasso <- glmnet(X_train, y_train, alpha = 1)
beta_lasso <- coef(lasso)       # matrix of estimated coefficients 
lambda_lasso <- lasso$lambda    # sequence of lambdas used to fit lasso 

dim(beta_lasso)      # One row per coefficient, one column per lambda value
```


## Predictions on test set and selection of $\lambda$

```{r lasso-ridge-predictions}
#| echo: true
# Predict values for second half of the time series
yhat_ridge <- predict(ridge, newx = as.matrix(X_test))
yhat_lasso <- predict(lasso, newx = as.matrix(X_test))

# Compute MAE 
mae_ridge <- colMeans(abs(yhat_ridge - y_test))
mae_lasso <- colMeans(abs(yhat_lasso - y_test))

# Select index of lambda vector which gives lowest MAE
min_ridge <- which.min(mae_ridge)
min_lasso <- which.min(mae_lasso)
paste('Best MAE ridge:', round(min(mae_ridge), 3),
      '; Best MAE lasso:', round(min(mae_lasso), 3))

# Get predictions for train and test sets
pred_train_ridge <- predict(ridge, newx = as.matrix(X_train))[, min_ridge] 
pred_test_ridge <- yhat_ridge[, min_ridge]
pred_train_lasso <- predict(lasso, newx = as.matrix(X_train))[, min_lasso] 
pred_test_lasso <- yhat_lasso[, min_lasso]
```

## Estimated coefficients: shrinkage vs sparsity

::: flex
::: w-50
```{r lasso-ridge-coeff}
# Estimated coefficients
cbind('ridge' = beta_ridge[, min_ridge], 
      'lasso' = beta_lasso[, min_lasso])
```

:::

::: w-50

```{r plot-ridge-lasso-coeff}
#| out-height: "600px"
data.frame('x'= 1:nrow(beta_lasso),
           'ridge' = beta_ridge[, min_ridge], 
           'lasso' = beta_lasso[, min_lasso]) %>%
  pivot_longer(cols = c('ridge', 'lasso'), names_to = 'Method') %>%
  ggplot(aes(x, value, col = Method)) +
  geom_point(alpha = .8) +
  geom_line() +
  xlab('Regressor') + 
  ylab('Estimated coefficient')
```

:::
:::

## Predictions: ARX + ridge/lasso (train and test set)

```{r shrinkage-sparsity}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         `predicted (ridge)` = c(rep(NA, max(lags)), pred_train_ridge, 
                                 pred_test_ridge),
         `predicted (lasso)` = c(rep(NA, max(lags)), pred_train_lasso, 
                                 pred_test_lasso)) %>%
  pivot_longer(cols = c(observed, `predicted (ridge)`, 
                        `predicted (lasso)`), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r regularization-train-test-errors}
errors_glmnet <- rbind(getErrors(y_train, pred_train_ridge, 
                                "ridge training"),
                       getErrors(y_test, pred_test_ridge, "ridge split-sample"),
                       getErrors(y_train, pred_train_lasso, 
                                 "lasso training"),
                       getErrors(y_test, pred_test_lasso, "lasso split-sample"))
errors_glmnet
```

## Time-series CV for ARX + ridge/lasso (trailing)

```{r regularization-cv}
#| echo: true

h <- 28  # number of days ahead 
w <- 200 # window length

# Initialize matrices for predictions (one column per lambda value)
yhat_ridge <- matrix(NA, ncol = length(lambda_ridge), nrow = n-h-t0+1) 
yhat_lasso <- matrix(NA, ncol = length(lambda_lasso), nrow = n-h-t0+1) 

for (t in t0:(n-h)) {
  # Indices of data within window
  inds = t-w < 1:n & 1:n <= t
  # Fit ARX + ridge/lasso
  ridge_trail = glmnet(X[inds, ], y[inds], alpha = 0, lambda = lambda_ridge)
  lasso_trail = glmnet(X[inds, ], y[inds], alpha = 1, lambda = lambda_lasso)
  # Predict
  yhat_ridge[t-t0+1, ] = predict(ridge_trail, newx = as.matrix(X[(t+h), ]))
  yhat_lasso[t-t0+1, ] = predict(lasso_trail, newx = as.matrix(X[(t+h), ]))
}

# MAE values for each lambda
mae_ridge <- colMeans(abs(yhat_ridge - y_test[-c(1:(k-1))]))
mae_lasso <- colMeans(abs(yhat_lasso - y_test[-c(1:(k-1))]))

# Select lambda that minimizes MAE and save corresponding predictions
min_ridge <- which.min(mae_ridge)
min_lasso <- which.min(mae_lasso)
pred_cv_ridge <- yhat_ridge[, min_ridge]
pred_cv_lasso <- yhat_lasso[, min_lasso]

paste('Best MAE ridge:', round(min(mae_ridge), 3))
paste('Best MAE lasso:', round(min(mae_lasso), 3))
```


## Predictions: time-series CV for ARX + ridge/lasso (trailing)

```{r plot-regularization-cv}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         `predicted (ridge)` = c(rep(NA, nrow(train)+k-1), pred_cv_ridge),
         `predicted (lasso)` = c(rep(NA, nrow(train)+k-1), pred_cv_lasso)) %>%
  pivot_longer(cols = c(observed, `predicted (ridge)`, 
                        `predicted (lasso)`), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line(aes(alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()
```

```{r regularization-cv-errors}
errors_glmnet_cv <- rbind(
                       getErrors(y_test[-(1:(k-1))], pred_cv_ridge, "ridge CV + trailing"),
                       getErrors(y_test[-(1:(k-1))], pred_cv_lasso, "lasso CV + trailing"))
errors_glmnet_cv
```

# Prediction Intervals

## Point predictions vs intervals 

* So far, we have only considered [point predictions]{.primary}, i.e. 
we have fitted models 
to provide our [best guess on the outcome]{.primary} at time $t+h$. 

::: {.callout-important icon="false"}
## 
What if we want to provide a [measure of uncertainty]{.primary} around the point 
prediction or a [likely range of values]{.primary} for the outcome at time $t+h$?

:::

* For each target time $t+h$, we can construct [prediction intervals]{.primary}, i.e. provide 
ranges of values that are expected to cover the true outcome value a fixed 
fraction of times.

## Prediction intervals for `lm` fits

* To get prediction intervals for the models we previously fitted, 
we only need to tweak our call to `predict` by adding as an input: 

  `interval = "prediction", level = p`

  where $p \in (0, 1)$ is the desired coverage.

* The output from `predict` will then be a matrix with 

  * first column a [point estimate]{.primary}
  
  * second column the [lower limit]{.primary} of the interval
  
  * third column the [upper limit]{.primary} of the interval

## Prediction intervals for ARX (test)

```{r arx-intervals-test}
#| echo: true
#| output-location: column
pred_test_ci <- predict(arx_fit, 
                        newdata = test, 
                        interval = "prediction", 
                        level = 0.95)

head(pred_test_ci)
```


```{r plot-arx-intervals}
ca %>% 
  mutate(observed = deaths, 
         predicted = c(rep(NA, nrow(train)), pred_test_ci[, 1]), 
         lower = c(rep(NA, nrow(train)), pred_test_ci[, 2]), 
         upper = c(rep(NA, nrow(train)), pred_test_ci[, 3])) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3, fill = "#00BFC4") +
  geom_line(aes(col = Deaths, alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()
```

## Prediction intervals for ARX (CV, trailing window)

```{r arx-intervals-time-series-cv}
# Initialize matrices to store predictions 
# 3 columns: point estimate, lower limit, and upper limit
pred_trailing <- matrix(NA, nrow = n-h-t0+1, ncol = 3)
colnames(pred_trailing) <- c('prediction', 'lower', 'upper')

for (t in t0:(n-h)) {
  # Fit ARX 
  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, 
                    subset = (1:n) <= t & (1:n) > (t-w)) 
  # Predict
  pred_trailing[t-t0+1, ] = predict(arx_trailing, newdata = data.frame(ca[t+h, ]),
                                    interval = "prediction", level = 0.95)
}
```


```{r plot arx-intervals-cv-trailing}
lm_pred_trailing <- cbind(ca, 
                          rbind(matrix(NA, nrow = nrow(train)+k-1, ncol = 3),
                                pred_trailing))

lm_pred_trailing %>%
  mutate(observed = deaths, 
         `predicted (CV + trailing)` = prediction) %>%
  pivot_longer(cols = c(observed, `predicted (CV + trailing)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3, fill = "#00BFC4") +
  geom_line(aes(col = Deaths, alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
    scale_alpha_identity()
```


## Quantile regression

* So far we only considered different ways to apply linear regression.

* Quantile regression is a different estimation method, and it directly targets conditional 
quantiles of the outcome over time.

::: {.callout-note}
## Definition
Conditional quantile = value below which a given percentage (e.g. 25%, 50%, 
75%) of observations fall, given specific values of the predictor variables. 
:::

* [Advantage]{.primary}: it provides a more complete picture of the outcome distribution.

## ARX model for COVID deaths via quantile regression

```{r q-reg}
#| echo: true
#install.packages("quantreg")
library(quantreg)  #library to perform quantile regression

# Set quantiles of interest: we will focus on 2.5%, 50% (i.e. median), and 97.5% quantiles
quantiles <- c(0.025, 0.5, 0.975)  

# Fit quantile regression on training set
q_reg <- rq(deaths ~ lagged_deaths + lagged_cases, data = train, tau = quantiles)

# Estimated coefficients
coef(q_reg)
```


```{r q-reg-training, eval=FALSE}
# Predict on test set
pred_test <- predict(q_reg, newdata = test)

ca %>% 
  mutate(observed = deaths, 
         predicted = c(rep(NA, nrow(train)), pred_test[, 2]),
         lower = c(rep(NA, nrow(train)), pred_test[, 1]),
         upper = c(rep(NA, nrow(train)), pred_test[, 3]))%>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3, fill = "#00BFC4") +
  geom_line(aes(col = Deaths, alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_alpha_identity()
```

## Predictions via quantile regression (CV, trailing)

```{r q-reg-time-series-cv}
#| echo: true
# Initialize matrix to store predictions 
# 3 columns: lower limit, median, and upper limit
pred_trailing <- matrix(NA, nrow = n-h-t0+1, ncol = 3)
colnames(pred_trailing) <- c('lower', 'median', 'upper')

for (t in t0:(n-h)) {
  # Fit quantile regression
  rq_trailing = rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles,
                   data = ca, subset = (1:n) <= t & (1:n) > (t-w)) 
  # Predict
  pred_trailing[t-t0+1, ] = predict(rq_trailing, newdata = data.frame(ca[t+h, ]))
}
```

## Predictions via quantile regression (CV, trailing)

```{r q-reg-plot-cv-predictions-trailing}
rq_pred_trailing <- cbind(ca, 
                          rbind(matrix(NA, nrow = nrow(train)+k-1, ncol = 3),
                                pred_trailing))

rq_pred_trailing %>%
  mutate(observed = deaths, 
         `predicted (CV + trailing)` = median) %>%
  pivot_longer(cols = c(`predicted (CV + trailing)`, observed), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3, fill = "#00BFC4") +
  geom_line(aes(col = Deaths, alpha = ifelse(time_value > t0_date, 1, .5))) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank()) + 
  scale_alpha_identity()
```

## Actual Coverage

* We would expect the ARX model fitted via `lm` and via `rq` to cover the truth
about 95\% of the times. Is this actually true in practice?

* The actual coverage of each predictive interval is  

```{r lm-coverage-all-past}
data.frame("lm trailing" = mean(ca$deaths >= lm_pred_trailing$lower & 
                                  ca$deaths <= lm_pred_trailing$upper, na.rm = T), 
           "rq trailing" = mean(ca$deaths >= rq_pred_trailing$lower & 
                                  ca$deaths <= rq_pred_trailing$upper, na.rm = T), 
           row.names = 'Coverage')

```

* Notice that the coverage of `lm` is close to 95\%, while `rq` has lower coverage.

## Evaluation

* Prediction intervals are “good” if they 

  * cover the truth most of the time
  
  * are not too wide
  
* Error metric that captures both desiderata: [Weighted Interval Score (WIS)]{.primary}

* $F$ = forecast composed of predicted quantiles $q_{\tau}$ for the set 
of quantile levels $\tau$. The WIS for target variable $Y$ is represented as 
([McDonald et al., 2021](https://www.pnas.org/doi/full/10.1073/pnas.2111453118)):


$$WIS(F, Y) = 2\sum_{\tau} \phi_{\tau} (Y - q_{\tau})$$


where $\phi_{\tau}(x) = \tau |x|$ for $x \geq 0$ and 
$\phi_{\tau}(x) = (1-\tau) |x|$ for $x < 0$.

## Computing the WIS 

```{r wis-function}
#| echo: true
WIS <- function(truth, estimates, quantiles) {
  2 * sum(pmax(
    quantiles * (truth - estimates),
    (1 - quantiles) * (estimates - truth),
    na.rm = TRUE
  ))
}
```

::: {.callout-important icon="false"}
## Note
WIS tends to [prioritize sharpness]{.primary} (how wide the interval is) relative to 
coverage (if the interval contains the truth).
:::

## WIS for ARX fitted via `lm` and `rq`

* The lowest mean WIS is attained by quantile regression. 

* Notice: this method has coverage below 95\% but is still preferred under WIS 
because its intervals are narrower than for linear regression.


```{r wis-lm}
lm_pred_trailing$method <- 'lm.trailing'
rq_pred_trailing$method <- 'rq.trailing'

lm_pred_trailing %>% 
  rowwise() %>%
  mutate(wis = WIS(deaths, c(lower, prediction, upper), quantiles)) %>%
  ungroup() %>%
  summarise(`Mean WIS lm` = mean(wis, na.rm = T)) %>%
  cbind(rq_pred_trailing %>% 
    rowwise() %>%
    mutate(wis = WIS(deaths, c(lower, median, upper), quantiles)) %>%
    ungroup() %>%
    summarise(`Mean WIS rq` = mean(wis, na.rm = T)))

```


# Forecasting with Versioned Data

## Versioned data

* In our forecasting examples, we have assumed the data are never revised 
(or have simply ignored revisions, and used data `as_of` today)

::: {.callout-important icon="false"}
## 
How can we train forecasters when dealing with versioned data?
:::

```{r get-versioned-data}
source(here::here("_code/versioned_data.R"))
```

```{r versioned-data}
#| echo: true
data_archive
```

## Version-aware forecasting

```{r versioned-weekly-avg}
fc_time_values <- seq(
  from = t0_date,
  to = as.Date("2021-12-31"),
  by = "1 month"
)

data_archive <- data_archive %>%
  epix_slide(
    .before = Inf, 
    .versions = fc_time_values,
    function(x, gk, rtv) {
      x %>%
        group_by(geo_value) %>%
        epi_slide_mean(case_rate, .window_size = 7L) %>%
        epi_slide_mean(death_rate, .window_size = 7L) %>%
        ungroup() %>%
        rename(case_rate_7d_av = slide_value_case_rate,
               death_rate_7d_av = slide_value_death_rate)
    }
  ) %>%
  rename(
    cases = case_rate_7d_av,
    deaths = death_rate_7d_av
  ) %>%
  select(version, time_value, geo_value, cases, deaths) %>%
  as_epi_archive(compactify = TRUE)

ca_archive <- data_archive$DT %>% 
  filter(geo_value == "ca") %>%
  as_epi_archive()
```


```{r versioned-quantile-reg}
#| echo: true
# initialize dataframe for predictions
# 5 columns: forecast date, target date, 2.5%, 50%, and 97.5% quantiles
pred_trailing <- data.frame(matrix(NA, ncol = 5, nrow = 0))
colnames(pred_trailing) <- c("forecast_date", "target_date", 'tau..0.025', 'tau..0.500', 'tau..0.975')

w <- 200         #trailing window size
h <- 28          #number of days ahead

# dates when predictions are made (set to be 1 month apart)
fc_time_values <- seq(from = t0_date, to = as.Date("2021-12-31"), by = "1 month")

for (fc_date in fc_time_values) {
  # get data version as_of forecast date
  data <- epix_as_of(ca_archive, max_version = as.Date(fc_date))
  # create lagged predictors
  data$lagged_deaths <- dplyr::lag(data$deaths, h) 
                                                  
  data$lagged_cases <- dplyr::lag(data$cases, h)
  # perform quantile regression
  rq_trailing <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, 
                    # only consider window of data
                    data = data %>% filter(time_value > (max(time_value) - w))) 
  # construct data.frame with the right predictors for the target date
  predictors <- data.frame(lagged_deaths = tail(data$deaths, 1), 
                           lagged_cases = tail(data$cases, 1))
  # make predictions for target date and add them to matrix of predictions
  pred_trailing <- rbind(pred_trailing, 
                         data.frame('forecast_date' = max(data$time_value),
                                    'target_date' = max(data$time_value) + h, 
                                    predict(rq_trailing, newdata = predictors)))
}
```


## Version-aware predictions (CV, trailing)

```{r clean-data-output}
# clean output and join it with finalized values (`ca` dataset)
pred_trailing <- pred_trailing %>%
  rename(median = `tau..0.500`,
         lower = `tau..0.025`,
         upper = `tau..0.975`) %>%
  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%
  arrange(target_date)
```


```{r plot-versioned-cv-trailing}
pred_trailing %>%
  ggplot(aes(x = target_date)) +
  geom_line(aes(y = deaths), col = 'gray50') + 
  geom_point(aes(y = median), col = 'blue') +
  geom_linerange(aes(ymin = lower, ymax = upper), col = 'blue', alpha = .6) +
  geom_vline(xintercept = fc_time_values, lty = 2, col = 'gray50') +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

# Modeling Multiple Time Series

## Using geo information

* Assume we observe data over time from [multiple locations]{.primary}
(e.g. states or counties).

* We could

  * Estimate coefficients [separately]{.primary} for each location (as we have done so far).
  
  * Fit one model using all locations together at each time point ([geo-pooling]{.primary}). 
Estimated coefficients will not be location specific.

  * Estimate coefficients separately for each location, but include predictors capturing 
averages across locations ([partial geo-pooling]{.primary}).



## Geo-pooling (trailing window)

```{r geo-pooling}
#| echo: true
usa_archive <- data_archive$DT %>% 
  as_epi_archive()

# initialize dataframe for predictions
# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles
pred_trailing <- data.frame(matrix(NA, ncol = 6, nrow = 0))
colnames(pred_trailing) <- c('geo_value', 'forecast_date', 'target_date',
                             'tau..0.025', 'tau..0.500', 'tau..0.975')

w <- 200         #trailing window size
h <- 28          #number of days ahead

for (fc_date in fc_time_values) {
  # get data version as_of forecast date
  data <- epix_as_of(usa_archive, max_version = as.Date(fc_date))
  
  # create lagged predictors for each state 
  data <- data %>%
    arrange(geo_value, time_value) %>%  
    group_by(geo_value) %>%
    mutate(lagged_deaths = dplyr::lag(deaths, h),
           lagged_cases = dplyr::lag(cases, h)) %>%
    ungroup()
  
  # perform quantile regression
  rq_trailing <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, 
                    # only consider window of data
                    data = data %>% filter(time_value > (max(time_value) - w))) 
  
  # construct dataframe with the right predictors for the target date
  new_lagged_deaths <- data %>% 
    filter(time_value == max(time_value)) %>%
    select(geo_value, deaths)
  
  new_lagged_cases <- data %>% 
    filter(time_value == max(time_value)) %>%
    select(geo_value, cases)
  
  predictors <- new_lagged_deaths %>%
    inner_join(new_lagged_cases, join_by(geo_value)) %>%
    rename(lagged_deaths = deaths,
           lagged_cases = cases)
  
  # make predictions for target date and add them to matrix of predictions
  pred_trailing <- rbind(pred_trailing, 
                         data.frame(
                           'geo_value' = predictors$geo_value,
                           'forecast_date' = max(data$time_value),
                           'target_date' = max(data$time_value) + h, 
                           predict(rq_trailing, newdata = predictors)))
}

# geo-pooled predictions for California
pred_ca <- pred_trailing %>%
  filter(geo_value == 'ca') %>%
  rename(median = `tau..0.500`,
         lower = `tau..0.025`,
         upper = `tau..0.975`) %>%
  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%
  arrange(target_date)

```

## Geo-pooled predictions for California

```{r plot-geo-pooling}
pred_ca %>%
  ggplot(aes(x = target_date)) +
  geom_line(aes(y = deaths), col = 'gray50') + 
  geom_point(aes(y = median), col = 'purple') +
  geom_linerange(aes(ymin = lower, ymax = upper), col = 'purple', alpha = .6) +
  geom_vline(aes(xintercept = forecast_date), lty = 2, col = 'gray50') +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Partial geo-pooling (trailing window)

```{r partial-geo-pooling}
#| echo: true

# initialize dataframe for predictions
# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles
pred_trailing <- data.frame(matrix(NA, ncol = 6, nrow = 0))
colnames(pred_trailing) <- c('geo_value', 'forecast_date', 'target_date',
                             'tau..0.025', 'tau..0.500', 'tau..0.975')

w <- 200         #trailing window size
h <- 28          #number of days ahead

for (fc_date in fc_time_values) {
  # get data version as_of forecast date
  data <- epix_as_of(usa_archive, max_version = as.Date(fc_date))
  
  # create lagged predictors 
  data <- data %>%
    arrange(geo_value, time_value) %>%  
    group_by(geo_value) %>%
    mutate(lagged_deaths = dplyr::lag(deaths, h),
           lagged_cases = dplyr::lag(cases, h)) %>%
    ungroup() %>%
    group_by(time_value) %>%
    mutate(avg_lagged_deaths = mean(lagged_deaths, na.rm = T),
           avg_lagged_cases = mean(lagged_cases, na.rm = T)) %>%
    ungroup() 
  
  # perform quantile regression
  rq_trailing <- rq(deaths ~ lagged_deaths + lagged_cases + avg_lagged_deaths +
                      avg_lagged_cases, tau = quantiles, 
                    data = (data %>% filter(geo_value == 'ca'))) 
  
  # construct data.frame with the right predictors for the target date
  new_lagged_deaths <- data %>% 
    filter(time_value == max(time_value)) %>%
    select(geo_value, deaths) %>%
    mutate(avg_lagged_deaths = mean(deaths, na.rm = T)) %>%
    filter(geo_value == 'ca')
  
  new_lagged_cases <- data %>% 
    filter(time_value == max(time_value)) %>%
    select(geo_value, cases) %>%
    mutate(avg_lagged_cases = mean(cases, na.rm = T)) %>%
    filter(geo_value == 'ca')
  
  predictors <- new_lagged_deaths %>%
    inner_join(new_lagged_cases, join_by(geo_value)) %>%
    rename(lagged_deaths = deaths,
           lagged_cases = cases)
  
  # make predictions for target date and add them to matrix of predictions
  pred_trailing <- rbind(pred_trailing, 
                         data.frame(
                           'geo_value' = predictors$geo_value,
                           'forecast_date' = max(data$time_value),
                           'target_date' = max(data$time_value) + h, 
                           predict(rq_trailing, newdata = predictors)))
}

# partially geo-pooled predictions for California
pred_ca <- pred_trailing %>%
  filter(geo_value == 'ca') %>%
  rename(median = `tau..0.500`,
         lower = `tau..0.025`,
         upper = `tau..0.975`) %>%
  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%
  arrange(target_date)

```

## Partially geo-pooled predictions for California

```{r plot-partial-geo-pooling}
pred_ca %>%
  ggplot(aes(x = target_date)) +
  geom_line(aes(y = deaths), col = 'gray50') + 
  geom_point(aes(y = median), col = 'darkgreen') +
  geom_linerange(aes(ymin = lower, ymax = upper), col = 'darkgreen', alpha = .6) +
  geom_vline(aes(xintercept = forecast_date), lty = 2, col = 'gray50') +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```


## Final slide {.smaller}

### Thanks:

```{r qr-codes}
#| include: false
#| fig-format: png
# Code to generate QR codes to link to any external sources
qrdat <- function(text, ecl = c("L", "M", "Q", "H")) {
  x <- qrcode::qr_code(text, ecl)
  n <- nrow(x)
  s <- seq_len(n)
  tib <- tidyr::expand_grid(x = s, y = rev(s))
  tib$z <- c(x)
  tib
}
qr1 <- qrdat("https://cmu-delphi.github.io/epiprocess/")
qr2 <- qrdat("https://cmu-delphi.github.io/epipredict/")
ggplot(qr1, aes(x, y, fill = z)) +
  geom_raster() +
  ggtitle("{epiprocess}") +
  coord_equal(expand = FALSE) +
  scale_fill_manual(values = c("white", "black"), guide = "none") +
  theme_void(base_size = 18) +
  theme(plot.title = element_text(hjust = .5))
ggplot(qr2, aes(x, y, fill = z)) +
  geom_raster() +
  labs(title = "{epipredict}") +
  coord_equal(expand = FALSE) +
  scale_fill_manual(values = c("white", "black"), guide = "none") +
  theme_void(base_size = 18) +
  theme(plot.title = element_text(hjust = .5))
```

- The whole [CMU Delphi Team](https://delphi.cmu.edu/about/team/) (across many institutions)
- Optum/UnitedHealthcare, Change Healthcare.
- Google, Facebook, Amazon Web Services.
- Quidel, SafeGraph, Qualtrics.
- Centers for Disease Control and Prevention.
- Council of State and Territorial Epidemiologists


::: {layout-row=1 fig-align="center"}
![](gfx/delphi.jpg){height="100px"}
![](gfx/berkeley.jpg){height="100px"}
![](gfx/cmu.jpg){height="100px"}
![](gfx/ubc.jpg){width="250px"}
![](gfx/stanford.jpg){width="250px"}
:::



