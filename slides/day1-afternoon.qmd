---
talk-title: "Data Cleaning, Versioning, Nowcasting With `{epiprocess}`"
talk-short-title: "Nowcasting"
talk-subtitle: "InsightNet Forecasting Workshop 2024"
talk-date: "11 December -- Afternoon"
format: revealjs
---
  
{{< include _titleslide.qmd >}}

```{r theme-load-pkg}
#| cache: false
library(tidyverse)
library(epidatr)
library(epipredict)
library(epidatasets)
theme_set(theme_bw())
```

## Outline

1. Essentials of `{dplyr}` and `{tidyr}` 

1. Epiverse Software ecosystem

1. Panel and Versioned Data in the epiverse

1. Basic Nowcasting using `{epiprocess}`

1. Nowcasting with One Variable

1. Nowcasting with Two Variables

1. Case Study - Nowcasting Cases Using %CLI


# Essentials of `{dplyr}` and `{tidyr}` 

## Down with spreadsheets for data manipulation

* Spreadsheets make it difficult to rerun analyses consistently.
* Using R (and `{dplyr}`) allows for:
  * Reproducibility 
  * Ease of modification
* [**Recommendation**]{.primary}: Avoid manual edits; instead, use code for transformations.
* Let's see what we mean by this...

## Introduction to `dplyr`

* `dplyr` is a powerful package in R for data manipulation.
* It is part of the `tidyverse`, which includes a collection of packages designed to work together... Here's some of its greatest hits:

<div style="text-align: center;">
![](gfx/tidyverse_packages.png){style="width: 30%; display: block; margin-left: auto; margin-right: auto;"}
<br>
<small>[Source](https://laddem.github.io/courses/tidyverse/)</small>
</div>

## Introduction to `dplyr`

To load `dplyr`, you may simply load the `tidyverse` package:

```{r load-tidyverse}
#| echo: true
# install.packages("tidyverse")
library(tidyverse)  
```

Our focus will be on basic operations like selecting and filtering data.

![](gfx/dplyr_and_fun.png){style="width: 60%;"}
<div style="text-align: center;">
<small>[Source](https://towardsdatascience.com/data-manipulation-in-r-with-dplyr-3095e0867f75)</small>
</div>


## Downloading JHU CSSE COVID-19 case data

* Let's start with something familiar... Here's a task for you:
* Use `pub_covidcast()` to download [**JHU CSSE COVID-19 confirmed case data**]{.primary} (`confirmed_incidence_num`) for CA, NC, and NY from March 1, 2022 to March 31, 2022 as of January 1, 2024.
* Try this for yourself. Then click the dropdown on the next slide to check your work...

## Downloading JHU CSSE COVID-19 case data

```{r fetch-jhu-dplyr-demo-data}
#| echo: true
#| code-fold: true
library(epidatr)

cases_df_api <- pub_covidcast(
  source = "jhu-csse",
  signals = "confirmed_incidence_num",
  geo_type = "state",
  time_type = "day",
  geo_values = "ca,nc,ny",
  time_values = epirange(20220301, 20220331),
  as_of = as.Date("2024-01-01")
)
```

Now we only really need a few columns here...
```{r head-jhu-dplyr-demo-data}
#| echo: true
cases_df <- cases_df_api |>
  select(geo_value, time_value, raw_cases = value) # We'll talk more about this soon :)
```

## Ways to inspect the dataset

Use `head()` to view the first six row of the data 
```{r head}
#| echo: true
head(cases_df)  # First 6 rows
```

and tail to view the last six

```{r tail}
tail(cases_df)  # Last 6 rows
```

## Ways to inspect the dataset
Now, for our first foray into the `tidyverse`...

Use `glimpse()` to get a compact overview of the dataset.

```{r glimpse}
#| echo: true
glimpse(cases_df)
```

<!-- ## Creating tibbles

* [**Tibbles**]{.primary}: Modern data frames with enhanced features.
* Rows represent [**observations**]{.primary} (or cases).
* Columns represent [**variables**]{.primary} (or features).
* You can create tibbles manually using the `tibble()` function.

```{r create-tibble}
#| echo: true
tibble(x = letters, y = 1:26)
```
-->

## Selecting columns with `select()`

The `select()` function is used to pick specific columns from your dataset.

```{r select-columns}
#| echo: true
select(cases_df, geo_value, time_value)  # Select the 'geo_value' and 'time_value' columns
```

## Selecting columns with `select()`

You can exclude columns by prefixing the column names with a minus sign `-`.

```{r select-columns-exclude}
#| echo: true
select(cases_df, -raw_cases)  # Exclude the 'raw_cases' column from the dataset
```

<!-- ## Extracting columns with `pull()`

* `pull()`: Extract a column as a vector.
* Let's try this with the `cases` column...

```{r pull-column-direct}
#| echo: true
pull(cases_df, raw_cases) 
```
-->

## Subsetting rows with `filter()`

<div style="text-align: center;">
![](gfx/dplyr_filter.png){style="width: 65%; display: block; margin-left: auto; margin-right: auto;"}
<br>
<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>
</div>

## Subsetting rows with `filter()`

* The `filter()` function allows you to subset rows that meet specific conditions.
* Conditions regard column values, such as filtering for only NC or cases higher than some threshold.
* This enables you to narrow down your dataset to focus on relevant data.

```{r filter-rows}
#| echo: true
filter(cases_df, geo_value == "nc", raw_cases > 500)  # Filter for NC with raw daily cases > 500
```

## Combining `select()` and `filter()` functions

* You can further combine `select()` and `filter()` to further refine the dataset.
* Use `select()` to choose columns and `filter()` to narrow down rows.
* This helps in extracting the exact data needed for analysis.

```{r select-filter-combine}
#| echo: true
select(filter(cases_df, geo_value == "nc", raw_cases > 1000), time_value, raw_cases) |> 
  head()
```

## Using the pipe operator `|>`

* The pipe operator (`|>`) makes code more readable by chaining multiple operations together.
* The output of one function is automatically passed to the next function.
* This allows you to perform multiple steps (e.g., `filter()` followed by `select()`) in a clear and concise manner.

```{r pipe-operator}
#| echo: true
# This code reads more like poetry!
cases_df |> 
  filter(geo_value == "nc", raw_cases > 1000) |> 
  select(time_value, raw_cases) |> 
  head()
```

## Key practices in `dplyr`

* Use [**tibbles**]{.primary} for easier data handling.
* Use `head()`, `tail()`, and `glimpse()` for quick data inspection.
* Use `select()` and `filter()` for data manipulation.
* Chain functions with `|>` for cleaner code.

<!-- * Use `pull()` to extract columns as vectors. --> 


## Grouping data with `group_by()`

* Use `group_by()` to group data by one or more columns.
* Allows performing operations on specific groups of data.

```{r group-by-ex}
#| echo: true
cases_df |>
  group_by(geo_value) |>
  filter(raw_cases == max(raw_cases, na.rm = TRUE))
```

## Creating new columns with `mutate()`

<div style="text-align: center;">
![](gfx/dplyr_mutate.jpg){style="width: 45%; display: block; margin-left: auto; margin-right: auto;"}
<br>
<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>
</div>

## Creating new columns with `mutate()`

* `mutate()` is used to create new columns.
* Perform calculations using existing columns and assign to new columns.

```{r mutate-one-var-ex}
#| echo: true
ny_subset = cases_df |>
  filter(geo_value == "ny")

ny_subset |> 
  mutate(cumulative_cases = cumsum(raw_cases)) |> 
  head()
```

<!-- ## Creating new columns with `mutate()`

* `mutate()` can create multiple new columns in one step.
* Logical comparisons (e.g., `over_5000 = raw_cases > 5000`) can be used within `mutate()`.

```{r mutate-two-var-ex}
#| echo: true
ny_subset |> 
  mutate(over_5000 = raw_cases > 5000,
         cumulative_cases = cumsum(raw_cases)) |> 
  head()
```
-->

## Combining `group_by()` and `mutate()`

* First, group data using `group_by()`.
* Then, use `mutate` to perform the calculations for each group.
* Finally, use `arrange` to display the output by `geo_value`.

```{r group-by-mutate-combo}
#| echo: true
cases_df |>
  group_by(geo_value) |>
  mutate(cumulative_cases = cumsum(raw_cases)) |> 
  arrange(geo_value) |> 
  head()
```

<!-- ## Conditional calculations with `if_else()`
* `if_else()` allows conditional logic within `mutate()`.
* Perform different operations depending on conditions, like "high" or "low."

```{r cond-calc-if-else}
#| echo: true
t <- 5000

cases_df |>
  mutate(high_low_cases = if_else(raw_cases > t, "high", "low")) |> 
  head()
```
-->

## Summarizing data with `summarise()`
* `summarise()` reduces data to summary statistics (e.g., mean, median).
* Typically used after `group_by()` to summarize each group.

```{r summarise-median-one-var}
#| echo: true
cases_df |>
  group_by(geo_value) |>
  summarise(median_cases = median(raw_cases))
```

## Using `count()` to aggregate data
`count()` is a shortcut for grouping and summarizing the data.

For example, if we want to get the total number of complete rows for each state, then
```{r summarise-count}
#| echo: true
cases_count <- cases_df |>
  drop_na() |> # Removes rows where any value is missing (from tidyr)
  group_by(geo_value) |>
  summarize(count = n())
```
<br>
is equivalent to 

```{r count-fun}
#| echo: true
cases_count <- cases_df |>
  drop_na() |> 
  count(geo_value)

cases_count # Let's see what the counts are.
```

## Key practices in `dplyr`: Round 2

* Use `group_by()` to group data by one or more variables before applying functions.
* Use `mutate` to create new columns or modify existing ones by applying functions to existing data.
* Use `summarise` to reduce data to summary statistics (e.g., mean, median).
* `count()` is a convenient shortcut for counting rows by group without needing `group_by()` and `summarise()`.

## Tidy data and Tolstoy

> "Happy families are all alike; every unhappy family is unhappy in its own way." — Leo Tolstoy  

* [**Tidy datasets**]{.primary} are like happy families: consistent, standardized, and easy to work with.  
* [**Messy datasets**]{.primary} are like unhappy families: each one messy in its own unique way.  
In this section:
* We'll define what makes data *tidy* and how to transform between the tidy and messy formats.

## Tidy data and Tolstoy

![](gfx/tidy_messy_data.jpg){style="width: 60%;"}

<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>


## What is tidy data?

* Tidy data follows a consistent structure: [**each row represents one observation, and each column represents one variable.**]{.primary}
* `cases_df` is one classic example of tidy data.

```{r head-tidy-ex}
head(cases_df)
```

* To convert between tidy and messy data, we can use the `tidyr` package in the tidyverse.

## `pivot_wider()` and  `pivot_longer()`
<div style="text-align: center;">
![](gfx/pivot_wider_longer.jpg){style="width: 40%; display: block; margin-left: auto; margin-right: auto;"}
<br>
<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>
</div>

## Making data wider with `pivot_wider()`
* To convert data from long format to wide/messy format use `pivot_wider()`.
* For example, let's try creating a column for each time value in `cases_df`:

<!-- Example. Spreadsheet from hell -->

```{r pivot-wider-ex}
#| echo: true
messy_cases_df <- cases_df |>
  pivot_wider(
    names_from = time_value,   # Create new columns for each unique date
    values_from = raw_cases    # Fill those columns with the raw_case values
  )

# View the result
messy_cases_df
```

##  Tidying messy data with `pivot_longer()`
* Use `pivot_longer()` to convert data from [**wide format**]{.primary} (multiple columns for the same variable) to [**long format**]{.primary} (one column per variable).
* Let's try turning `messy_cases_df` back into the original tidy `cases_df`!

```{r pivot-longer-ex}
#| echo: true
tidy_cases_df <- messy_cases_df |>
  pivot_longer(
    cols = -geo_value,          # Keep the 'geo_value' column as it is
    names_to = "time_value",    # Create a new 'time_value' column from the column names
    values_to = "raw_cases"     # Values from the wide columns should go into 'raw_cases'
  )

# View the result
head(tidy_cases_df, n = 3) # Notice the class of time_value here
```

##  Tidying messy data with `pivot_longer()`

* When we used `pivot_longer()`, the `time_value` column is converted to a character class because the column names are treated as strings.
* So, to truly get the original `cases_df` we need to convert `time_value` back to the `Date` class.
* Then, we can use `identical()` to check if the two data frames are exactly the same.
```{r check-identical}
#| echo: true
tidy_cases_df = tidy_cases_df |> mutate(time_value = as.Date(time_value))

identical(tidy_cases_df |> arrange(time_value), cases_df)
```

Great. That was a success!

## Introduction to joins in `dplyr`
* Joining datasets is a powerful tool for combining info. from multiple sources.
* In R, `dplyr` provides several functions to perform different types of joins.
* We'll demonstrate joining a subset of `cases_df` (our case counts dataset) with `state_census`.
* [**Motivation**]{.primary}: We can scale the case counts by population to make them comparable across regions of different sizes.

## Subset `cases_df`

To simplify things, let's use `filter()` to only grab one date of `cases_df`:
```{r cases-df-one-date}
#| echo: true
cases_df_sub <- cases_df |> filter(time_value == "2022-03-01")
cases_df_sub
```

Though note that what we're going to do can be applied to the entirety of `cases_df`.

## Load state census data

The `state_census` dataset from `epidatasets` contains state populations from the 2019 census.
```{r state-census}
#| echo: true
# State census dataset from epidatasets
library(epidatasets)
state_census = state_census |> select(abbr, pop) |> filter(abbr != "us")

state_census |> head()
```

Notice that this includes many states that are not in `cases_df_sub`.

## Left Join: Keep all rows from the first dataset

* A [**left join**]{.primary} keeps all rows from the [**first dataset**]{.primary} (`cases_df_sub`), and adds matching data from the second dataset (`state_census`).
* So [**all rows from the first dataset**]{.primary} (`cases_df_sub`) will be preserved.
* The datasets are joined by matching the `geo_value` column, specified by the by argument.

```{r left-join}
#| echo: true
# Left join: combining March 1, 2022 state case data with the census data
cases_sub_left_join <- cases_df_sub |>
  left_join(state_census, join_by(geo_value == abbr))

cases_sub_left_join
```

## Right Join: Keep all rows from the second dataset
* A [**right join**]{.primary} keeps all rows from the [**second dataset**]{.primary} (`state_census`), and adds matching data from the first dataset (`cases_df_sub`).
* If a row in the second dataset doesn't have a match in the first, then the columns from the first will be filled with NA. 
* For example, can see this for the `al` row from `state_census`...

```{r right-join}
#| echo: true
# Right join: keep all rows from state_census
cases_sub_right_join <- cases_df_sub |>
  right_join(state_census, join_by(geo_value == abbr))

head(cases_sub_right_join)
```

## Inner Join: Only keep matching rows
* An [**inner join**]{.primary} will only keep rows where there is a match in both datasets.
* So, if a state in `state_census` does not have a corresponding entry in `cases_df_sub`, then that row will be excluded.
```{r inner-join}
#| echo: true
# Inner join: only matching rows are kept
cases_sub_inner_join <- cases_df_sub |>
  inner_join(state_census, join_by(geo_value == abbr))

cases_sub_inner_join
```

## Full Join: Keep all rows from both datasets

* A [**full join**]{.primary} will keep all rows from both datasets.
* If a state in either dataset has no match in the other, the missing values will be filled with NA.
```{r full-join}
#| echo: true
# Full join: keep all rows from both datasets
cases_sub_full_join <- cases_df_sub |>
  full_join(state_census, join_by(geo_value == abbr))

head(cases_sub_full_join)
```

## Pictorial summary of the four join functions

<!--* **Left join:** All rows from the left dataset and matching rows from the right dataset.
* **Right join:** All rows from the right dataset and matching rows from the left dataset.
* **Inner join:** Only matching rows from both datasets.
* **Full join:** All rows from both datasets, with NA where no match exists.-->

![](gfx/join_funs_cheatsheet.png){style="width: 40%; display: block; margin-left: auto; margin-right: auto;"}
<div style="text-align: center;">
<small>[Source](https://ohi-science.org/data-science-training/dplyr.html)</small>
</div>

<!-- ## Final thoughts on joins
* Joins are an essential part of data wrangling in R.
* The choice of join depends on the analysis you need to perform:
    + Use [**left joins**]{.primary} when you want to keep all data from the first dataset.
    + Use [**right joins**]{.primary} when you want to keep all data from the second dataset.
    + Use [**inner joins**]{.primary} when you're only interested in matching rows.
    + Use [**full joins**]{.primary} when you want to preserve all information from both datasets.
-->

## Two review questions

**Q1)**: What join function should you use if your goal is to scale the cases by population in `cases_df`?
```{r question-2}
#| echo: true
#| code-fold: true
#| results: hide
# Either left_join
cases_left_join <- cases_df |>
  left_join(state_census, join_by(geo_value == abbr))

cases_left_join

# Or inner_join
cases_inner_join <- cases_df |>
  inner_join(state_census, join_by(geo_value == abbr))

cases_inner_join
```

**Q2)**: Lastly, please create a new column in `cases_df` where you scale the cases by population and multiply by `1e5` to get cases / 100k.
```{r question-3}
#| echo: true
#| code-fold: true
#| results: hide
case_rates_df <- cases_inner_join |>
  mutate(scaled_cases = raw_cases / pop * 1e5) # cases / 100K
head(case_rates_df)
```

Congratulations for making it through this crash course! That's all for this `glimpse()` into the tidyverse.

## Epi. data processing with `epiprocess`

* `epiprocess` is a package that offers additional functionality to pre-process such epidemiological data.
* You can work with an `epi_df` like you can with a tibble by using dplyr verbs.
* For example, on `cases_df`, we can easily use `epi_slide_mean()` to calculate trailing 14 day averages of cases:

```{r trailing-average-ex}
#| echo: true
case_rates_df <- case_rates_df |>
  as_epi_df(as_of = as.Date("2024-01-01")) |>
  group_by(geo_value) |>
  epi_slide_mean(scaled_cases, .window_size = 14, na.rm = TRUE) |>
  rename(smoothed_scaled_cases = scaled_cases_14dav)
head(case_rates_df)
```

## Epi. data processing with `epiprocess`
It is easy to produce an autoplot the smoothed confirmed daily cases for each `geo_value`:
```{r autoplot-ex}
#| echo: true
case_rates_df |>
  autoplot(smoothed_scaled_cases)
```

## Epi. data processing with `epiprocess`

Alternatively, we can display both the smoothed and the original daily case rates:

```{r smoothed-original-plot}
#| echo: false
ggplot(case_rates_df) +
  geom_line(aes(x = time_value, y = scaled_cases, color = geo_value), size = 0.25) +
  geom_line(aes(x = time_value, y = smoothed_scaled_cases, color = geo_value), size = 1) +
  facet_wrap(vars(geo_value), nrow = 1, scales = "free") +
  ylab("Cases per 100k") +
  theme_bw() +
  theme(legend.position = "none") +
  guides(x =  guide_axis(angle = 25))
```
Now, before exploring some more features of `epiprocess`, let's have a look at the epiverse software ecosystem it's part of...

# Epiverse Software Ecosystem

## The epiverse ecosystem
Interworking, community-driven, packages for epi tracking & forecasting.

![](gfx/epiverse_packages_flow.jpg){style="width: 60%; display: block; margin-left: auto; margin-right: auto;"}

<!-- 1. Fetch data: epidatr, epidatpy, and other sources, 2. Explore, clean, transform & backtest 3. Pre-built forecasters, modular forecasting framework: epipredict -->
  
  
  
# Panel and Versioned Data in the Epiverse
  
## What is panel data?

* Recall that [panel data](https://en.wikipedia.org/wiki/Panel_data), or longitudinal data, 
contain cross-sectional measurements of subjects over time. 
* Built-in example: [`covid_case_death_rates`](
  https://cmu-delphi.github.io/epidatasets/reference/covid_case_death_rates.html) 
dataset, which is a snapshot [**as of**]{.primary} May 31, 2022 that contains daily state-wise measures of `case_rate` and `death_rate` for COVID-19 over 2021:
  
```{r head-edf}
#| echo: false
edf <- covid_case_death_rates
# Only consider the 50 US states (no territories)
edf <- edf |> filter(geo_value %in% tolower(state.abb)) 
head(edf |> as_tibble())
```

* How do we store & work with such snapshots in the epiverse software ecosystem?

  
  
## `epi_df`: Snapshot of a dataset

* You can convert panel data into an `epi_df` with the required `geo_value` and `time_value` columns

Therefore, an `epi_df` is...

* a tibble that requires columns `geo_value` and `time_value`.

* arbitrary additional columns containing [measured values]{.primary}

* additional [keys]{.primary} to index (`age_group`, `ethnicity`, etc.)

::: {.callout-note}
## `epi_df`

Represents a [snapshot]{.primary} that
contains the most [up-to-date values]{.primary} of the signal variables, [as of]{.primary} a given time.
:::

## `epi_df`: Snapshot of a dataset

* Consider the same dataset we just encountered on JHU daily COVID-19 cases and deaths rates from all states [as of]{.primary} May 31, 2022.

* We can see that it meets the criteria `epi_df` (has `geo_value` and `time_value` columns) and that it contains additional metadata (i.e. `geo_type`, `time_type`, `as_of`, and `other_keys`).

:::: {.columns}

::: {.column width="60%"}

```{r epi-df-ex}
#| echo: true
edf |> head()
```

:::

::: {.column width="40%"}

```{r extract-metadata}
#| echo: true
attr(edf, "metadata")
```

::: 

:::: 

## Examples of preprocessing

### EDA features

1. Making locations commensurate (per capita scaling)
1. Correlating signals across location or time 
1. Computing growth rates
1. Detecting and removing outliers
1. Calculating summaries with rolling windows
1. Dealing with revisions 

## Features - Correlations at different lags

<!-- * There are always at least two ways to compute correlations in an `epi_df`: grouping by `time_value`, and by `geo_value`. 

* The latter is obtained by setting `cor_by = geo_value`. -->

* The below plot addresses the question: "For each state, are case and death rates linearly associated across all days?"

* To explore **lagged correlations** and how case rates associate with future death rates, we can use the `dt1` parameter in `epi_cor()` to shift case rates by a specified number of days. 

<!--  * For example, setting `dt1 = -14` means that case rates on June 1st will be correlated with death rates on June 15th, assessing how past case rates influence future death rates. -->

```{r corr-lags-ex}
#| echo: true
cor0 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value)
cor14 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value, dt1 = -14)
```

```{r plot-corr-lags-ex}
#| fig-width: 7
#| warning: false
rbind(
  cor0 |> mutate(lag = 0),
  cor14 |> mutate(lag = 14)
) |>
  mutate(lag = as.factor(lag)) |>
  ggplot(aes(cor)) +
  geom_density(aes(fill = lag, col = lag), alpha = 0.5) +
  labs(x = "Correlation", y = "Density", fill = "Lag", col = "Lag")
```

* We can see that, in general, lagging the case rates back by 14 days improves the correlations.


## Features - Systematic lag analysis

The analysis helps identify the lag at which case rates from the past have the strongest correlation with future death rates.

```{r sys-lag-ex}
#| fig-width: 7
lags <- 0:35

z <- map_dfr(lags, function(lag) {
  epi_cor(edf, case_rate, death_rate, cor_by = geo_value, dt1 = -lag) %>%
    mutate(lag = .env$lag)
})

z_summary <- z %>%
  group_by(lag) %>%
  summarize(mean = mean(cor, na.rm = TRUE))

# Find the lag with the maximum correlation
max_lag <- z_summary$lag[which.max(z_summary$mean)]
```

```{r plot-sys-lag-ex}
#| fig-width: 7
#| warning: false
z_summary |> 
  ggplot(aes(x = lag, y = mean)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = max_lag, linetype = "dashed", color = "royalblue", lwd = 1) + 
  labs(x = "Lag", y = "Mean correlation")
```
The strongest correlation occurs at a lag of about 23 days, indicating that case rates are best correlated with death rates 23 days from now.

## Features - Compute growth rates

* Growth rate measures the relative change in a signal over time. <!-- indicating how quickly a quantity (like case rates) is increasing or decreasing. -->

* We can compute time-varying growth rates for the two states, and see how this cases evolves over time.

```{r growth-rates-ex}
#| echo: true
edfg <- filter(edf, geo_value %in% c("ut", "ca")) |>
  group_by(geo_value) |>
  mutate(gr_cases = growth_rate(time_value, case_rate, method = "trend_filter")) |>
  ungroup()
```

```{r plot-growth-rates-ex}
#| fig-align: center
#| fig-width: 12
#| fig-height: 5
edfg |>
  select(-death_rate) |>
  mutate(`Growth Rate` = gr_cases) |>
  pivot_longer(c(case_rate, gr_cases)) |>
  mutate(name = recode(name, case_rate = "Case Rate", gr_cases = "Growth Rate")) |>
  ggplot(aes(x = time_value, y = value, color = `Growth Rate`)) +
  facet_grid(name ~ geo_value, scales = "free_y", switch = "y") +
  geom_line(linewidth = 1) +
  scale_color_binned(type = function(...) continuous_scale("colour", ..., palette = function(x) {
    scales::pal_gradient_n(scales::pal_brewer(palette = "RdPu")(9))(x * 0.75 + 0.25)
  })) +
  geom_hline(aes(yintercept = 0),
             data = tibble(name = "Growth Rate"),
             linetype = "dashed") +
  theme_bw() +
  scale_x_date(name = "Date") +
  scale_y_continuous(name = NULL)
```

* As expected, the peak growth rates for both states occurred during the January 2022 Omicron wave, reflecting the sharp rise in cases over that period.

## Features - Outlier detection

<!-- * There are multiple outliers in these data that a modeler may want to detect and correct. -->

* The `detect_outlr()` function offers multiple outlier detection methods on a signal.

* The simplest is `detect_outlr_rm()`, which works by calculating an outlier threshold using the rolling median and the rolling Interquartile Range (IQR) for each time point:

**Threshold = Rolling Median ± (Detection Multiplier × Rolling IQR)**

* Note that the default number of time steps to use in the rolling window by default is 21 and is centrally aligned. 
* The detection multiplier default is 2 and controls how far away a data point must be from the median to be considered an outlier.

```{r outlier-ex}
#| echo: true
#| message: false
edfo <- filter(edf, geo_value %in% c("ca", "ut")) |>
  select(geo_value, time_value, case_rate) |>
  as_epi_df() |>
  group_by(geo_value) |>
  mutate(outlier_info = detect_outlr_rm(
    x = time_value, y = case_rate
  )) |>
  ungroup()
```

## Features - Outlier detection

* Several data points that deviate from the expected case cadence have been flagged as outliers, and may require further investigation.

* However, the peak in Jan. 2022 has also been flagged as an outlier. This highlights the importance of manual inspection before correcting the data, as these may represent valid events (e.g., a genuine surge in cases).

```{r plot-outlier-ex}
#| fig-width: 7
edfo |> 
  unnest() |> 
  mutate(case_corrected = replacement) |> 
  select(geo_value, time_value, case_rate, case_corrected) |> 
  pivot_longer(starts_with("case")) |> 
  mutate(
    name = case_when(
      name == "case_corrected" ~ "corrected",
      TRUE ~ "original"
    ),
    name = as.factor(name),
    name = fct_relevel(name, "original")
  ) |> 
  ggplot(aes(x = time_value)) +
  geom_line(aes(y = value, color = name)) +
  scale_color_brewer(palette = "Set1", name = "") +
  geom_hline(yintercept = 0) +
  facet_wrap(vars(geo_value), scales = "free_y", nrow = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "Date", y = "COVID-19 case rates") +
  theme(legend.position = c(.075, .8), 
        legend.background = element_rect(fill = NA), 
        legend.key = element_rect(fill = NA))
```


## Features -- sliding a computation on an `epi_df`

* It is often useful to compute rolling summaries of signals. 

* These depend on the reference time, and are computed separately over geographies (and other groups). 

* For example, a trailing average can smooth out daily variation.

* In `epiprocess`, this is achieved by `epi_slide()`.

```{r epi-slide-example-call}
#| echo: true
#| eval: false

epi_slide(
  .x,
  .f,
  ...,
  .window_size = NULL,
  .align = c("right", "center", "left"),
  .ref_time_values = NULL,
  .new_col_name = NULL,
  .all_rows = FALSE
)
```

For example, we can use `epi_slide()` to compute a trailing 7-day average. 

## Features -- sliding a computation on an `epi_df`

* The simplest way to use `epi_slide` is tidy evaluation. 

* For a grouped `epi_df`, `epi_slide()` applies the computation to groups [separately]{.primary}. 

```{r, grouped-df-to-slide}
#| echo: false

cases_edf <- cases_df |>
  group_by(geo_value) |>
  as_epi_df()

```

```{r epi-slide-tidy-eval}
#| echo: true

cases_7dav <- epi_slide(
  .x = cases_edf,
  cases_7dav = mean(raw_cases, na.rm = TRUE),
  .window_size = 7,
  .align = "right"
)
```


```{r epi-slide-tidy-eval-display-result}
#| echo: false

cases_7dav |>
  arrange(geo_value, time_value) |>
  group_by(geo_value) |>
  slice_min(time_value, n = 2) |>
  as_tibble() |>
  print()

```



## Features -- sliding a computation on an `epi_df`

`epi_slide` also accepts custom functions of a certain form. 

```{r epi-slide-custom-fucntion-mandatory-form}
#| eval: false
#| echo: true

custom_function <- function(x, g, t, ...) {
  
  # Function body
  
}
```

* `x`: the data frame with all the columns with original object [except]{.primary} groupping vars. 
* `g`: the one-row tibble with values of gropping vars of the given group. 
* `t`: the `.ref_time_value` of the current window. 
* `...`: additional arguments. 


## Features -- sliding a computation on an `epi_df`

```{r slide-apply-custom-function}
#| echo: true
#| code-line-numbers: "|9-17"

mean_by_hand <- function(x, g, t, ...) {
  data.frame(cases_7dav = mean(x$raw_cases, na.rm = TRUE))
}

cases_mean_custom_f = epi_slide(
    .x = cases_edf,
    .f = mean_by_hand,
    .window_size = 7,
    .align = "right"
)

```



```{r epi-slide-custom-function-display-result}
cases_mean_custom_f |>
  arrange(geo_value, time_value) |>
  slice_min(time_value, n = 2) |>
  as_tibble() |>
  print()
```



## `epi_archive`: Collection of `epi_df`s

* full version history of a data set
* acts like a bunch of `epi_df`s --- but stored [compactly]{.primary}
* allows similar functionality as `epi_df` but using only [data that would have been available at the time]{.primary}

::: {.callout-note}
## Revisions

Epidemiology data gets revised frequently.

* We may want to use the data [as it looked in the past].{.primary} 
* or we may want to examine [the history of revisions]{.primary}.
:::

## `epi_archive`: Collection of `epi_df`s

Subset of daily COVID-19 doctor visits (Optum) and cases (JHU CSSE) from all U.S. states in `archive` format:

```{r archive-ex}
#| echo: true
archive_cases_dv_subset_all_states |> head()
```

## Features -- sliding computation over `epi_df`s

* We can apply a computation over different snapshots in an `epi_archive`.

```{r}
#| echo: true
#| eval: false

epix_slide(
  .x,
  .f,
  ...,
  .before = Inf,
  .versions = NULL,
  .new_col_name = NULL,
  .all_versions = FALSE
)

```

This functionality is very helpful in version aware forecasting. We will return with a concrete example. 
  

## Features -- summarize revision behavior

* `revision_summary()` is a helper function that summarizes revision behavior of an `epix_archive`.

```{r epiprocess-revision-summary-demo}
#| echo: true
#| eval: true

revision_data <- revision_summary(
  archive_cases_dv_subset,
  case_rate_7d_av,
  drop_nas = TRUE,
  print_inform = FALSE, # NOT the default, to save space
  min_waiting_period = as.difftime(60, units = "days"),
  within_latest = 0.2,
  quick_revision = as.difftime(3, units = "days"),
  few_revisions = 3,
  abs_spread_threshold = NULL,
  rel_spread_threshold = 0.1,
  compactify_tol = .Machine$double.eps^0.5,
  should_compactify = TRUE
)
```

## Features -- summarize revision behavior

```{r epiprocess-revision-summary-results}
#| echo: true

head(revision_data)
```



## Visualize revision patterns

```{r create-snapshots-of-data}
#| echo: false
#| eval: true

versions <- seq(as.Date("2020-07-01"), as.Date("2021-11-30"), by = "1 month")
max_version <- max(versions)

snapshots <- map(versions, \(v) {
  epix_as_of(archive_cases_dv_subset_all_states, v) |>
    mutate(version = v) |>
    filter(geo_value %in% c("ca", "ut"))
  }) |>
  bind_rows() |>
  mutate(latest = version == max_version) 

```



```{r plot-revision-patterns}
#| fig-width: 7
ggplot(snapshots |> filter(!latest),
       aes(x = time_value, y = percent_cli)) +  
  geom_line(aes(color = factor(version))) + 
  geom_vline(aes(color = factor(version), xintercept = version), lty = 3) +
  facet_wrap(~ geo_value, scales = "free_y", nrow = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "", y = "% of doctor's visits with\n Covid-like illness") + 
  scale_color_viridis_d(option = "B", end = .8) +
  theme(legend.position = "none") +
  geom_line(data = snapshots |> filter(latest),
            aes(x = time_value, y = percent_cli), 
            inherit.aes = FALSE, color = "black")
```

  
## Types of prediction

* Counts are revised as time proceeds
* Want to know the [final]{.primary} value 
* Often not available until weeks/months later

  Forecasting
: At time $t$, predict the final value for time $t+h$, $h > 0$
  
  <br>
  
  Backcasting
: At time $t$, predict the final value for time $t-h$, $h < 0$

  <br>
  
  Nowcasting
: At time $t$, predict the final value for time $t$



# Basic Nowcasting in the Epiverse

<!-- predicting a finalized value from a provisional value and making predictions. -->
## Backfill Canadian edition
  
* Every week the [BC CDC releases COVID-19 hospitalization data](http://www.bccdc.ca/health-info/diseases-conditions/covid-19/archived-b-c-covid-19-data).

* Following week they revise the number upward (by ~25%) due to lagged reports.

![](gfx/bc_hosp_admissions_ex.jpg){style="width: 45%; display: block; margin-left: auto; margin-right: auto;"}
<!-- Newest iteration of "backfill”, Canada edition. Every week the BC CDC releases hospitalization data. The following week they revise the number upward (by about 25%) due to lagging reports. Every single week, the newspaper says “hospitalizations have declined”. This week the BC CDC’s own report said “hospitalizations have declined”. The takeaway in the news is that hospitalizations ALWAYS fall from the previous week, but once backfilled, they’re rarely down -->

* [**Takeaway**]{.primary}: Once the data is backfilled, hospitalizations rarely show a decline, challenging the common media narrative.

## Backfill American edition

* Again, we can see a similar systematic underestimation problem for COVID-19 morality rates in CA. <!-- 2023-2024 -->

* This plot also illustrates the [**revision process**]{.primary} - how the reported mortality changes & increases across multiple updates until it stabilizes at the final value (black line).

![](gfx/am_mortality_revisions_ex.jpg){style="width: 45%; display: block; margin-left: auto; margin-right: auto;"}

* These two examples show the problem and now we need a solution...

## Nowcasting and its mathematical setup

* **Nowcasting**: Predict a finalized value from a provisional value.

* Suppose today is time $t$

* Let $y_i$ denote a series of interest observed at times $i=1,\ldots, t$.

::: {.callout-important icon="false"}
## Our goal

* Produce a [**point nowcast**]{.primary} for the finalized values of $y_t$.
* Accompany with time-varying prediction intervals

:::

* We may also have access to $p$ other time series 
$x_{ij},\; i=1,\ldots,t, \; j = 1,\ldots, p$ which may be subject to revisions.



## Case study: NCHS mortality

* In this example, we'll demonstrate the concept of nowcasting using [**NHCS mortality data**]{.primary}.
(the number of weekly new deaths with confirmed or presumed COVID-19, per 100,000 population).
* We will work with [**provisional**]{.primary} data (real-time reports) and compare them to **finalized** data (final reports).
* The goal is to estimate or [**nowcast the mortality rate**]{.primary} for weeks when only provisional data is available.
  
## Fetch versioned data

Let's fetch versioned mortality data from the API (`pub_covidcast`) for CA (`geo_values = "ca"`) and the signal of interest (`deaths_covid_incidence_num`) over early 2024.

```{r mortality-archive-construct}
#| echo: true

# Fetch the versioned NCHS mortality data (weekly)
nchs_archive <- pub_covidcast(
  source = "nchs-mortality",
  signals = "deaths_covid_incidence_num",
  geo_type = "state",
  time_type = "week",
  geo_values = c("ca", "ut"),  
  time_values = epirange(202001, 202440),  
  issues = "*"
) |> 
  select(geo_value, time_value, version = issue, mortality = value) |> 
  as_epi_archive(compactify = TRUE)

```


## Analysis of versioning behavior 

Recall, we need to watch out for:

* [**Latency**]{.primary} the time difference between date of occurrence and date of the initial report
* [**Backfill**]{.primary} how data for a given date is updated after initial report. 

`revision_summary()` provides a sumamry to both aspects.  

```{r inspect-revision_summary}
#| echo: true
revision_data = revision_summary(nchs_archive, mortality, print_inform = FALSE)
```


## Versioning analysis -- latency

* [**Question:**]{.primary} What is the latency of NCHS data? 

```{r nchs-latency-via-revision-summary}
#| echo: true

revision_data |> select(geo_value, time_value, min_lag) |> slice_sample(n = 10)
```

* We randomly sampled some dates to check if there is a consistent latency pattern. 
* Understanding latency prevents us from using data that we shouldn't have access to. 

## Versioning analysis -- backfill

* [**Question:**]{.primary} How long does it take for the reported value to be close to the finalized value? 

```{r revision-summary-time-near-latest-demo}
revision_data |> select(geo_value, time_value, time_near_latest) |> slice_sample(n = 10)
```
* It generally takes at least 4 weeks for reported value to be within 20\% of the finalized value. 
* We can change the threshold of percentage difference by specifying `within_latest`  argument of `revision_summary()`. 

## Versioning analysis - backfill 

* [**Question:**]{.primary} When is the [**finalized value**]{.primary} first attained for each date? Would we have access to any in real-time?
* How fast are the final values attained & what's the pattern for these times, if any?


```{r finalized-value-first-attained-fun}
#| echo: false
check_when_finalized <- function(epi_archive, start_date = NULL, end_date = NULL) {
  # Extract the mortality archive data
  dt <- epi_archive$DT
  
  # Extract the latest (finalized) version
  mortality_latest <- epix_as_of(epi_archive, max(dt$version))
  
  # Merge the finalized mortality data with all versions
  merged_data <- dt |>
    filter(geo_value %in% mortality_latest$geo_value &
             time_value %in% mortality_latest$time_value) |>
    inner_join(mortality_latest, by = join_by(geo_value, time_value), suffix = c("", "_finalized"))
  
  # Find the minimal version where the finalized mortality first occurred
  finalized_version_data <- merged_data |>
    filter(mortality == mortality_finalized) |>
    group_by(geo_value, time_value) |>
    summarize(min_version = min(version), .groups = 'drop') |>
    mutate(diff = min_version - time_value)
  
  return(finalized_version_data)
}
```

```{r check-when-finalized-run}
#| echo: false
res <- check_when_finalized(nchs_archive, 
  start_date = min(nchs_archive$DT$time_value), 
  end_date = max(nchs_archive$DT$time_value))

head(res)
```
Here is a look at its quantiles:
```{r summary-diff}
#| echo: false
summary(as.numeric(res$diff))
```

* [**Conclusion**]{.primary}: The revision behavior is pretty long-tailed. Value reported 4 weeks later is reasonably close to the finalized value. 


## Revision pattern visualization  
This shows the finalized rates in comparison to [**multiple revisions**]{.primary} to see how the data changes over time:

```{r mortality-by-accessed-weeks-later}
#| echo: false

ref_dates <- unique(nchs_archive$DT$time_value)
offsets = seq(1, 7) * 7
max_version <- nchs_archive$versions_end

get_val_asof <- function(time_val, archive, offsets) {
  
  as_of_dates <- pmin(time_val + offsets, max_version)
  
  result <- map(as_of_dates, function(x) {
    
    qd <- archive |>
      epix_as_of(x) |>
      filter(time_value == time_val) |>
      select(geo_value, time_value, mortality) |>
      mutate(lag = x - time_val)
  }) |>
    list_rbind()
  
  return(result)

  
}

value_at_lags <- map(ref_dates, get_val_asof, 
  archive <- nchs_archive, offsets <- offsets) |>
  list_rbind()
  
values_final <- epix_as_of(nchs_archive, max(nchs_archive$versions_end))

```

```{r final-vs-revisions-plot}
#| echo: false
#| fig-width: 9
#| fig-height: 4
#| out-height: "500px"
ggplot(value_at_lags, aes(x = time_value, y = mortality)) +  
  geom_line(aes(color = factor(lag))) + 
  facet_wrap(~ geo_value, scales = "free_y", ncol = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "", y = "Weekly new COVID deaths") + 
  # scale_color_viridis_d(option="D", end=0.8) +
  theme(legend.position = "none") +
  geom_line(data = values_final, aes(x = time_value, y = mortality), 
            inherit.aes = FALSE, color = "black")


```

## Revision pattern visualization 

```{r nchs-plot-different-ver}
#| echo: false
#| eval: true
  
versions = seq.Date(as.Date("2021-01-01"), nchs_archive$versions_end, by = "1 month")
nchs_snapshots = map(versions, function(v) {
  epix_as_of(nchs_archive, v) |>
    mutate(version = v)}) |>
  bind_rows() |>
  mutate(latest = version == nchs_archive$versions_end)
```

```{r nchs-plot-val-different-ver}
#| echo: false
#| fig-width: 9
#| fig-height: 4
#| out-height: "500px"

ggplot(nchs_snapshots |> filter(!latest),
       aes(x = time_value, y = mortality)) +
  geom_line(aes(color = factor(version))) +
  geom_vline(aes(color = factor(version), xintercept = version), lty = 3) +
  facet_wrap(~ geo_value, scales = "free_y", ncol = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "", y = "Weekly covid deaths") +
  scale_color_viridis_d(option = "B", end = .8) +
  theme(legend.position = "none") +
  geom_line(data = nchs_snapshots |> filter(latest),
            aes(x = time_value, y = mortality),
            inherit.aes = FALSE, color = "black")
```

## Aside: Do we need a burn-in/training set? 

* Typical stat-ML practise suggests a train, validation, test split.
* But our exploratory analysis covered all available data, is that fine?


* Generally, for exploratory analysis, it is fine to not do train/test split. 
  + These analyses do not involve model fitting, we have little risk of an overly optimistic performance evaluation (no overfitting on test data).
* However, for a [**psuedo-prospective analysis**]{.primary}, the best practise is to do a train/test split.
  + In such analysis, one would be fitting and validating many models, a train/test split provides a more rigorous control on overfitting to test set. 

## Ratio nowcaster: jumping from provisional to finalized value

* Recall, the goal of nowcast at date $t$ is to use project the [*finalized value*]{.primary} of $y_t,$ given the information available on date $t$. 
* A very simple nowcaster is the ratio between finalized and provisional value. 

<br>

How can we sensibly estimate this quantity? 

<br>

::: {.fragment .fade-in}

* At nowcast date $t,$ would have recieved reports with versions up to and including $t.$
* We need to build training samples, which
  + correctly aligns finalized value against provisional value 
  + uses features that would have been available at test time 
  + have enough data to ensure sensible estimation results
:::

```{r nchs-ca-only}
#| echo: false
#| eval: true
nchs_archive <- pub_covidcast(
  source = "nchs-mortality",
  signals = "deaths_covid_incidence_num",
  geo_type = "state",
  time_type = "week",
  geo_values = "ca",  
  time_values = epirange(202001, 202440),  
  issues = "*"
) |> 
  select(geo_value, time_value, version = issue, mortality = value) |> 
  as_epi_archive(compactify = TRUE)
```


## Ratio nowcaster: building training samples

* At nowcast date $t,$ would have recieved reports with versions up to and including $t.$
* We need to build training samples, which
  + correctly aligns finalized value against provisional value 
  + uses features that would have been available at test time 
  + have enough samples to ensure sensible estimation results

::: {.fragment .fade-in}
* Build training samples by treating dates prior to date $t$ as actual nowcast dates.
  + What is the provisional data on that date?
  + Have we recieved finalized value for that date? 
:::

## Ratio nowcaster: building training samples

* At an earlier nowcast date $t_0,$ we define 
  + [**Provisional value**]{.primary} as the reported value of $Y_s$ with version $t_0.$ Here $s_0$ is the largest occurence date among all values reported up until $t_0.$
  + [**Finalized value**]{.primary} as the (potentially unobserved) finalized value of $Y_{s_0}.$
    - We only know in *hindsight* when reported value of $Y_{s_0}$ is finalized -- need an approximation. 

::: {.fragment .fade-in}
```{r revision-summary-time-near-latest-show-again}
#| echo: false
#| eval: true

revision_data |> select(geo_value, time_value, time_near_latest) |> slice_sample(n = 5)
quantile(revision_data$time_near_latest)
```
:::

::: {.fragment .fade-in}
We can say data reported 49 days after occurrence date is good enough to be considered finalized. 
:::

## Ratio nowcaster: test time feature

* At test time $t,$ take provisional value to be $Y_s,$ where $s$ is the largest occureence date among all values reported up until time $t.$

## Nowcasting at a single date: building training samples 

::: {.fragment .fade-in}
* Searching for provisional values, at previous hypothetical nowcast dates.

```{r one-date-look-for-provisional}
#| echo: true

nowcast_date <- as.Date("2022-01-02"); window_length = 180

initial_data <- nchs_archive$DT |>
  group_by(geo_value, time_value) |>
  filter(version == min(version)) |>
  rename(initial_val = mortality) |>
  select(geo_value, time_value, initial_val)

```
:::


::: {.fragment .fade-in}
* Searching for finalized values, at previous hypothetical nowcast dates.

```{r one-date-look-for-final}
#| echo: true
#| 
finalized_data <- epix_as_of(nchs_archive, nowcast_date) |>
  filter(time_value >= nowcast_date - 49 - window_length & time_value <= nowcast_date - 49) |>
  rename(finalized_val = mortality) |>
  select(geo_value, time_value, finalized_val)
```
:::


::: {.fragment .fade-in}
* After searching for both provisional and finalized values, we merge them together and estimate the ratio. 
```{r one-date-combine-provsional-and-final}
#| echo: true
  
ratio <- finalized_data |>
  inner_join(initial_data, by = c("geo_value", "time_value")) |>
  mutate(ratio = finalized_val / initial_val) |>
  pull(ratio) |>
  mean(na.rm = TRUE)
```
:::

## Nowcasting at a single date: test feature and actual nowcast 

```{r one-date-test-feat and nowcast}
#| echo: true

last_avail <- epix_as_of(nchs_archive, nowcast_date) |>
  slice_max(time_value) |>
  pull(mortality) 

nowcast <- last_avail * ratio
nowcast
```


## Nowcasting for multiple dates

* All previous manipulations should really be seen as a template for all nowcast dates. 
* The template computation sould be applied over all nowcast dates, [**but we must respect data versioning**]{.primary}! 
* `epix_slide()` is designed just for this! It behaves similarly to `epi_slide`.
* Key exception: `epix_slide()` is version aware: the sliding computation at any reference time $t$ is performed on [**data that would have been available as of t**]{.primary}.



## Nowcasting for multiple dates via `epix_slide()`

We begin by templatizing our previous operations. 

```{r nowcaster-to-slide}
#| echo: true

nowcaster = function(x, g, t, wl=180, appx=49) {
  

  initial_data = x$DT |>
    group_by(geo_value, time_value) |>
    filter(version ==  min(version)) |>
    filter(time_value >= t - wl - appx & time_value <= t - appx) |>
    rename(initial_val = mortality) |>
    select(geo_value, time_value, initial_val)

  finalized_data = x$DT |>
    group_by(geo_value, time_value) |>
    filter(version ==  max(version)) |>
    filter(time_value >= t - wl - appx & time_value <= t - appx) |>
    rename(finalized_val = mortality) |>
    select(geo_value, time_value, finalized_val)
  
  ratio = finalized_data |>
    inner_join(initial_data, by = c("geo_value", "time_value")) |>
    mutate(ratio = finalized_val / initial_val) |>
    pull(ratio) |>
    median(na.rm=TRUE)

  last_avail = epix_as_of(x, t) |>
    slice_max(time_value) |>
    pull(mortality) 
  
  res = tibble(geo_value = x$geo_value, target_date = t, nowcast = last_avail * ratio)
  
  return(res)
  
}

```

## Nowcasting for multiple dates via `epix_slide()`

```{r epix-slide-extract-nowcast-date}
#| echo: false
#| eval: true

all_nowcast_dates = nchs_archive$DT |>
  filter(time_value >= as.Date("2022-01-01")) |>
  distinct(time_value) |>
  pull(time_value)
```


```{r nowcasts-slided}
#| echo: true
#| eval: true

nowcasts = nchs_archive |>
  group_by(geo_value) |>
  epix_slide(
    nowcaster,
    .before=Inf,
    .versions = all_nowcast_dates,
    .all_versions = TRUE
)
```



## Details of `epix_slide()`


```{r epix-slide-demo-call-allversions-FALSE}
#| echo: true
#| eval: false

epix_slide(
  .x,
  .f,
  ...,
  .before = Inf,
  .versions = NULL,
  .new_col_name = NULL,
  .all_versions = FALSE
)
```


* `.f` in `epix_slide()` can be specified with the same form of custom function as `epi_slide()`. 

```{r epix-slide-form-of-custom-function}
#| echo: true
#| eval: false

function(x, g, t) {
  # function body
}
```

* Mandatory variables of `.f` would have different forms depending on the value of `.all_versions`. 


## Details of `epix_slide()`

```{r epix-slide-demo-call-allversions-FALSE-again}
#| echo: true
#| eval: false
#| code-line-numbers: "|8"

epix_slide(
  .x,                          
  .f,                         
  ...,                        
  .before = Inf,             
  .versions = NULL,           
  .new_col_name = NULL,      
  .all_versions = FALSE
)
```

::: {.fragment .fade-in}
* When `.all_versions = FALSE`, `epix_slide()` essentially iterates the templatized computation over snapshots. 
* Said differently, when `.all_versions = FALSE`, data accessed at any sliding iteration [**only involves a single version**]{.primary}. 
:::

::: {.fragment .fade-in}
* Hence: 
  + `x`: an `epi_df` with same column names as archive's `DT`, minus the `version` column.
  +  `g`: a one-row tibble containing the values of groupping variables of the associated group.
  + `t`: the `ref_time_value` of the current window.
  + `...`: additional arguments. 
:::

## Details of `epix_slide()`

```{r epix-slide-demo-call-allversions-TRUE}
#| echo: true
#| eval: false
#| code-line-numbers: "|8"

epix_slide(
  .x,                          
  .f,                         
  ...,                        
  .before = Inf,             
  .versions = NULL,           
  .new_col_name = NULL,      
  .all_versions = TRUE
)
```

::: {.fragment .fade-in}
* When `.all_versions = FALSE`, data accessed at any sliding iteration involves versions [**up to and including .version**]{.primary}. 
:::

::: {.fragment .fade-in}
* Hence: 
  + `x`: an `epi_archive`, with version up to and including `.version`. 
  +  `g`: a one-row tibble containing the values of groupping variables of the associated group.
  + `t`: the `.version` of the current window.
  + `...`: additional arguments. 
:::


## Details of `epix_slide()`

```{r nowcasts-slide-demo-only, eval=FALSE}
#| echo: true
#| eval: false
#| code-line-numbers: "|7,13,17"

nowcasts <- nchs_archive |>
  group_by(geo_value) |>
  epix_slide(
    nowcaster,
    .before=Inf,
    .versions = all_nowcast_dates,
    .all_versions = TRUE
)

nowcaster <- function(x, g, t, wl=180, appx=49) {
  

  initial_data = x$DT |>
    # Function body, omitted


  finalized_data = x$DT |>
    # Function body, omitted
  
}


```



## Visualize nowcasts


We are now finally able to compare nowcasts against first available reports:

```{r nowcast-subsetting}
#| echo: false


intial_val_extracter <- function(x, g, t, wl=180, appx=49) {
  
  last_avail = epix_as_of(x, t) |>
    slice_max(time_value) |>
    pull(mortality)
  
  res = tibble(geo_value = x$geo_value, target_date = t, value = last_avail)
  
  return(res)
  
}


provisional_val = epix_slide(nchs_archive, intial_val_extracter,  .versions = all_nowcast_dates, .all_versions = TRUE)

finalized_val = nchs_archive$DT |>
  filter(time_value >= as.Date("2022-01-01")) |>
  group_by(geo_value, time_value) |>
  filter(version == max(version))

```


```{r nowcast-fun-plot-results}
#| echo: false

ggplot() + 
  geom_line(data = finalized_val, aes(x = time_value, y = mortality, color = "Finalized")) +
  geom_point(data = finalized_val, aes(x = time_value, y = mortality, color = "Finalized"), shape = 16) +
  geom_line(data = provisional_val, aes(x = target_date, y = value, color = "Provisional")) +
  geom_point(data = provisional_val, aes(x = target_date, y = value, color = "Provisional"), shape = 16) +
  geom_line(data = nowcasts, aes(x = target_date, y = nowcast, color = "Nowcast")) +
  geom_point(data = nowcasts, aes(x = target_date, y = nowcast, color = "Nowcast"), shape = 16) +
  # geom_ribbon(data = nowcast_results_df, 
  #             aes(x = time_value, ymin = lower_PI, ymax = upper_PI), 
  #             alpha = 0.2, fill = "blue") +
  ylab("Mortality") +
  xlab("Date") +
  scale_color_manual(name =  "", 
    values = c("Nowcast" = "#71c5e8", "Provisional" = "#FF6900", "Finalized" = "black")) + 
  theme(legend.position = "bottom")
```

* The real-time counts tend to be biased below the finalized counts. Nowcasted values tend to provide a much better approximation of the truth (at least for these dates).


## Smoothing nowcasts

* Nowcasts are quite volatile, reflecting the provisional counts are far from complete. 
* We can use a trailing average to smooth them.

```{r smooth-nowcasts-epi-slide}
#| echo: true

smoothed_nowcasts <- epi_slide(
  nowcasts |> as_epi_df(),
  smoothed_nowcasts = mean(nowcast, na.rm = TRUE),
  .window_size = as.difftime(3, units = "weeks")
)

```

```{r nowcast-smoothed-vis}
#| echo: false

cbPalette = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442",
              "#0072B2", "#D55E00", "#CC79A7")

ggplot() + 
  geom_line(data = finalized_val, aes(x = time_value, y = mortality, color = "Finalized")) +
  geom_point(data = finalized_val, aes(x = time_value, y = mortality, color = "Finalized"), shape = 16) +
  geom_line(data = smoothed_nowcasts, aes(x = time_value, y = smoothed_nowcasts, color = "Smoothed")) +
  geom_point(data = smoothed_nowcasts, aes(x = time_value, y = smoothed_nowcasts, color = "Smoothed"), shape = 16) +
  geom_line(data = nowcasts, aes(x = target_date, y = nowcast, color = "Nowcast")) +
  geom_point(data = nowcasts, aes(x = target_date, y = nowcast, color = "Nowcast"), shape = 16) +
  # geom_ribbon(data = nowcast_results_df, 
  #             aes(x = time_value, ymin = lower_PI, ymax = upper_PI), 
  #             alpha = 0.2, fill = "blue") +
  ylab("Mortality") +
  xlab("Date") +
  scale_color_manual(name = "", values = c("Nowcast" = "#71c5e8", "Smoothed" = "#97D700", "Finalized" = "black")) + 
  theme(legend.position = "bottom")
```



## Evaluation using MAE

* Assume we have prediction $\hat y_{t}$ for the provisional value at time $t$.

* Then for $y_{t}$ over times $t = 1, \dots, N$, then we may compute error metrics like mean absolute error (MAE).


* MAE measures the average absolute difference between the nowcast and finalized values. 

$$MAE = \frac{1}{N} \sum_{t=1}^N |y_{t}- \hat y_{t}|$$

* Note that it's scale-dependent, meaning it can vary depending on the units of the data (e.g., cases, deaths, etc.).

## Evaluation using MAE

Let's numerically evaluate our point nowcasts for the provisional values of a time series (e.g., COVID-19 mortality) using MAE.

<!-- Accuracy of nowcast is assessed by how close provisional estimates are to the finalized values to gauge the model's performance. -->


```{r mae-code}
#| echo: false
# Step 1: Join the mortality data with nowcast data
mae_data <- finalized_val |> 
  select(-version) |>
  inner_join(nowcasts |> select(-version), by = join_by("geo_value", "time_value" == "target_date")) |>
  inner_join(initial_data, by = c("geo_value", "time_value")) |>
  inner_join(smoothed_nowcasts |> select(-version, -nowcast), by = c("geo_value", "time_value"))

# Step 2: Calculate the absolute error between actual and nowcasted values
mae_data <- mae_data |> 
  mutate(raw_nc_res = abs(mortality - nowcast),
         prov_res = abs(mortality - initial_val),
         smoothed_res = abs(mortality - smoothed_nowcasts))  

# Step 3: Compute the MAE (mean of absolute errors)
mae_value <- mae_data |> 
  group_by(geo_value) |>
  summarise(`Smoothed MAE` = mean(smoothed_res, na.rm = TRUE),
            `Unsmoothed nowcast MAE` = mean(raw_nc_res, na.rm = TRUE),
            `Provisional value MAE` = mean(prov_res, na.rm = TRUE)) |>
  select(-geo_value) 
  
knitr::kable(mae_value)
```


# Nowcasting with Regression

## Nowcasting: Moving from one predictor to multiple

* The ratio model predicts the finalized value of $Y_t$ from $Y_{s}$, the last value included in the version $t$ report.
* $Y_s$ is the closest in time we can get to $Y_t$, but we also expect it to be the least reliable value in version $t$.
* Can we add $Y_{s - 1}$, $Y_{s - 2}$, and even other data sources to the model to try to find a good mix of relevant and reliable signals?
* Regressions models will let us do that; let's start by adding $Y_{s - 1}$, etc.

##

We'll start experimenting with just a single nowcast date:

```{r regression-trial-nowcast-data-2}
#| echo: true
nchs_ca_archive <- nchs_archive$DT[geo_value == "ca",] |>
  as_epi_archive()

trial_nowcast_date <- all_nowcast_dates[[1]]

# This is the version history we'd have seen at that point:
nchs_ca_past_archive <- nchs_ca_archive |>
  epix_as_of(trial_nowcast_date, all_versions = TRUE)

# And this is what the latest version was at that point:
nchs_past_latest <- nchs_ca_past_archive |>
  epix_as_of(trial_nowcast_date)

# At version t, our target is finalized Y_t:
target_time_value <- trial_nowcast_date
```

## What predictors do we use?

Let's try using provisional $Y_s$ to predict $Y_t$ as long as $s$ is within 2 weeks of $t$:

```{r predictor-descriptions-2}
#| echo: true
predictor_descriptions <- nchs_past_latest |>
  filter(as.integer(target_time_value - time_value) <= 14) |>
  mutate(lag_days = as.integer(trial_nowcast_date - time_value)) |>
  select(-c(geo_value, time_value)) |>
  pivot_longer(-lag_days, names_to = "varname") |>
  mutate(predictor_name = paste0(varname, "_lag", lag_days, "_realtime")) |>
  drop_na() |>
  select(varname, lag_days, predictor_name)
predictor_descriptions
```

## Line up with training data

Just like with the ratio nowcaster, we need to make sure to line up our predictors in `nchs_past_latest` with training data that is analogous (e.g., "equally unreliable").  Here's a utility function we'll be using:

```{r regression-trial-nowcast-analogues-2}
#| echo: true
#| code-fold: true
library(data.table)
get_predictor_training_data <- function(archive, varname, lag_days, predictor_name) {
  epikeytime_names <- setdiff(key(archive$DT), "version")
  requests <- unique(archive$DT, by = epikeytime_names, cols = character())[
  , version := time_value + ..lag_days
  ]
  setkeyv(requests, c(epikeytime_names, "version"))
  result <- archive$DT[
    requests, c(key(archive$DT), varname), roll = TRUE, nomatch = NULL, allow.cartesian = TRUE, with = FALSE
  ][
  , time_value := version
  ][
  , version := NULL
  ]
  nms <- names(result)
  nms[[match(varname, nms)]] <- predictor_name
  setnames(result, nms)
  setDF(result)
  as_tibble(result)
}
```

TODO print fn prototype or name

##


```{r regression-trial-nowcast-analogues-example-2}
#| echo: true
get_predictor_training_data(nchs_ca_past_archive, "mortality", 7, "mortality_lag7_realtime")
```
The first value here is a version of $Y_{\text{2020-11-30}}$ as it was reported in version 2020-12-06.  We expect it to have similar characteristics as $Y_{t - 7\text{ days}}$ as reported in version $t$ for other values of $t$.

## Get our predictor and target data

```{r regression-trial-nowcast-predictor-target-2}
#| echo: true
predictors <- predictor_descriptions |>
  pmap(function(varname, lag_days, predictor_name) {
    get_predictor_training_data(nchs_ca_past_archive, varname, lag_days, predictor_name)
  }) |>
  reduce(full_join, by = c("geo_value", "time_value"))

target <- nchs_past_latest |>
  filter(time_value <= max(time_value) - 49) |>
  rename(mortality_stable = mortality)
```

## Fit the regression model

```{r regression-trial-nowcast-fit-2}
#| echo: true
training_test <- full_join(predictors, target, by = c("geo_value", "time_value"))

training <- training_test |> drop_na()
test <- training_test |> filter(time_value == trial_nowcast_date)

fit <- training |>
  select(all_of(predictor_descriptions$predictor_name), mortality_stable) |>
  quantreg::rq(formula = mortality_stable ~ ., tau = 0.5)

# TODO test linear and use if not worse

pred <- tibble(
  nowcast_date = trial_nowcast_date,
  target_date = target_time_value,
  prediction = unname(predict(fit, test))
)
```

## Our prediction

```{r regression-trial-nowcast-pred-2}
#| echo: true

pred
```

## Backtesting our nowcaster

We'll use `epix_slide()` again.

* And get an error --- some versions $t$ don't include a value $Y_{t-1}$ or $Y_{t-2}$.
    * So let's try looking farther into the past at $Y_{t-3}$, etc.
    * ... but don't look too far: $Y_{t-5}$ is the limit.
    * The same idea applies to models with 3 or more features.
    * Including more features tends to improve performance, up to a point.
* Some other features and fixes are also included in the code below.

```{r regression-nowcaster-function-2}
regression_nowcaster <- function(archive, settings, return_info = FALSE) {
  if (!inherits(archive, "epi_archive")) {
    stop("`archive` isn't an `epi_archive`")
  }
  if (length(unique(archive$DT$geo_value)) != 1L) {
    stop("Expected exactly one unique `geo_value`")
  }
  if (archive$time_type == "day") {
    archive <- thin_daily_to_weekly_archive(archive)
  }

  nowcast_date <- archive$versions_end
  target_time_value <- nowcast_date
  latest_edf <- archive |> epix_as_of(nowcast_date)
  print(nowcast_date)

  predictor_descriptions <-
    latest_edf |>
    mutate(lag_days = as.integer(nowcast_date - time_value)) |>
    select(-c(geo_value, time_value)) |>
    pivot_longer(-lag_days, names_to = "varname", values_to = "value") |>
    drop_na(value) |>
  # TODO sanity-check presence of requested variables
    inner_join(settings$predictors, by = "varname") |>
    filter(abs(lag_days) <= max_abs_shift_days) |>
    arrange(varname, abs(lag_days)) |>
    group_by(varname) |>
    filter(seq_len(n()) <= max_n_shifts[[1]]) |>
    ungroup() |>
    mutate(predictor_name = paste0(varname, "_lag", lag_days, "_realtime")) |>
    select(varname, lag_days, predictor_name)

  predictor_edfs <- predictor_descriptions |>
    pmap(function(varname, lag_days, predictor_name) {
      get_predictor_training_data(archive, varname, lag_days, predictor_name)
    }) |>
    lapply(na.omit) |>
    keep(~ nrow(.x) >= settings$min_n_training_per_predictor)

  if (length(predictor_edfs) == 0) {
    stop("Couldn't find acceptable predictors in the latest data.")
  }

  predictors <- predictor_edfs |>
    reduce(full_join, by = c("geo_value", "time_value"))

  target <- latest_edf |>
    filter(time_value <= max(time_value) - settings$days_until_target_semistable) |>
    select(geo_value, time_value, mortality_semistable = mortality)

  training_test <- full_join(predictors, target, by = c("geo_value", "time_value"))

  training <- training_test |>
    drop_na() |>
    slice_max(time_value, n = settings$max_n_training_intersection)

  test <- training_test |>
    filter(time_value == nowcast_date)

  fit <- training |>
    select(any_of(predictor_descriptions$predictor_name), mortality_semistable) |>
    quantreg::rq(formula = mortality_semistable ~ ., tau = 0.5)

  pred <- tibble(
    geo_value = "ca",
    nowcast_date = nowcast_date,
    target_date = target_time_value,
    prediction = unname(predict(fit, test))
  )

  # TODO intercept toggle?

  if (return_info) {
    return(tibble(
      coefficients = list(coef(fit)),
      predictions = list(pred)
    ))
  } else {
    return(pred)
  }
}

# We can apply this separately for each nowcast_date to ensure that we consider
# the latest possible value for every signal, though whether that is advisable
# or not may depend on revision characteristics of the signals.
thin_daily_to_weekly_archive <- function(archive) {
  key_nms <- key(archive$DT)
  val_nms <- setdiff(names(archive$DT), key_nms)
  update_tbl <- as_tibble(archive$DT)
  val_nms |>
    lapply(function(val_nm) {
      update_tbl[c(key_nms, val_nm)] |>
        # thin out to weekly, making sure that we keep the max time_value with non-NA value:
        filter(as.POSIXlt(time_value)$wday == as.POSIXlt(max(time_value[!is.na(.data[[val_nm]])]))$wday) |>
        # re-align:
        mutate(
          time_value = time_value - as.POSIXlt(time_value)$wday, # Sunday of same epiweek
          old_version = version,
          version = version - as.POSIXlt(version)$wday # Sunday of same epiweek
        ) |>
        slice_max(old_version, by = all_of(key_nms)) |>
        select(-old_version) |>
        as_epi_archive(other_keys = setdiff(key_nms, c("geo_value", "time_value", "version")),
                       compactify = TRUE)
    }) |>
    reduce(epix_merge, sync = "locf")
}

# Baseline model:
locf_nowcaster <- function(archive) {
  nowcast_date <- archive$versions_end
  target_time_value <- nowcast_date
  latest_edf <- archive |> epix_as_of(nowcast_date)

  latest_edf |>
    complete(geo_value, time_value = target_time_value) |>
    arrange(geo_value, time_value) |>
    group_by(geo_value) |>
    fill(mortality) |>
    ungroup() |>
    filter(time_value == target_time_value) |>
    transmute(
      geo_value,
      nowcast_date = nowcast_date,
      target_date = time_value,
      prediction = mortality
    )
}
```

##

```{r regression-model-settings-2}
ar_settings <- list(
  predictors = tribble(
    ~varname,    ~max_abs_shift_days, ~max_n_shifts,
    "mortality",                  35,             3,
    # FIXME TODO vs. list of lags relative to max time value? target max time
    # value? after turning into weekly via extraction? maybe do sequential
    # intersection and skip joining when would reduce number of rows too much?
    ),
  min_n_training_per_predictor = 30, # or else exclude predictor
  days_until_target_semistable = 7 * 7, # filter out unstable when training (and evaluating)
  min_n_training_intersection = 20, # or else raise error
  max_n_training_intersection = Inf # or else filter down rows
)

arx_settings <- list(
  predictors = tribble(
    ~varname, ~max_abs_shift_days, ~max_n_shifts,
    # FIXME TODO vs. list of lags relative to max time value? target max time
    # value? after turning into weekly via extraction? maybe do sequential
    # intersection and skip joining when would reduce number of rows too much?
    "admissions", 35, 3,
    "mortality", 35, 3,
    ),
  min_n_training_per_predictor = 30, # or else exclude predictor
  days_until_target_semistable = 7 * 4, # filter out unstable when training (and evaluating)
  min_n_training_intersection = 20, # or else raise error
  max_n_training_intersection = Inf # or else filter down rows
)

hhs_hosp_archive <- pub_covidcast(
  source = "hhs",
  signals = "confirmed_admissions_covid_1d_7dav",
  geo_type = "state",
  time_type = "day",
  geo_values = "ca",
  time_values = epirange(20210301, 20241001),
  issues = "*"
) |>
  transmute(geo_value, time_value, version = issue, admissions = 7 * value) |>
  as_epi_archive(compactify = TRUE)

nchs_ca_archive_daily <-
  nchs_ca_archive$DT |>
  as_tibble() |>
  mutate(
    time_value = time_value + 6L, # align with trailing averages
    version = version + 4L # assume NCHS data were released only on Thursdays
  ) |>
  group_by(geo_value, time_value, version) |>
  reframe(time_value = time_value - 6:0,
          mortality = c(rep(NA, 6), mortality)) |>
  as_epi_archive(compactify = TRUE)

hosp_mort_archive <- epix_merge(hhs_hosp_archive, nchs_ca_archive_daily, sync = "locf")

locf_nowcasts <- nchs_ca_archive |>
  epix_slide(~ locf_nowcaster(.x), .versions = all_nowcast_dates, .all_versions = TRUE)

ar_nowcasts <- nchs_ca_archive |>
  epix_slide(~ regression_nowcaster(.x, ar_settings), .versions = all_nowcast_dates, .all_versions = TRUE)

arx_nowcasts <- hosp_mort_archive |>
  epix_slide(~ regression_nowcaster(.x, arx_settings),
             .versions = all_nowcast_dates + 4, # assume we nowcast on Thursday, same day as assumed NCHS release
             .all_versions = TRUE)

```

## Comparison

```{r regression-nowcast-plot-comparison-2}
ratio_nowcasts_archive <- nowcasts |>
  filter(geo_value == "ca") |>
  rename(time_value = target_date,
         prediction_ratio = nowcast) |>
  as_epi_archive(compactify = TRUE)

list(
  arx_nowcasts |> rename(prediction_arx = prediction),
  ar_nowcasts |> rename(prediction_ar = prediction)#,
  # locf_nowcasts |> rename(prediction_locf = prediction),
  # ratio_nowcasts_archive$DT |> as_tibble() |> rename(nowcast_date = version, target_date = time_value),
#   get_predictor_training_data(nchs_ca_archive, "mortality", 14L, "mortality_lag14_realtime") |>
#     transmute(geo_value, nowcast_date = time_value, target_date = time_value, mortality_lag14_realtime)
) |>
  lapply(select, -any_of("version")) |>
  reduce(full_join, by = c("geo_value", "nowcast_date", "target_date")) |>
  full_join(nchs_ca_archive |> epix_as_of(nchs_ca_archive$versions_end),
            by = c("geo_value", "target_date" = "time_value")) |>
  pivot_longer(starts_with("prediction"), names_to = "nowcaster", values_to = "prediction") |>
  ggplot() +
  geom_line(aes(target_date, mortality)) +
  geom_line(aes(target_date, prediction, colour = nowcaster)) +
  scale_color_delphi()
```

TODO improve legends, standardize plots

TODO explain in slides that are including hospitalizations

TODO `_ar`, `_arx` -> `reg` / etc.

TODO mea culpa complicated quick, will explain concepts of regression, lagged features, evaluation more carefully tomorrow; call it after the evaluations

## Evaluations

```{r regression-nowcast-eval-comparison-2}
list(
  arx_nowcasts |> rename(prediction_arx = prediction),
  ar_nowcasts |> rename(prediction_ar = prediction),
  locf_nowcasts |> rename(prediction_locf = prediction),
  ratio_nowcasts_archive$DT |> as_tibble() |> rename(nowcast_date = version, target_date = time_value),
  get_predictor_training_data(nchs_ca_archive, "mortality", 14L, "mortality_lag14_realtime") |>
    transmute(geo_value, nowcast_date = time_value, target_date = time_value, mortality_lag14_realtime)
) |>
  lapply(select, -any_of("version")) |>
  reduce(full_join, by = c("geo_value", "nowcast_date", "target_date")) |>
  full_join(nchs_ca_archive |> epix_as_of(nchs_ca_archive$versions_end),
            by = c("geo_value", "target_date" = "time_value")) |>
  # Limit evaluation to forecasting tasks with predictions from all methods:
  drop_na(starts_with("prediction")) |>
  pivot_longer(starts_with("prediction"), names_to = "nowcaster", values_to = "prediction") |>
  # Filter evaluation based on target stability
  filter(target_date <= nchs_ca_archive$versions_end - 49) |>
  summarize(.by = nowcaster,
            MAE = mean(abs(mortality - prediction)),
            MAPE = 100*mean(abs(mortality - prediction)/abs(mortality)))
```


## Data Sources: Google searches & hospital admissions

* [**Google Search Trends**]{.primary}: Symptoms like cough, fever, and shortness of breath.
  * [**s01**]{.primary}: Cough, Phlegm, Sputum, Upper respiratory tract infection  
  * [**s02**]{.primary}: Nasal congestion, Post nasal drip, Sinusitis, Common cold

* [**Hospital Admissions**]{.primary}: Data from the Department of Health & Human Services on confirmed influenza admissions.

* Using these, we will [**nowcast**]{.primary} hospital admissions by using Google symptom search trends for GA from April to June 2023.

* The first step is to fetch this data...

## Data Sources: Google searches & hospital admissions

```{r fetch-google-data}
#| echo: true
# Fetch Google symptom data for s01 and s02
x1 <- pub_covidcast(
  source = "google-symptoms",
  signals = "s01_smoothed_search", 
  geo_type = "state",
  time_type = "day",
  geo_values = "ga",
  time_values = epirange(20230401, 20230701),
  issues = "*"
) |>
  select(geo_value, time_value, version = issue, avg_search_vol_s01 = value) |>
  as_epi_archive(compactify = FALSE)

x2 <- pub_covidcast(
  source = "google-symptoms",
  signals = "s02_smoothed_search",
  geo_type = "state",
  time_type = "day",
  geo_values = "ga",
  time_values = epirange(20230401, 20230701),
  issues = "*"
) |>
  select(geo_value, time_value, version = issue, avg_search_vol_s02 = value) |>
  as_epi_archive(compactify = FALSE)

# Fetch hospital admissions data
y1 <- pub_covidcast(
  source = "hhs",
  signals = "confirmed_admissions_influenza_1d",
  geo_type = "state",
  time_type = "day",
  geo_values = "ga",
  time_values = epirange(20230401, 20230701),
  issues = "*"
) |>
  select(geo_value, time_value, version = issue, admissions = value) |>
  as_epi_archive(compactify = FALSE)
```

## Merging the archives

* We'll merge the symptom search trends (`x1`, `x2`) with hospital admissions data (`y`) using `epix_merge()` from `epiprocess`.
* This allows us to match data by time and geography, & fill any missing values with the most recent observation (LOCF).

```{r merge-archives-two-pred}
#| echo: false
# Merge the Google trends data (x1, x2) with hospital admissions data (y)
archive <- epix_merge(x1, y1, sync = "locf", compactify = FALSE)
archive <- epix_merge(archive, x2, sync = "locf", compactify = FALSE)
```

## Linear Model: A simple approach for nowcasting

* Aside from ratios, one of the simplest approach to nowcasting is to use a [**linear regression model**]{.primary}.
* We model the relationship between provisional (predictor) data and response data.
* This model helps us make [**predictions**]{.primary} for the finalized data based on the current (provisional) signals.

## Linear regression
* [**Goal**]{.primary}: Estimate the coefficients $\beta_0$ and $\beta_1$ that describe the relationship between the predictor $x_i$ and the outcome $y_i$.
* [**Linear Model**]{.primary}: The relationship is assumed to be:

  $$y_i \approx \beta_0 + \beta_1 x_i $$
  
  where
  $\beta_0$ is the intercept,
  $\beta_1$ is the slope.
* **In R**: Use `lm(y ~ x)` to estimate the coefficients, where `y` is the outcome variable and `x` is the predictor.

## Multiple linear regression
* [**Goal**]{.primary}: Estimate coefficients $\beta_0, \beta_1, \dots, \beta_p$ that describe the relationship between multiple predictors $x_{i1}, x_{i2}, \dots, x_{ip}$ and the outcome $y_i$.
* [**Model**]{.primary}: The relationship is assumed to be:

  $$y_i \approx \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}$$
  
  where:
  $\beta_0$ is the intercept,
  $\beta_1, \dots, \beta_p$ are the coefficients.
* [**In R**]{.primary}: Use `lm(y ~ x1 + x2 + ... + xp)` to estimate the coefficients, where `y` is the outcome and `x1, x2, ..., xp` are the predictors.

## Multiple linear regression model

* A linear model is a good choice to describe the relationship between search trends and hospital admissions.
* The model will include two predictors (s01 and s02).
* We'll use these two search trend signals to predict hospital admissions (response).

<!-- A linear regression model will be used to predict hospital admissions from search trends (s01 and s02). -->

## Multiple linear regression model
```{r multiple-lr-mod-predict}
#| echo: true
# Define the function for lm model fit and prediction
lm_mod_pred <- function(data, gk, rtv, ...) {
  
  # Fit the linear model
  model <- lm(admissions ~ avg_search_vol_s01 + avg_search_vol_s02, data = data)
  
  # Make predictions
  predictions = predict(model,
                        newdata = data |>
                          # Use tidyr::fill() for LOCF if predictor data is incomplete 
                          fill(avg_search_vol_s01, .direction = "down") |> 
                          fill(avg_search_vol_s02, .direction = "down") |>
                          filter(time_value == max(time_value)),
                        interval = "prediction", level = 0.9
  )

  # Pull off true time value for comparison to target
  real_time_val = data |> filter(time_value == max(time_value)) |> pull(admissions)

  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val))
}
```
Note that this code is intentionally simple; while it can be refined to handle cases like negatives or other boundary conditions, we aim to avoid unnecessary complexity.

## Nowcasting with `epix_slide()`

* We will use `epix_slide()` to create a sliding window of training data.
* The model will be trained on a 14-day window before the target date, and predictions will be made for the target date.
* The beauty of this function is that it is version-aware - the sliding computation at any given reference time [**t**]{.primary} is performed on data that would have been available as of [**t**]{.primary} automatically. 

## Nowcasting with `epix_slide()`
```{r nowcasting-epix-slide}
#| echo: true
# Define the reference time points for nowcasting
targeted_nowcast_dates <- seq(as.Date("2023-04-15"), as.Date("2023-06-15"), by = "1 week")
versions = targeted_nowcast_dates + 2  # Adjust for the systematic 2-day latency in the response
# Determine this from revision_summary(y1, print_inform = TRUE) 

# Perform nowcasting using epix_slide
nowcast_res <- archive |>
  group_by(geo_value) |>
  epix_slide(
    .f = lm_mod_pred,
    .before = 14,  # 14-day training period
    .versions = versions
  ) |>
  mutate(targeted_nowcast_date = targeted_nowcast_dates, time_value = actual_nowcast_date) |>
  ungroup()

# View results
head(nowcast_res, n=2)
```

## Compare with the actual admissions 

After making predictions, we compare them to the actual hospital admissions.

```{r join-with-actual-admissions}
#| echo: true
# Left join with latest results 
# Latest snapshot of data (with the latest/finalized admissions)
x_latest <- epix_as_of(archive, max(archive$DT$version)) |> select(-c(avg_search_vol_s01, avg_search_vol_s02))

res <- nowcast_res |> left_join(x_latest, by = join_by(geo_value, time_value))
head(res)
```

## Visualizing the nowcast results

We can then visualize the nowcast results alongside the true values using `ggplot2`:

```{r plot-multiple-lr-nowcast-res}
#| echo: false
# Plot the predictions vs real-time vs actual admissions
ggplot(res, aes(x = time_value)) +
  geom_line(aes(y = admissions, color = "Finalized Admissions"), size = 1.2) +
  geom_point(aes(y = fit, color = "Nowcast"), size = 3) +
  geom_point(aes(y = real_time_val, color = "Real Time"), size = 3) +
  geom_line(aes(y = fit, color = "Nowcast"), size = 1.2, linetype = "dashed") +
  geom_ribbon(aes(ymin = lwr, ymax = upr, fill = "Pred. Interval"), alpha = 0.3) +
  labs(title = "",
       x = "Date", y = "Hospital Admissions", color = "Legend", fill = "Legend") +
  scale_color_manual(values = c("Finalized Admissions" = "black", "Nowcast" = "#1f78b4", "Real Time" = "darkred")) +
  scale_fill_manual(values = c("Pred. Interval" = "#a6cee3")) +
  theme_minimal() +
  theme(legend.position = "top", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

## Evaluation using MAE

* As before, we can evaluate our point nowcasts numerically using MAE.

```{r mae-code-hosp}
#| echo: true
# Calculate the absolute error between actual and nowcasted values
mae_data_admissions <- res |> 
  mutate(nc_abs_error = abs(admissions - fit),  # Nowcast vs Finalized admissions
         rt_abs_error = abs(admissions - real_time_val))  # Real-Time vs Finalized admissions

# Compute the MAE (mean of absolute errors)
mae_value_admissions <- mae_data_admissions |> 
  summarise(nc_MAE = mean(nc_abs_error),
            rt_MAE = mean(rt_abs_error))
knitr::kable(mae_value_admissions)
```

* Based off of comparing these simple error measures, the nowcast MAE is clearly better.

## Evaluation using MAE

However, when we visualize the distribution of errors across time, it is not so cut-and-dry:
```{r abs-error-plot-hosp}
# Plot the absolute errors for Nowcast and Real-Time vs Finalized Admissions
ggplot(mae_data_admissions) +
  # Nowcast Absolute Error 
  geom_point(aes(x = time_value, y = nc_abs_error), color = "#1f78b4", size = 2) +
  geom_line(aes(x = time_value, y = nc_abs_error), color = "#1f78b4") +
  # Horizontal line for Nowcast Mean Absolute Error
  geom_hline(aes(yintercept = mean(nc_abs_error), color = "Nowcast MAE"), linetype = "dashed") +
  
  # Real-Time Absolute Error 
  geom_point(aes(x = time_value, y = rt_abs_error), color = "darkred", size = 2) +
  geom_line(aes(x = time_value, y = rt_abs_error), color = "darkred") +
  # Horizontal line for Real-Time Mean Absolute Error
  geom_hline(aes(yintercept = mean(rt_abs_error), color = "Real-Time MAE"), linetype = "dashed") +
  
  # Customize the x and y axes labels, legend & add title
  xlab("Time") + ylab("Absolute Error") +
  scale_color_manual(values = c("Real-Time MAE" = "darkred", "Nowcast MAE" = "#1f78b4")) +
  labs(color = "Mean Absolute Error") +
  theme_minimal() +
  theme(legend.position = "top", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

The main driver behind the real-time MAE being greater is the "outlier-like" May 25 AE.

So visualizing can provide an important perspective that is missed from a simple numerical summary of error.

## Key Takeaways: Linear regression nowcasting example

* [**Provisional Data as Predictors**]{.primary}: Using [Google symptom search trends]{.primary} to predict [influenza hospital admissions]{.primary}.
* [**Simple Linear Model**]{.primary}: A linear regression model captures the relationship between symptom searches and hospital admissions.
* [**Actionable Predictions**]{.primary}: Nowcasts provide [timely insights]{.primary} for hospital admissions, even before data is finalized.
* [**Sliding Window Approach**]{.primary}: Predictions are based on [data up to the current time]{.primary}, ensuring no future information influences the nowcast.
* [**Evaluation**]{.primary}: Predictions are compared with actual admissions using numerical and visual perspectives.

# Case Study - Nowcasting Cases Using %CLI 

## Goal of this case study

[**Goal**]{.primary}: Nowcast COVID-19 Cases for MA using the estimated percentage of COVID-related doctor's visits (%CLI), based on outpatient data from Optum.

* %CLI is contained in the Epidata API.
* Cases by specimen collection date are not. They are from the MA gov website.
* Cases in the API (JHU) are aligned by report date, not specimen collection/test date.
* Working with cases aligned by [**test date**]{.primary} allows us to avoid the more unpredictable delays introduced by the [**report date**]{.primary}.

## Summary of main steps

The workflow is similar to the previous example where we nowcasted using two variables, only more involved. 
The main steps are...

1. [**Fetch Data**]{.primary}: Retrieve %CLI and COVID-19 case data (by specimen collection date) for MA.

2. [**Merge Data**]{.primary}: Align %CLI and case data using `epix_merge`, filling missing values via last observation carried forward (LOCF).

3. [**Model & Prediction**]{.primary}: Fit a linear model to predict cases based on %CLI, trained on a 30-day rolling window.

4. [**Nowcast Execution**]{.primary}: Use `epix_slide` to nowcast the cases dynamically. 

5. [**Visualization**]{.primary}: Plot actual vs. nowcasted cases with confidence intervals to assess model accuracy.

So the first step is to fetch the data...

## Construct an `epi_archive` from scratch

[Here's]("https://www.mass.gov/info-details/archive-of-covid-19-cases-2020-2021") the archive of COVID-19 case excel files from the MA gov website, which we'll use to construct our own `epi_archive`.
<br>
<br>
Brief summary of this data:

* [**First release**]{.primary}: Raw .xlsx data was first released early January 2021.

* [**Change in reporting**]{.primary}: Starting [**July 1, 2021**]{.primary}, the dashboard shifted from [**7 days/week**]{.primary} to [**5 days/week**]{.primary} (Monday-Friday).

* [**Friday, Saturday, and Sunday**]{.primary} data is included in the [**Monday**]{.primary} dashboard.

* When [**Monday**]{.primary} is a holiday, the [**Friday through Monday**]{.primary} data is posted on [**Tuesday**]{.primary}.


## Construct an `epi_archive` from scratch

* [**Purpose**]{.primary}: To create an `epi_archive` object for storing versioned time series data.
* [**Required Columns**]{.primary}:
  * `geo_value`: Geographic data (e.g., region).
  * `time_value`: Time-related data (e.g., date, time).
  * `version`: Tracks when the data was available (enables version-aware forecasting).
* [**Constructor**]{.primary}:
  * `new_epi_archive()`: For manual construction of `epi_archive` (assumes validation of inputs).
* [**Recommended Method**]{.primary}:
  * `as_epi_archive()`: Simplifies the creation process, ensuring proper formatting and validation. We'll use this one when we download some data from the MA gov website!


## Main steps to construct the `epi_archive`

1. [**Load necessary Libraries**]{.primary}: Such as `tidyverse`, `readxl`, `epiprocess`.
2. [**Process Each Date's Data**]{.primary}: 
   * A function we'll make (`process_covid_data`) downloads and processes daily COVID-19 data from the MA gov Excel files on their website.
   * The data is cleaned and formatted with columns: `geo_value`, `time_value`, `version`, and values.
3. [**Handle Missing Data**]{.primary}: Checks if a date's data is available (handle 404 errors).
4. [**Create `epi_archive`**]{.primary}: 
   * Combine processed data into a tibble.
   * Convert the tibble to an `epi_archive` object using `as_epi_archive()`.


## Fetch Data - Code for one date
```{r fetch-web-data-one-date}
#| echo: true
# Load required libraries
library(tidyverse)
library(readxl)
library(httr)
library(tibble)
library(epiprocess)

# Function to download and process each Excel file for a given date
process_covid_data <- function(Date) {
  # Generate the URL for the given date
  url <- paste0("https://www.mass.gov/doc/covid-19-raw-data-", tolower(gsub("-0", "-", format(Date, "%B-%d-%Y"))), "/download") 
  # Applies gsub("-0", "-", ...) to replace any occurrence of -0 (such as in "April-01") with just - (resulting in "April-1").
  
  # Check if the URL exists (handle the 404 error by skipping that date)
  response <- GET(url)
  
  if (status_code(response) != 200) {
    return(NULL)  # Skip if URL doesn't exist (404)
  }
  
  # Define the destination file path for the Excel file
  file_path <- tempfile(fileext = ".xlsx")
  
  # Download the Excel file
  GET(url, write_disk(file_path, overwrite = TRUE))
  
  # Read the relevant sheet from the Excel file
  data <- read_excel(file_path, sheet = "CasesByDate (Test Date)")
  
  # Process the data: rename columns and convert Date
  data <- data |>
    rename(
      Date = `Date`,
      Positive_Total = `Positive Total`,
      Positive_New = `Positive New`,
      Case_Average_7day = `7-day confirmed case average`
    ) |>
    mutate(Date = as.Date(Date))  # Convert to Date class
  
  # Create a tibble with the required columns for the epi_archive
  tib <- tibble(
    geo_value = "ma",  # Massachusetts (geo_value)
    time_value = data$Date,  # Date from the data
    version = Date,  # The extracted version date
    case_rate_7d_av = data$Case_Average_7day  # 7-day average case value
  )
  
  return(tib)
}
```

## Fetch Data - Code breakdown 

* This purpose of this function is to download and process each Excel file as of a date.
* [**URL Creation**]{.primary}: Dynamically generates the URL based on the date, removing leading zeros in day values (e.g., "April-01" → "April-1").
* [**Check URL**]{.primary}: Sends a request (`GET(url)`) and skips the date if the URL returns a non-200 status (e.g., 404 error).
* [**Download File**]{.primary}: Saves the Excel file to a temporary path using `tempfile()` and `GET()`.
* [**Read Data**]{.primary}: Loads the relevant sheet ("CasesByDate") from the Excel file using `read_excel()`.
* [**Tibble Creation**]{.primary}: Constructs a tibble with `geo_value`, `time_value`, `version`, and `case_rate_7d_av` to later compile into an `epi_archive` (you can think of an `epi_archive` as being a comprised of many `epi_df`s).


## Fetch Data - Process eange of dates
* Note that `process_covid_data()` works on one date at a time.
* So now, we need a function that iterates over a date range and applies `process_covid_data()` to each date & combines the resulting tibbles into an `epi_archive`.
* We call this function `process_data_for_date_range()`...

## Fetch Data - Process range of dates
```{r process-range-of-dates}
#| echo: true
# Function to process data for a range of dates
process_data_for_date_range <- function(start_date, end_date) {
  # Generate a sequence of dates between start_date and end_date
  date_sequence <- seq(as.Date(start_date), as.Date(end_date), by = "day")
  
  # Process data for each date and combine results
  covid_data_list <- lapply(date_sequence, function(Date) {
    process_covid_data(Date)  # Skip over dates with no data (NULLs will be ignored)
  })
  
  # Combine all non-null individual tibbles into one data frame
  combined_data <- bind_rows(covid_data_list[!sapply(covid_data_list, is.null)])
  
  # Convert the combined data into an epi_archive object
  if (nrow(combined_data) > 0) {
    epi_archive_data <- combined_data |>
      as_epi_archive(compactify = FALSE)
    
    return(epi_archive_data)
  } else {
    message("No valid data available for the given date range.")
    return(NULL)
  }
}
```

## Fetch Data - Code breakdown
Here's a summary of what `process_data_for_date_range()` does:
1. [**Generates Date Range**]{.primary}: Creates a sequence of dates between `start_date` and `end_date`.

2. [**Processes Data**]{.primary}: Applies the `process_covid_data` function to each date in the range (skip over dates with no data).

3. [**Combines Results**]{.primary}: Combines all valid (non-NULL) tibbles into one single data frame.

4. [**Creates `epi_archive`**]{.primary}: Converts the combined data into an `epi_archive` object.

## Fetch Data - Run the function & inspect archive

* Now, let's run the function & inspect the resulting `epi_archive` of 7-day avg. COVID-19 case counts:
* Expect building the archive to some time (enough for a cup of coffee or to meditate on life).

<!-- To wonder why you chose Expect building the archive to take a nontrivial amount of time (enough for a cup of coffee or to wonder why you chose coding in the first place). -->
```{r run-fun-process-data-range}
#| echo: true
# Example usage: process data between Jan. 10, 2021, and Dec. 1, 2021
y <- process_data_for_date_range("2021-01-10", "2021-12-01")  # Raw .xlsx data is first released on Jan. 4, 2021
y
```

* Alternatively, you may run the following to load `y` that was previously saved as an RDS file: 
```{r load-ma-case-data}
#| echo: true
#| eval: FALSE
#| results: hide
y <- readRDS("_data/ma_case_archive.rds")
```


## Fetch Data - % Outpatient doctors visits for CLI

* Now, from the Epidata API, let's download the [estimated percentage of outpatient doctor visits]("https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/doctor-visits.html") primarily for COVID-related symptoms, based on health system data.
* Comes pre-smoothed in time using a Gaussian linear smoother
* This will be the predictor when we nowcast COVID-19 cases in MA.

```{r fetch-outpatient-cli}
#| echo: true
# Step 1: Fetch Versioned Data 
x <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  geo_type = "state",
  time_type = "day",
  geo_values = "ma", # Just for MA to keep it simple (& to go with the case data by test date for that state)
  time_values = epirange(20210301, 20211231),
  issues = epirange(20210301, 20211231)
) |>
  select(geo_value, time_value,
         version = issue,
         percent_cli = value
  ) |>
  as_epi_archive(compactify = FALSE)
```

## Use `epix_merge()` to merge the two archives
Now we'll use `epix_merge()` to combine the two `epi_archive`s that share the same `geo_value` & `time_value`.

<!-- LOCF is used to ensure missing data is handled by filling forward. -->
```{r merge-archives-one-pred}
#| echo: true
archive <- epix_merge(
  x, y,
  sync = "locf",
  compactify = FALSE
)
archive
```

## Fitting and predicting with linear model

* Define `lm_mod_pred()`: A function that fits a linear model to forecast cases based on the `percent_cli` predictor.
* Use `predict()` with a 90% prediction interval.
* Save the actual cases to compare to the nowcasts later.

```{r simple-lr-mod-predict}
#| echo: true
lm_mod_pred <- function(data, ...) {
  # Linear model
  model <- lm(case_rate_7d_av ~ percent_cli, data = data)

  # Make predictions
  predictions = predict(model,
                        newdata = data |>
                          fill(percent_cli, .direction = "down") |> 
                          filter(time_value == max(time_value)),
                        interval = "prediction", level = 0.9)
  
  # Pull off real-time value for later comparison to the nowcast value
  real_time_val = data |> filter(time_value == max(time_value)) |> pull(case_rate_7d_av)
  
  # Could clip predictions and bounds at 0
  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val)) 
}
```

## Nowcasting with `epix_slide()`
* [**Specify targets**]{.primary}: Define the target dates for nowcasting (e.g., 1st of each month) & adjust training data to include the lag for the latent case data.
* [**Sliding window**]{.primary}: Use `epix_slide()` to apply the linear model across a sliding window of data for each region.
* [**Training-test split**]{.primary}: Use the last 30 days of data to train and predict cases for each target nowcast date.

## Nowcasting with `epix_slide()`
```{r nowcasting-epix-slide-cases}
#| echo: true
# Define the reference time points / versions (to give the training/test split)
targeted_nowcast_dates <- seq(as.Date("2021-04-01"), as.Date("2021-11-01"), by = "1 month") 
versions = targeted_nowcast_dates + 1 # + 1 because the case data is 1 day latent. 
# Determine this from revision_summary(y)

# Use epix_slide to perform the nowcasting with a training-test split
nowcast_res <- archive |>
  group_by(geo_value) |>
  epix_slide(
    .f = lm_mod_pred,  # Pass the function defined above
    .before = 30,   # Training period of 30 days
    .versions = versions # Determines the day where training data goes up to (not inclusive)
  ) |>
  mutate(targeted_nowcast_date = targeted_nowcast_dates,
         time_value = actual_nowcast_date)

# Take a peek at the results
head(nowcast_res, n = 1)
```

## Visualizing nowcasts vs. actual values
Merge the nowcast results with the latest data for more direct comparison:

```{r join-with-actual-cases}
#| echo: true
x_latest <- epix_as_of(archive, max(archive$DT$version)) |>
  select(-percent_cli) 

res <- nowcast_res |> left_join(x_latest, by = join_by(geo_value, time_value))

res
```

## Visualizing nowcasts vs. actual values

Now, plot the predictions & real-time values on top of latest COVID-19 cases using `ggplot2`:

```{r plot-simple-lr-nowcast-res}
#| echo: false
ggplot(res, aes(x = time_value)) +
  geom_line(aes(y = case_rate_7d_av, color = "Finalized Cases (7-dav)"), size = 1.2) +
  geom_point(aes(y = fit, color = "Nowcast"), size = 3) +
  # Plot the real-time values
  geom_point(aes(y = real_time_val, color = "Real Time"), size = 3) +
  geom_line(aes(y = fit, color = "Nowcast"), size = 1.2, linetype = "dashed") +
  geom_ribbon(aes(ymin = lwr, ymax = upr, fill = "Pred. Interval"), alpha = 0.3) +
  # Title and labels
  labs(title = "",
       x = "Date",
       y = "Case Count",
       color = "Legend",
       fill = "Legend") +
  # Adjust colors
  scale_color_manual(values = c("Finalized Cases (7-dav)" = "black",
                                "Nowcast" = "#1f78b4",
                                "Real Time" = "darkred")) + 
  scale_fill_manual(values = c("Pred. Interval" = "#a6cee3")) + # Light blue
  # Improve the theme
  theme_minimal() +
  theme(
    legend.position = "top",
    legend.title = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

## Evaluation using MAE

* Finally, we numerically evaluate our nowcasts using MAE.

* Shows that the nowcast errors are lower than those of the real-time estimates.

```{r mae-code-cases}
#| echo: true
# Calculate the absolute error between actual and nowcasted COVID-19 cases
mae_data_cases <- res |> 
  mutate(nc_abs_error = abs(case_rate_7d_av - fit),  # Nowcast vs Finalized cases (7-day average)
         rt_abs_error = abs(case_rate_7d_av - real_time_val))  # Real-Time vs Finalized cases

# Compute the MAE (mean of absolute errors)
mae_value_cases <- mae_data_cases |> 
  summarise(nc_MAE = mean(nc_abs_error),
            rt_MAE = mean(rt_abs_error))
knitr::kable(mae_value_cases)
```


## Evaluation using MAE

```{r abs-error-plot-cases}
# Plot the absolute errors for Nowcast and Real-Time vs Finalized COVID-19 cases
ggplot(mae_data_cases) +
  # Nowcast Absolute Error 
  geom_point(aes(x = time_value, y = nc_abs_error), color = "#1f78b4", size = 2) +
  geom_line(aes(x = time_value, y = nc_abs_error), color = "#1f78b4") +
  # Horizontal line for Nowcast Mean Absolute Error
  geom_hline(aes(yintercept = mean(nc_abs_error), color = "Nowcast MAE"), linetype = "dashed") +
  
  # Real-Time Absolute Error 
  geom_point(aes(x = time_value, y = rt_abs_error), color = "darkred", size = 2) +
  geom_line(aes(x = time_value, y = rt_abs_error), color = "darkred") +
  # Horizontal line for Real-Time Mean Absolute Error
  geom_hline(aes(yintercept = mean(rt_abs_error), color = "Real-Time MAE"), linetype = "dashed") +
  
  # Customize the x and y axes labels, legend & add title
  xlab("Date") + ylab("Absolute Error") +
  scale_color_manual(values = c("Real-Time MAE" = "darkred", "Nowcast MAE" = "#1f78b4")) +
  labs(color = "Mean Absolute Error") +
  theme_minimal() +
  theme(legend.position = "top", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

The elevated errors at both ends highlight periods where the discrepancies between the real-time and nowcast estimates are most pronounced.

## Takeaways

[**Goal**]{.primary}: Predict COVID-19 cases using %CLI, overcoming delays in report data.

Main Steps:

1. [**Fetch Data**]{.primary}: Collect case and %CLI data.

2. [**Merge Data**]{.primary}: Align datasets with `epix_merge()` and fill missing values.

3. [**Model**]{.primary}: Fit a linear model to predict cases.

4. [**Nowcast**]{.primary}: Apply dynamic forecasting with `epix_slide()`.

5. [**Evaluate**]{.primary}: Calculate error measures and numerically and visually assess the results.

Overall, nowcasting, based on the linear model, provided a closer approximation of true cases compared to the real-time values.



## Aside on nowcasting

* To some Epis, "nowcasting" can be equated with "estimate the time-varying instantaneous reproduction number, $R_t$"

* Ex. using the number of reported COVID-19 cases in British Columbia between Jan. 2020 and Apr. 15, 2023. 

<!-- This data is the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. The values are.up-to-date as of August 2023. -->
```{r rtestim}
#| fig-width: 9
#| fig-height: 3
#| out-height: "400px"
#| label: nowcasting
library(rtestim)
source(here::here("_code/bccovid.R"))

p1 <- bccovid|>
  ggplot(aes(date, cases)) + 
  geom_line(colour = primary) +
  geom_vline(xintercept = ymd("2023-04-15"), colour = secondary,
             linewidth = 2) +
  labs(y = "BC Covid-19 cases", x = "Date") +
  scale_y_continuous(expand = expansion(c(0, NA)))
bc_rt <- estimate_rt(bccovid$cases, x = bccovid$date, 
                     lambda = c(1e6, 1e5))
p2 <- plot(confband(bc_rt, lambda = 1e5)) + 
  coord_cartesian(ylim = c(0.5, 2)) +
  scale_y_continuous(expand = expansion(0))
cowplot::plot_grid(p1, p2)
```

* Group built [`{rtestim}`](https://dajmcdon.github.io/rtestim) doing for this nonparametrically.

* We may come back to this later...
