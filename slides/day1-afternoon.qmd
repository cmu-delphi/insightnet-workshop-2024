---
talk-title: "Data Cleaning, Versioning, and Nowcasting"
talk-short-title: "Nowcasting"
talk-subtitle: "InsightNet Forecasting Workshop 2024"
talk-date: "11 December -- Afternoon"
format: revealjs
---

{{< include _titleslide.qmd >}}

```{r theme-load-pkg}
#| cache: false
library(tidyverse)
library(epidatr)
library(epipredict)
library(epidatasets)
theme_set(theme_bw())
```

## Outline


1. Epiverse Software ecosystem

1. Panel and Versioned Data in the epiverse

1. Basic Nowcasting using `{epiprocess}`

1. Nowcasting with Two Variables

1. Case Study - Nowcasting Cases Using %CLI


  

## Epi. data processing with `epiprocess`


```{r fetch-jhu-dplyr-demo-data}
#| echo: false
library(epidatr)
library(epidatasets)


cases_df_api <- pub_covidcast(
  source = "jhu-csse",
  signals = "confirmed_incidence_num",
  geo_type = "state",
  time_type = "day",
  geo_values = "ca,nc,ny",
  time_values = epirange(20220301, 20220331),
  as_of = as.Date("2024-01-01")
)

cases_df <- cases_df_api |>
  select(geo_value, time_value, raw_cases = value) # We'll talk more about this soon :)

cases_count <- cases_df |>
  drop_na() |> # Removes rows where any value is missing (from tidyr)
  group_by(geo_value) |>
  summarize(count = n())

cases_count <- cases_df |>
  drop_na() |> 
  count(geo_value)


state_census = state_census |> select(abbr, pop) |> filter(abbr != "us")

cases_inner_join <- cases_df |>
  inner_join(state_census, join_by(geo_value == abbr))

case_rates_df <- cases_inner_join |>
  mutate(scaled_cases = raw_cases / pop * 1e5) # cases / 100K


```


* `epiprocess` is a package that offers additional functionality to pre-process such epidemiological data.
* You can work with an `epi_df` like you can with a tibble by using dplyr verbs.
* For example, on `cases_df`, we can easily use `epi_slide_mean()` to calculate trailing 14 day averages of cases:

```{r trailing-average-ex}
#| echo: true
case_rates_df <- case_rates_df |>
  as_epi_df(as_of = as.Date("2024-01-01")) |>
  group_by(geo_value) |>
  epi_slide_mean(scaled_cases, .window_size = 14, na.rm = TRUE) |>
  rename(smoothed_scaled_cases = slide_value_scaled_cases)
head(case_rates_df)
```

## Epi. data processing with `epiprocess`
It is easy to produce an autoplot the smoothed confirmed daily cases for each `geo_value`:
```{r autoplot-ex}
#| echo: true
case_rates_df |>
  autoplot(smoothed_scaled_cases)
```

## Epi. data processing with `epiprocess`

Alternatively, we can display both the smoothed and the original daily case rates:

```{r smoothed-original-plot}
#| echo: false
ggplot(case_rates_df) +
  geom_line(aes(x = time_value, y = scaled_cases, color = geo_value), size = 0.25) +
  geom_line(aes(x = time_value, y = smoothed_scaled_cases, color = geo_value), size = 1) +
  facet_wrap(vars(geo_value), nrow = 1, scales = "free") +
  ylab("Cases per 100k") +
  theme_bw() +
  theme(legend.position = "none") +
  guides(x =  guide_axis(angle = 25))
```
Now, before exploring some more features of `epiprocess`, let's have a look at the epiverse software ecosystem it's part of...

# Epiverse Software Ecosystem

## The epiverse ecosystem
Interworking, community-driven, packages for epi tracking & forecasting.

![](gfx/epiverse_packages_flow.jpg){style="width: 60%; display: block; margin-left: auto; margin-right: auto;"}

<!-- 1. Fetch data: epidatr, epidatpy, and other sources, 2. Explore, clean, transform & backtest 3. Pre-built forecasters, modular forecasting framework: epipredict -->
  
  
  
# Panel and Versioned Data in the Epiverse
  
## What is panel data?

* Recall that [panel data](https://en.wikipedia.org/wiki/Panel_data), or longitudinal data, 
contain cross-sectional measurements of subjects over time. 
* Built-in example: [`covid_case_death_rates`](
  https://cmu-delphi.github.io/epidatasets/reference/covid_case_death_rates.html) 
dataset, which is a snapshot [**as of**]{.primary} May 31, 2022 that contains daily state-wise measures of `case_rate` and `death_rate` for COVID-19 over 2021:
  
```{r head-edf}
#| echo: false
edf <- covid_case_death_rates
# Only consider the 50 US states (no territories)
edf <- edf |> filter(geo_value %in% tolower(state.abb)) 
head(edf |> as_tibble())
```

* How do we store & work with such snapshots in the epiverse software ecosystem?

  
  
## `epi_df`: Snapshot of a dataset

* You can convert panel data into an `epi_df` with the required `geo_value` and `time_value` columns

Therefore, an `epi_df` is...

* a tibble that requires columns `geo_value` and `time_value`.

* arbitrary additional columns containing [measured values]{.primary}

* additional [keys]{.primary} to index (`age_group`, `ethnicity`, etc.)

::: {.callout-note}
## `epi_df`

Represents a [snapshot]{.primary} that
contains the most [up-to-date values]{.primary} of the signal variables, [as of]{.primary} a given time.
:::

## `epi_df`: Snapshot of a dataset

* Consider the same dataset we just encountered on JHU daily COVID-19 cases and deaths rates from all states [as of]{.primary} May 31, 2022.

* We can see that it meets the criteria `epi_df` (has `geo_value` and `time_value` columns) and that it contains additional metadata (i.e. `geo_type`, `time_type`, `as_of`, and `other_keys`).

:::: {.columns}

::: {.column width="60%"}

```{r epi-df-ex}
#| echo: true
edf |> head()
```

:::

::: {.column width="40%"}

```{r extract-metadata}
#| echo: true
attr(edf, "metadata")
```

::: 

:::: 

## Examples of preprocessing

### EDA features

1. Making locations commensurate (per capita scaling)
1. Correlating signals across location or time 
1. Computing growth rates
1. Detecting and removing outliers
1. Dealing with revisions 

## Features - Correlations at different lags

<!-- * There are always at least two ways to compute correlations in an `epi_df`: grouping by `time_value`, and by `geo_value`. 

* The latter is obtained by setting `cor_by = geo_value`. -->

* The below plot addresses the question: "For each state, are case and death rates linearly associated across all days?"

* To explore **lagged correlations** and how case rates associate with future death rates, we can use the `dt1` parameter in `epi_cor()` to shift case rates by a specified number of days. 

<!--  * For example, setting `dt1 = -14` means that case rates on June 1st will be correlated with death rates on June 15th, assessing how past case rates influence future death rates. -->

```{r corr-lags-ex}
#| echo: true
cor0 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value)
cor14 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value, dt1 = -14)
```

```{r plot-corr-lags-ex}
#| fig-width: 7
#| warning: false
rbind(
  cor0 |> mutate(lag = 0),
  cor14 |> mutate(lag = 14)
) |>
  mutate(lag = as.factor(lag)) |>
  ggplot(aes(cor)) +
  geom_density(aes(fill = lag, col = lag), alpha = 0.5) +
  labs(x = "Correlation", y = "Density", fill = "Lag", col = "Lag")
```

* We can see that, in general, lagging the case rates back by 14 days improves the correlations.


## Features - Systematic lag analysis

The analysis helps identify the lag at which case rates from the past have the strongest correlation with future death rates.

```{r sys-lag-ex}
#| fig-width: 7
lags <- 0:35

z <- map_dfr(lags, function(lag) {
  epi_cor(edf, case_rate, death_rate, cor_by = geo_value, dt1 = -lag) %>%
    mutate(lag = .env$lag)
})

z_summary <- z %>%
  group_by(lag) %>%
  summarize(mean = mean(cor, na.rm = TRUE))

# Find the lag with the maximum correlation
max_lag <- z_summary$lag[which.max(z_summary$mean)]
```

```{r plot-sys-lag-ex}
#| fig-width: 7
#| warning: false
z_summary |> 
  ggplot(aes(x = lag, y = mean)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = max_lag, linetype = "dashed", color = "royalblue", lwd = 1) + 
  labs(x = "Lag", y = "Mean correlation")
```
The strongest correlation occurs at a lag of about 23 days, indicating that case rates are best correlated with death rates 23 days from now.

## Features - Compute growth rates

* Growth rate measures the relative change in a signal over time. <!-- indicating how quickly a quantity (like case rates) is increasing or decreasing. -->

* We can compute time-varying growth rates for the two states, and see how this cases evolves over time.

```{r growth-rates-ex}
#| echo: true
edfg <- filter(edf, geo_value %in% c("ut", "ca")) |>
  group_by(geo_value) |>
  mutate(gr_cases = growth_rate(time_value, case_rate, method = "trend_filter")) |>
  ungroup()
```

```{r plot-growth-rates-ex}
#| fig-align: center
#| fig-width: 12
#| fig-height: 5
edfg |>
  select(-death_rate) |>
  mutate(`Growth Rate` = gr_cases) |>
  pivot_longer(c(case_rate, gr_cases)) |>
  mutate(name = recode(name, case_rate = "Case Rate", gr_cases = "Growth Rate")) |>
  ggplot(aes(x = time_value, y = value, color = `Growth Rate`)) +
  facet_grid(name ~ geo_value, scales = "free_y", switch = "y") +
  geom_line(linewidth = 1) +
  scale_color_binned(type = function(...) continuous_scale("colour", ..., palette = function(x) {
    scales::pal_gradient_n(scales::pal_brewer(palette = "RdPu")(9))(x * 0.75 + 0.25)
  })) +
  geom_hline(aes(yintercept = 0),
             data = tibble(name = "Growth Rate"),
             linetype = "dashed") +
  theme_bw() +
  scale_x_date(name = "Date") +
  scale_y_continuous(name = NULL)
```

* As expected, the peak growth rates for both states occurred during the January 2022 Omicron wave, reflecting the sharp rise in cases over that period.

## Features - Outlier detection

<!-- * There are multiple outliers in these data that a modeler may want to detect and correct. -->

* The `detect_outlr()` function offers multiple outlier detection methods on a signal.

* The simplest is `detect_outlr_rm()`, which works by calculating an outlier threshold using the rolling median and the rolling Interquartile Range (IQR) for each time point:

**Threshold = Rolling Median ± (Detection Multiplier × Rolling IQR)**

* Note that the default number of time steps to use in the rolling window by default is 21 and is centrally aligned. 
* The detection multiplier default is 2 and controls how far away a data point must be from the median to be considered an outlier.

```{r outlier-ex}
#| echo: true
#| message: false
edfo <- filter(edf, geo_value %in% c("ut", "ca")) |>
  select(geo_value, time_value, case_rate) |>
  as_epi_df() |>
  group_by(geo_value) |>
  mutate(outlier_info = detect_outlr_rm(
    x = time_value, y = case_rate
  )) |>
  ungroup()
```

## Features - Outlier detection

* Several data points that deviate from the expected case cadence have been flagged as outliers, and may require further investigation.

* However, the peak in Jan. 2022 has also been flagged as an outlier. This highlights the importance of manual inspection before correcting the data, as these may represent valid events (e.g., a genuine surge in cases).

```{r plot-outlier-ex}
#| fig-width: 7
edfo |> 
  unnest() |> 
  mutate(case_corrected = replacement) |> 
  select(geo_value, time_value, case_rate, case_corrected) |> 
  pivot_longer(starts_with("case")) |> 
  mutate(
    name = case_when(
      name == "case_corrected" ~ "corrected",
      TRUE ~ "original"
    ),
    name = as.factor(name),
    name = fct_relevel(name, "original")
  ) |> 
  ggplot(aes(x = time_value)) +
  geom_line(aes(y = value, color = name)) +
  scale_color_brewer(palette = "Set1", name = "") +
  geom_hline(yintercept = 0) +
  facet_wrap(vars(geo_value), scales = "free_y", nrow = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "Date", y = "COVID-19 case rates") +
  theme(legend.position = c(.075, .8), 
        legend.background = element_rect(fill = NA), 
        legend.key = element_rect(fill = NA))
```


## `epi_archive`: Collection of `epi_df`s

* full version history of a data set
* acts like a bunch of `epi_df`s --- but stored [compactly]{.primary}
* allows similar functionality as `epi_df` but using only [data that would have been available at the time]{.primary}


::: {.callout-note}
## Revisions

Epidemiology data gets revised frequently.

* We may want to use the data [as it looked in the past]{.primary} 
* or we may want to examine [the history of revisions]{.primary}.
:::

## `epi_archive`: Collection of `epi_df`s

Full version history of national provisional death counts. 

```{r nchs-build-archive}
#| echo: true
#| fig-width: 7

nchs_allver = pub_covidcast(
  "nchs-mortality",
  "deaths_covid_incidence_num",
  "state",
  "week",
  # Recall you can specify these options to only pull
  # a subset of data
  geo_values = c("ca", "ut"),
  # This data has weekly resolution, different form of 
  # time specification
  time_values = epirange(202001, 202440),
  # Recall this pulls all available issues
  issues = "*"
) |>
  select(geo_value, time_value, issue, value)

nchs_archive = as_epi_archive(nchs_allver, compactify = TRUE)
```

How is this data getting revised? 


## Revision pattern

Some problem here -- NCHS's revision time is on the order of weeks, jumping by months hides a lot of the revisioning. 
```{r get-snapshots}
# Obtain data at mulitple versions
edf_latest = epix_as_of(nchs_archive, version = nchs_archive$versions_end)
max_version = max(nchs_archive$DT$version)
edf_latest = epix_as_of(nchs_archive, version = max_version)
versions = seq(as.Date("2020-01-01"), max_version - 1, by = "1 month")

monthly_snapshots = map(versions, function(v) {
  
  epix_as_of(nchs_archive, v) |> mutate(version = v)
  
}) |>
  bind_rows(
    edf_latest |> mutate(version = max_version)
  ) |>
  mutate(latest = version == max_version)


```

```{r plot-revision-patterns}
ggplot(monthly_snapshots |> filter(!latest),
       aes(x = time_value, y = value)) +  
  geom_line(aes(color = factor(version))) + 
  geom_vline(aes(color = factor(version), xintercept = version), lty = 3) +
  facet_wrap(~ geo_value, scales = "free_y", ncol = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "", y = "Weekly new COVID deaths") + 
  scale_color_viridis_d(option="D", end=0.8) +
  theme(legend.position = "none") +
  geom_line(data = monthly_snapshots |> filter(latest),
            aes(x = time_value, y = value), 
            inherit.aes = FALSE, color = "black")
```
A lot of values at UT is censored due to privacy constraints. 

## Revision pattern -- alternative visualization

If the above is not satisfactory, we can plot by values of the series, accessed k weeks after the reference date...

```{r}
ref_dates = unique(nchs_allver$time_value)
offsets = seq(1, 7) * 7

get_val_asof = function(time_val, archive, offsets) {
  
  as_of_dates = pmin(time_val + offsets, max_version)
  
  result = map(as_of_dates, function(x) {
    
    qd = archive |>
      epix_as_of(x) |>
      filter(time_value == time_val) |>
      select(geo_value, time_value, value) |>
      mutate(lag = x - time_val)
  }) |>
    list_rbind()
  
  return(result)

  
}

value_at_lags = map(ref_dates, get_val_asof, 
  archive=nchs_archive, offsets=offsets) |>
  list_rbind()
  
values_final = epix_as_of(nchs_archive, max(nchs_archive$versions_end))


ggplot(value_at_lags,
       aes(x = time_value, y = value)) +  
  geom_line(aes(color = factor(lag))) + 
  # geom_vline(aes(color = factor(version), xintercept = version), lty = 3) +
  facet_wrap(~ geo_value, scales = "free_y", ncol = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "", y = "Weekly new COVID deaths") + 
  # scale_color_viridis_d(option="D", end=0.8) +
  theme(legend.position = "none") +
  geom_line(data = values_final,
            aes(x = time_value, y = value), 
            inherit.aes = FALSE, color = "black")



```

# Backfill projection in Epiverse


## Types of predictions

* Finalized value often not available until weeks/months later
* Want to 'project' the [**final**]{.primary} value given the provisional [**real-time**]{.primary} value.

  Forecasting
: At time $t$, predict the final value for time $t+h$, $h > 0$
  
  <br>
  
  Backcasting
: At time $t$, predict the final value for time $t-h$, $h < 0$

  <br>
  
  Nowcasting
: At time $t$, predict the final value for time $t$


## Nowcasting via simple ratio: NCHS mortality 

* [**NHCS mortality data**]{.primary} contains a versioned history of number of weekly new deaths with confirmed or presumed COVID-19.

<br>


* We use this data to demonstrate the nowcasting and backcasting.

<br>


* The goal is to project [**nowcast the mortality rate**]{.primary} based on provisional/real-time data. 
  
## Nowcasting via simple ratio: NCHS mortality

* Specifically, we will make nowcasts from 2022-01 to 2024-01. 
* We will take the remainder of data as training set and for exploratory analysis. 
  
  
## Fetch versioned data

Let's fetch versioned mortality data from the API (`pub_covidcast`) for CA (`geo_values = "ca"`) and the signal of interest (`deaths_covid_incidence_num`) for all the dates where versioned data is recorded.
```{r mortality-archive-construct}
#| echo: true

# Fetch the training archive 
nchs_train_archive = pub_covidcast(
  source = "nchs-mortality",
  signal = "deaths_covid_incidence_num",
  geo_type = "state",
  time_type = "week",
  geo_values = "ca",
  time_values = epirange(202108, 202152),
  issues = "*"
) |>
  select(geo_value, time_value, version = issue, mortality = value) |>
  as_epi_archive(compactify=TRUE)


# Fetch the entire versioned history (train + test set)
nchs_fullarchive <- pub_covidcast(
  source = "nchs-mortality",
  signals = "deaths_covid_incidence_num",
  geo_type = "state",
  time_type = "week",
  geo_values = "ca",  # California (CA)
  time_values = epirange(202108, 202440),  
  issues = "*"
) |> 
  select(geo_value, time_value, version = issue, mortality = value) |> 
  as_epi_archive(compactify = TRUE)

```



## Exploratory analysis: latency in reporting 

* We inspect the latency (how long it takes to have any reported value) on our training set.
* Why only looking at training set? Because at time of nowcast, only have information before the nowcast date.
* Otherwise we will be using information that would not have been available. 

```{r inspect-latency-dplyr-way}
#| echo: true
nchs_train_archive$DT |> group_by(geo_value, time_value) |>
  filter(version == min(version)) |>
  mutate(lag = version - time_value) |>
  slice_head() |>
  head()
```



## Exploratory analysis - Finalized value attainment

* [**Question:**]{.primary} When is the [**finalized value**]{.primary} first attained for each date? Would we have access to any in real-time?
* How fast are the final values attained & what's the pattern for these times, if any?


```{r finalized-value-first-attained-fun}
#| echo: false
check_when_finalized <- function(epi_archive, start_date = NULL, end_date = NULL) {
  # Extract the mortality archive data
  dt <- epi_archive$DT
  
  # Extract the latest (finalized) version
  mortality_latest <- epix_as_of(epi_archive, max(dt$version))
  
  # Merge the finalized mortality data with all versions
  merged_data <- dt |>
    filter(geo_value %in% mortality_latest$geo_value &
             time_value %in% mortality_latest$time_value) |>
    inner_join(mortality_latest, by = join_by(geo_value, time_value), suffix = c("", "_finalized"))
  
  # Find the minimal version where the finalized mortality first occurred
  finalized_version_data <- merged_data |>
    filter(mortality == mortality_finalized) |>
    group_by(geo_value, time_value) |>
    summarize(min_version = min(version), .groups = 'drop') |>
    mutate(diff = min_version - time_value)
  
  return(finalized_version_data)
}
```

```{r check-when-finalized-run}
#| echo: false
res <- check_when_finalized(nchs_train_archive, start_date = start_time, end_date = end_time)
head(res)
```
We can also look at some quantiles: 
```{r summary-diff}
#| echo: false
summary(as.numeric(res$diff))
```

<br>

It generally takes long to attain finalized value. 


## Comparison of final vs. multiple revisions

The following plots gives a more direct message on how the values are revised. 

```{r mortality-by-revision-date}
#| echo: false

ref_dates = unique(nchs_train_archive$DT$time_value)
offsets = seq(1, 7) * 7

get_val_asof = function(time_val, archive, offsets) {
  
  as_of_dates = pmin(time_val + offsets, max_version)
  
  result = map(as_of_dates, function(x) {
    
    qd = archive |>
      epix_as_of(x) |>
      filter(time_value == time_val) |>
      select(geo_value, time_value, mortality) |>
      mutate(lag = x - time_val)
  }) |>
    list_rbind()
  
  return(result)

  
}

value_at_lags = map(ref_dates, get_val_asof, 
  archive=nchs_train_archive, offsets=offsets) |>
  list_rbind()
  
values_final = epix_as_of(nchs_train_archive, max(nchs_train_archive$versions_end))

```

```{r final-vs-revisions-plot}
#| echo: false
#| fig-width: 9
#| fig-height: 4
#| out-height: "500px"
ggplot(value_at_lags, aes(x = time_value, y = mortality)) +  
  geom_line(aes(color = factor(lag))) + 
  facet_wrap(~ geo_value, scales = "free_y", ncol = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "", y = "Weekly new COVID deaths") + 
  # scale_color_viridis_d(option="D", end=0.8) +
  theme(legend.position = "none") +
  geom_line(data = values_final, aes(x = time_value, y = mortality), 
            inherit.aes = FALSE, color = "black")


```

## Ratio estimator: jumping from provisional to finalized data

* A simple nowcaster is the ratio between finalized value and provisional value. 

* Don't know value of any given week is finalized. Need a working approximation. 

* From previous plot, we also see data published 49 days later is pretty close to the finalized value. 

* We want to use the most recent data available, since that is likely to be most informative.


## Ratio estimator: jumping from provisional to finalized data

* As a demonstration, we first do this for a single nowcast date: 2022-01-02 (the first date in 2022).

```{r}
#| echo: true

nowcast_date = as.Date("2022-01-02"); window_length = 90

# First step: estimate the ratio based on some past data
finalized_data = epix_as_of(nchs_fullarchive, nowcast_date) |>
  filter(time_value >= nowcast_date - 49 - window_length & time_value <= nowcast_date - 49) |>
  rename(finalized_val = mortality) |>
  select(geo_value, time_value, finalized_val)

initial_data = nchs_fullarchive$DT |>
  filter(time_value %in% finalized_data$time_value) |>
  group_by(geo_value, time_value) |>
  filter(version == min(version)) |>
  rename(initial_val = mortality) |>
  select(geo_value, time_value, initial_val)

ratio = finalized_data |>
  inner_join(initial_data, by = c("geo_value", "time_value")) |>
  mutate(ratio = finalized_val / initial_val) |>
  pull(ratio) |>
  mean()

# Recall latency! Can only project based on latest reported data
# Given the ratio, we can multiply that with the last available reported value
last_avail = epix_as_of(nchs_fullarchive, nowcast_date) |>
  slice_max(time_value) |>
  pull(mortality) 

nowcast = last_avail * ratio
nowcast
```

## Ratio estimator: sliding the computation

One nowcast is not enough! We need to slide that computation across all the nowcasts. 

```{r}
#| echo: true
#| 
nowcaster = function(x, g, t, wl=90, appx=49) {
  
  finalized_data = x$DT |>
    group_by(geo_value, time_value) |>
    filter(version ==  max(version)) |>
    filter(time_value >= t - wl - appx & time_value <= t - appx) |>
    rename(finalized_val = mortality) |>
    select(geo_value, time_value, finalized_val)
  
  
  initial_data = x$DT |>
    group_by(geo_value, time_value) |>
    filter(version ==  min(version)) |>
    filter(time_value >= t - wl - appx & time_value <= t - appx) |>
    rename(initial_val = mortality) |>
    select(geo_value, time_value, initial_val)
  
  ratio = finalized_data |>
    inner_join(initial_data, by = c("geo_value", "time_value")) |>
    mutate(ratio = finalized_val / initial_val) |>
    pull(ratio) |>
    mean(na.rm=TRUE)

  last_avail = epix_as_of(x, t) |>
    slice_max(time_value) |>
    pull(mortality) 
  
  res = tibble(target_date = t, nowcast = last_avail * ratio)
  
  return(res)
  
}
```


## Ratio estimator: sliding the computation


```{r}
#| echo: true

all_nowcast_dates = nchs_fullarchive$DT |>
  filter(time_value > as.Date("2021-12-31")) |>
  distinct(time_value) |>
  pull(time_value)

nowcasts = epix_slide(
  nchs_fullarchive,
  nowcaster,
  .before=Inf,
  .versions = all_nowcast_dates,
  .all_versions = TRUE
)


tmp = epix_as_of(nchs_fullarchive, as.Date("2022-04-10")) |>
  slice_max(time_value)

```


## Calculating the ratio using multiple dates
Now, let's plot the real-time vs the finalized mortality rates:
```{r real-time-vs-finalized}
ggplot() +
  geom_line(data = mortality_latest |> filter(time_value <= (as_of_date - 7)), 
            aes(x = time_value, y = mortality, color = "Finalized")) +
  geom_line(data = mortality_real_time_df, 
            aes(x = time_value, y = mortality, color = "Real-Time")) +
  ylab("Mortality") +
  xlab("Date") + 
  scale_color_manual(values = c("Finalized" = "black", "Real-Time" = "red")) + 
  labs(color = "Mortality Type")  # Adds the legend title

```
* [**Takeaways**]{.primary}: The real-time counts are biased [**well below**]{.primary} the finalized counts.
* Systematic underreporting tends to lessen over time (the gap between the lines decreases).

## Realistic limitation of nowcasting - Finalized data
* Recall that real-time access to finalized data is limited as finalized values can take months to report (e.g., Jan. 7 is finalized 294 days later).
* To nowcast accurately, we must rely on the [**best available approximation of finalized data**]{.primary} at the time of estimation (Feb. 25).

```{r finalized-data-as-of-feb25}
#| echo: true
mortality_as_of_feb25 <- epix_as_of(mortality_archive, as_of_date)
head(mortality_as_of_feb25)
```

## Ratio calculation & summary

We then use these "finalized" and real-time values to compute the mean ratio:
```{r ratio-calc-summary}
#| echo: true
# exclude date we're nowcasting for
mortality_real_time_df = mortality_real_time_df |> filter(time_value != "2024-02-18") 
mortality_as_of_feb25 = mortality_as_of_feb25 |> filter(time_value != "2024-02-18")
ratio_real_time_to_feb25 <- mortality_real_time_df$mortality / mortality_as_of_feb25$mortality
summary(ratio_real_time_to_feb25)
```
On average, the real-time rates are ~25.7% of the finalized.

```{r boxplot-ratio}
#| echo: false
ratio_df <- data.frame(ratio_real_time_to_feb25, mean_ratio = mean(ratio_real_time_to_feb25))

# Create the boxplot with mean marked as a bold cross
ggplot(ratio_df, aes(y = ratio_real_time_to_feb25)) +
  geom_boxplot(fill = "lightblue", color = "black") +
  geom_point(
    aes(x = 0, y = mean_ratio),  # Place point at the mean
    shape = 4,                  # Cross shape
    size = 7,                   # Size of the cross
    color = "darkblue",         # Color of the cross
    stroke = 2                  # Boldness of the cross
  ) +
  labs(
    title = "Distribution of Real-Time to Finalized Mortality Ratios",
    y = "Real-Time to Finalized Ratio"
  ) +
  theme_minimal() +  # Minimal theme for clean look
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.text.x = element_blank()
  ) +
  coord_cartesian(ylim = c(0.2, 0.3))  # Limit y-axis between 0 and 0.5 to zoom in
```
Tells us the distribution is right-skewed (mean > median) and so we should opt for the median.


## Nowcasting on Feb. 25

* Since the [**median ratio**]{.primary} between real-time and finalized values is [**0.250**]{.primary} (i.e., real-time values are typically 25% of the finalized), then the nowcast is
```{r nowcast-feb25-point}
#| echo: true
# Now we can nowcast properly:
nowcast <- provisional *
  1 / median(ratio_real_time_to_feb25)
nowcast
```

* To get the accompanying 95% prediction interval, calculate the 2.5th and 97.5th percentiles:

```{r nowcast-feb25-lower}
#| echo: true
percentile_97.5 <- quantile(ratio_real_time_to_feb25, 0.975) |> unname()

(lower_PI <- provisional * 1 / percentile_97.5)
```

```{r nowcast-feb25-upper}
#| echo: true
percentile_2.5 <- quantile(ratio_real_time_to_feb25, 0.025) |> unname()
(upper_PI <- provisional * 1 / percentile_2.5)
```

* So, the [**nowcast is 92**]{.primary} with 95% PI: [61, 140], which is much closer to the [**finalized value of 104**]{.primary} than the [**provisional value of 23**]{.primary}.

## Summary of three main steps 
So the main steps for this type of fixed lag nowcasting are...

1. Obtain the [**provisional value**]{.primary} for the target.

2. Estimate the ratio using the [**real-time**]{.primary} and [**"finalized"**]{.primary} data (for all previous dates that follow a consistent pattern in reporting). 

3. Profit.

```{r ratio-summary-steps}
#| echo: true
#| eval: false
#| code-fold: true
#| code-summary: "Expand for the accompanying code"
# Today
as_of_date = as.Date("2024-02-25")

# 1. Obtain the provisional value
provisional <- epix_as_of(mortality_archive, as_of_date) |>
  filter(time_value == as_of_date - 7) |>
  pull(mortality)
provisional

# 2. Estimate the ratio 
mortality_real_time_df <- map_dfr(dates, mortality_real_time) |> filter(time_value != "2024-02-18") # Real-time
mortality_as_of_feb25 <- epix_as_of(mortality_archive, as_of_date) |> filter(time_value != "2024-02-18")  # "Finalized"

ratio_real_time_to_feb25 <- mortality_real_time_df$mortality / mortality_as_of_feb25$mortality

# 3. Profit.
(nowcast <- provisional * 1 / median(ratio_real_time_to_feb25))

(upper_PI <- provisional * 1 / quantile(ratio_real_time_to_feb25, 0.025))
(lower_PI <- provisional * 1 / quantile(ratio_real_time_to_feb25, 0.975))

```

## Nowcasting mortality for multiple dates

* [**Define Nowcast Function**]{.primary}:
  * [**Input**]{.primary}: Takes in the dates to nowcast and the fixed lag
  * [**Output**]{.primary}: The nowcasted mortality rates based on the ratio of real-time to finalized data.
```{r nowcasting-multipl-dates-fun}
#| echo: true
#| code-fold: true
nowcast_function <- function(nowcast_date, fixed_lag) {
  as_of_date = nowcast_date + fixed_lag
  
  # 1. Obtain the provisional value for the target.
  provisional <- epix_as_of(mortality_archive, as_of_date) |>
    filter(time_value == as_of_date - fixed_lag) |>
    pull(mortality)
  
  #2. Estimate the ratio multiplier using
  # real-time
  dates_seq <- seq(start_time, (nowcast_date - fixed_lag), by = "week")
  mortality_real_time <- map_dfr(dates_seq, mortality_real_time)
  
  # and "finalized" data
  finalized <- epix_as_of(mortality_archive, as_of_date) |> filter(time_value >= start_time & time_value <= (nowcast_date - fixed_lag)) 
  
  ratios <- mortality_real_time$mortality / finalized$mortality
  
  # Remove infinite or NaN ratios (i.e., keep only finite values)
  median_ratio <- median(ratios[is.finite(ratios)])
  
  #3. Profit.
  nowcast <- provisional * (1 / median_ratio)
  upper_PI <- provisional * (1 / quantile(ratios[is.finite(ratios)], 0.025))
  lower_PI <- provisional * (1 / quantile(ratios[is.finite(ratios)], 0.975))
  
  # Return a dataframe with the nowcast and date
  tibble(
    time_value = nowcast_date,
    nowcast = nowcast,
    lower_PI = lower_PI,
    upper_PI = upper_PI 
  )
}
```

## Map nowcast over multiple dates
* We can use `map2()` to apply the function to a series of weeks (e.g., Jan. 28 to Mar. 24).
* Returns a [**dataframe**]{.primary} with nowcasted results.

```{r map-nowcast-fun-multiple-dates}
#| echo: true
# Apply Nowcast Function Over Multiple Dates
nowcast_dates <- seq(as.Date("2024-01-28"), as.Date("2024-03-24"), by = "week")
fixed_lag <- 7
nowcast_results_df <- map2(nowcast_dates, fixed_lag, nowcast_function) |> list_rbind()
```

Let's smooth with a rolling trailing mean (window size 4) & see the results:

```{r smooth-print-res}
# Smooth results: Apply rolling median to nowcast and bounds with a window size of 4
library(zoo)
nowcast_results_df <- nowcast_results_df |> 
  mutate(across(.cols = -time_value,  
                .fns = ~ zoo::rollapply(.x, width = 4, FUN = mean, partial = TRUE, align = "right"),
                .names = "{.col}"))
nowcast_results_df
```

## Visualize nowcast, real-time, and finalized values
Finally, we can compare these nowcast results to the real-time and finalized values:
```{r nowcast-fun-plot-results}
#| echo: false
ggplot() + 
  geom_line(data = nowcast_results_df, aes(x = time_value, y = nowcast, color = "Nowcast")) +
  geom_point(data = nowcast_results_df, aes(x = time_value, y = nowcast, color = "Nowcast"), shape = 16) +
  geom_line(data = map_dfr(nowcast_dates, mortality_real_time), aes(x = time_value, y = mortality, color = "Real-Time")) +
  geom_point(data = map_dfr(nowcast_dates, mortality_real_time), aes(x = time_value, y = mortality, color = "Real-Time"), shape = 16) +
  geom_line(data = mortality_latest |> filter(time_value %in% nowcast_dates), aes(x = time_value, y = mortality, color = "Finalized")) +
  geom_point(data = mortality_latest |> filter(time_value %in% nowcast_dates), aes(x = time_value, y = mortality, color = "Finalized"), shape = 16) +
  geom_ribbon(data = nowcast_results_df, 
              aes(x = time_value, ymin = lower_PI, ymax = upper_PI), 
              alpha = 0.2, fill = "blue") +
  ylab("Mortality") +
  xlab("Date") +
  scale_color_manual(values = c("Nowcast" = "blue", "Real-Time" = "red", "Finalized" = "black")) + 
  labs(color = "Mortality Type")  # Adds the legend title
```
The real-time counts tend to be biased below the finalized counts. Nowcasted values tend to provide a much better approximation of the truth (at least for these dates).

## Evaluation using MAE

* Assume we have prediction $\hat y_{t}$ for the provisional value at time $t$.

* Then for $y_{t}$ over times $t = 1, \dots, N$, then we may compute error metrics like mean absolute error (MAE).

<!-- We'll see other error measures later on! For now, let's start with one that is simple and easy to interpret.-->

* MAE measures the average absolute difference between the nowcast and finalized values. 

$$MAE = \frac{1}{N} \sum_{t=1}^N |y_{t}- \hat y_{t}|$$

* Note that it's scale-dependent, meaning it can vary depending on the units of the data (e.g., cases, deaths, etc.).

## Evaluation using MAE

Let's numerically evaluate our point nowcasts for the provisional values of a time series (e.g., COVID-19 mortality) using MAE.

<!-- Accuracy of nowcast is assessed by how close provisional estimates are to the finalized values to gauge the model's performance. -->

```{r mae-code}
#| echo: true
# Step 1: Join the mortality data with nowcast data
mae_data <- mortality_latest |> 
  filter(time_value %in% nowcast_dates) |>   
  left_join(nowcast_results_df, by = "time_value") |> 
  left_join(map_dfr(nowcast_dates, mortality_real_time) |> rename(real_time = mortality), by = c("geo_value", "time_value"))

# Step 2: Calculate the absolute error between actual and nowcasted values
mae_data <- mae_data |> 
  mutate(nc_abs_error = abs(mortality - nowcast),
         rt_abs_error = abs(mortality - real_time))  

# Step 3: Compute the MAE (mean of absolute errors)
mae_value <- mae_data |> 
  summarise(nc_MAE = mean(nc_abs_error),
            rt_MAE = mean(rt_abs_error))
knitr::kable(mae_value)
```

## Evaluation using MAE

Finally, we may visualize the distribution of errors across time:
```{r abs-error-plot}
ggplot(mae_data) +
  # Nowcast Absolute Error 
  geom_point(aes(x = time_value, y = nc_abs_error), color = "blue", size = 2) +
  geom_line(aes(x = time_value, y = nc_abs_error), color = "blue") +
  # Horizontal line for Nowcast Mean Absolute Error
  geom_hline(aes(yintercept = mean(nc_abs_error), color = "Nowcast MAE"), linetype = "dashed") +
  
  # Real-Time Absolute Error 
  geom_point(aes(x = time_value, y = rt_abs_error), color = "red", size = 2) +
  geom_line(aes(x = time_value, y = rt_abs_error), color = "red") +
  # Horizontal line for Real-Time Mean Absolute Error
  geom_hline(aes(yintercept = mean(rt_abs_error), color = "Real-Time MAE"), linetype = "dashed") +
  
  # Customize the x and y axes labels, legend & add title
  xlab("Time") + ylab("Absolute Error") +
  scale_color_manual(values = c("Real-Time MAE" = "red", "Nowcast MAE" = "blue")) +
  labs(color = "Mean Absolute Error")
```

We can see that the absolute errors are almost always lower for nowcasting.

# Nowcasting with auxilliary variables 

## Mathematical setup

* **Nowcasting**: Predict a finalized value from a provisional value.

* Suppose today is time $t$

* Let $y_i$ denote a series of interest observed at times $i=1,\ldots, t$.

::: {.callout-important icon="false"}
## Our goal

* Produce a [**point nowcast**]{.primary} for the finalized values of $y_t$.
* Accompany with time-varying prediction intervals

:::

* We also have access to $p$ other time series 
$x_{ij},\; i=1,\ldots,t, \; j = 1,\ldots,p$

* All may be subject to revisions.




## Nowcasting: Moving from one signal to two

* Recall that in nowcasting the goal is to predict a finalized value from a provisional value.
* Now, we'll move from one signal to two, creating a simple linear model to nowcast.
* Exogenous features (predictors) could include relevant signals, such as Google symptom search trends.
* We will use these signals to nowcast hospital admissions related to influenza.


## Data Sources: Google searches & hospital admissions

* [**Google Search Trends**]{.primary}: Symptoms like cough, fever, and shortness of breath.
  * [**s01**]{.primary}: Cough, Phlegm, Sputum, Upper respiratory tract infection  
  * [**s02**]{.primary}: Nasal congestion, Post nasal drip, Sinusitis, Common cold

* [**Hospital Admissions**]{.primary}: Data from the Department of Health & Human Services on confirmed influenza admissions.

* Using these, we will [**nowcast**]{.primary} hospital admissions by using Google symptom search trends for GA from April to June 2023.

* The first step is to fetch this data...

## Data Sources: Google searches & hospital admissions

```{r fetch-google-data}
#| echo: true
# Fetch Google symptom data for s01 and s02
x1 <- pub_covidcast(
  source = "google-symptoms",
  signals = "s01_smoothed_search", 
  geo_type = "state",
  time_type = "day",
  geo_values = "ga",
  time_values = epirange(20230401, 20230701),
  issues = "*"
) |>
  select(geo_value, time_value, version = issue, avg_search_vol_s01 = value) |>
  as_epi_archive(compactify = FALSE)

x2 <- pub_covidcast(
  source = "google-symptoms",
  signals = "s02_smoothed_search",
  geo_type = "state",
  time_type = "day",
  geo_values = "ga",
  time_values = epirange(20230401, 20230701),
  issues = "*"
) |>
  select(geo_value, time_value, version = issue, avg_search_vol_s02 = value) |>
  as_epi_archive(compactify = FALSE)

# Fetch hospital admissions data
y1 <- pub_covidcast(
  source = "hhs",
  signals = "confirmed_admissions_influenza_1d",
  geo_type = "state",
  time_type = "day",
  geo_values = "ga",
  time_values = epirange(20230401, 20230701),
  issues = "*"
) |>
  select(geo_value, time_value, version = issue, admissions = value) |>
  as_epi_archive(compactify = FALSE)
```

## Merging the archives

* We'll merge the symptom search trends (`x1`, `x2`) with hospital admissions data (`y`) using `epix_merge()` from `epiprocess`.
* This allows us to match data by time and geography, & fill any missing values with the most recent observation (LOCF).

```{r merge-archives-two-pred}
#| echo: false
# Merge the Google trends data (x1, x2) with hospital admissions data (y)
archive <- epix_merge(x1, y1, sync = "locf", compactify = FALSE)
archive <- epix_merge(archive, x2, sync = "locf", compactify = FALSE)
```

## Linear Model: A simple approach for nowcasting

* Aside from ratios, one of the simplest approach to nowcasting is to use a [**linear regression model**]{.primary}.
* We model the relationship between provisional (predictor) data and response data.
* This model helps us make [**predictions**]{.primary} for the finalized data based on the current (provisional) signals.

## Linear regression
* [**Goal**]{.primary}: Estimate the coefficients $\beta_0$ and $\beta_1$ that describe the relationship between the predictor $x_i$ and the outcome $y_i$.
* [**Linear Model**]{.primary}: The relationship is assumed to be:

  $$y_i \approx \beta_0 + \beta_1 x_i $$
  
  where
  $\beta_0$ is the intercept,
  $\beta_1$ is the slope.
* **In R**: Use `lm(y ~ x)` to estimate the coefficients, where `y` is the outcome variable and `x` is the predictor.

## Multiple linear regression
* [**Goal**]{.primary}: Estimate coefficients $\beta_0, \beta_1, \dots, \beta_p$ that describe the relationship between multiple predictors $x_{i1}, x_{i2}, \dots, x_{ip}$ and the outcome $y_i$.
* [**Model**]{.primary}: The relationship is assumed to be:

  $$y_i \approx \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}$$
  
  where:
  $\beta_0$ is the intercept,
  $\beta_1, \dots, \beta_p$ are the coefficients.
* [**In R**]{.primary}: Use `lm(y ~ x1 + x2 + ... + xp)` to estimate the coefficients, where `y` is the outcome and `x1, x2, ..., xp` are the predictors.

## Multiple linear regression model

* A linear model is a good choice to describe the relationship between search trends and hospital admissions.
* The model will include two predictors (s01 and s02).
* We'll use these two search trend signals to predict hospital admissions (response).

<!-- A linear regression model will be used to predict hospital admissions from search trends (s01 and s02). -->

## Multiple linear regression model
```{r multiple-lr-mod-predict}
#| echo: true
# Define the function for lm model fit and prediction
lm_mod_pred <- function(data, gk, rtv, ...) {
  
  # Fit the linear model
  model <- lm(admissions ~ avg_search_vol_s01 + avg_search_vol_s02, data = data)
  
  # Make predictions
  predictions = predict(model,
                        newdata = data |>
                          # Use tidyr::fill() for LOCF if predictor data is incomplete 
                          fill(avg_search_vol_s01, .direction = "down") |> 
                          fill(avg_search_vol_s02, .direction = "down") |>
                          filter(time_value == max(time_value)),
                        interval = "prediction", level = 0.9
  )

  # Pull off true time value for comparison to target
  real_time_val = data |> filter(time_value == max(time_value)) |> pull(admissions)

  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val))
}
```
Note that this code is intentionally simple; while it can be refined to handle cases like negatives or other boundary conditions, we aim to avoid unnecessary complexity.

## Nowcasting with `epix_slide()`

* We will use `epix_slide()` to create a sliding window of training data.
* The model will be trained on a 14-day window before the target date, and predictions will be made for the target date.
* The beauty of this function is that it is version-aware - the sliding computation at any given reference time [**t**]{.primary} is performed on data that would have been available as of [**t**]{.primary} automatically. 

## Nowcasting with `epix_slide()`
```{r nowcasting-epix-slide}
#| echo: true
# Define the reference time points for nowcasting
targeted_nowcast_dates <- seq(as.Date("2023-04-15"), as.Date("2023-06-15"), by = "1 week")
ref_time_values = targeted_nowcast_dates + 2  # Adjust for the systematic 2-day latency in the response
# Determine this from revision_summary(y1, print_inform = TRUE) 

# Perform nowcasting using epix_slide
nowcast_res <- archive |>
  group_by(geo_value) |>
  epix_slide(
    .f = lm_mod_pred,
    .before = 14,  # 14-day training period
    .versions = ref_time_values, 
    .new_col_name = "res"
  ) |>
  unnest() |> # Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns. 
  mutate(targeted_nowcast_date = targeted_nowcast_dates, time_value = actual_nowcast_date) |>
  ungroup()

# View results
head(nowcast_res, n=2)
```

## Compare with the actual admissions 

After making predictions, we compare them to the actual hospital admissions.

```{r join-with-actual-admissions}
#| echo: true
# Left join with latest results 
# Latest snapshot of data (with the latest/finalized admissions)
x_latest <- epix_as_of(archive, max(archive$DT$version)) |> select(-c(avg_search_vol_s01, avg_search_vol_s02))

res <- nowcast_res |> left_join(x_latest, by = join_by(geo_value, time_value))
head(res)
```

## Visualizing the nowcast results

We can then visualize the nowcast results alongside the true values using `ggplot2`:

```{r plot-multiple-lr-nowcast-res}
#| echo: false
# Plot the predictions vs real-time vs actual admissions
ggplot(res, aes(x = time_value)) +
  geom_line(aes(y = admissions, color = "Finalized Admissions"), size = 1.2) +
  geom_point(aes(y = fit, color = "Nowcast"), size = 3) +
  geom_point(aes(y = real_time_val, color = "Real Time"), size = 3) +
  geom_line(aes(y = fit, color = "Nowcast"), size = 1.2, linetype = "dashed") +
  geom_ribbon(aes(ymin = lwr, ymax = upr, fill = "Pred. Interval"), alpha = 0.3) +
  labs(title = "",
       x = "Date", y = "Hospital Admissions", color = "Legend", fill = "Legend") +
  scale_color_manual(values = c("Finalized Admissions" = "black", "Nowcast" = "#1f78b4", "Real Time" = "darkred")) +
  scale_fill_manual(values = c("Pred. Interval" = "#a6cee3")) +
  theme_minimal() +
  theme(legend.position = "top", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

## Evaluation using MAE

* As before, we can evaluate our point nowcasts numerically using MAE.

```{r mae-code-hosp}
#| echo: true
# Calculate the absolute error between actual and nowcasted values
mae_data_admissions <- res |> 
  mutate(nc_abs_error = abs(admissions - fit),  # Nowcast vs Finalized admissions
         rt_abs_error = abs(admissions - real_time_val))  # Real-Time vs Finalized admissions

# Compute the MAE (mean of absolute errors)
mae_value_admissions <- mae_data_admissions |> 
  summarise(nc_MAE = mean(nc_abs_error),
            rt_MAE = mean(rt_abs_error))
knitr::kable(mae_value_admissions)
```

* Based off of comparing these simple error measures, the nowcast MAE is clearly better.

## Evaluation using MAE

However, when we visualize the distribution of errors across time, it is not so cut-and-dry:
```{r abs-error-plot-hosp}
# Plot the absolute errors for Nowcast and Real-Time vs Finalized Admissions
ggplot(mae_data_admissions) +
  # Nowcast Absolute Error 
  geom_point(aes(x = time_value, y = nc_abs_error), color = "#1f78b4", size = 2) +
  geom_line(aes(x = time_value, y = nc_abs_error), color = "#1f78b4") +
  # Horizontal line for Nowcast Mean Absolute Error
  geom_hline(aes(yintercept = mean(nc_abs_error), color = "Nowcast MAE"), linetype = "dashed") +
  
  # Real-Time Absolute Error 
  geom_point(aes(x = time_value, y = rt_abs_error), color = "darkred", size = 2) +
  geom_line(aes(x = time_value, y = rt_abs_error), color = "darkred") +
  # Horizontal line for Real-Time Mean Absolute Error
  geom_hline(aes(yintercept = mean(rt_abs_error), color = "Real-Time MAE"), linetype = "dashed") +
  
  # Customize the x and y axes labels, legend & add title
  xlab("Time") + ylab("Absolute Error") +
  scale_color_manual(values = c("Real-Time MAE" = "darkred", "Nowcast MAE" = "#1f78b4")) +
  labs(color = "Mean Absolute Error") +
  theme_minimal() +
  theme(legend.position = "top", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

The main driver behind the real-time MAE being greater is the "outlier-like" May 25 AE.

So visualizing can provide an important perspective that is missed from a simple numerical summary of error.

## Key Takeaways: Linear regression nowcasting example

* [**Provisional Data as Predictors**]{.primary}: Using [Google symptom search trends]{.primary} to predict [influenza hospital admissions]{.primary}.
* [**Simple Linear Model**]{.primary}: A linear regression model captures the relationship between symptom searches and hospital admissions.
* [**Actionable Predictions**]{.primary}: Nowcasts provide [timely insights]{.primary} for hospital admissions, even before data is finalized.
* [**Sliding Window Approach**]{.primary}: Predictions are based on [data up to the current time]{.primary}, ensuring no future information influences the nowcast.
* [**Evaluation**]{.primary}: Predictions are compared with actual admissions using numerical and visual perspectives.

# Case Study - Nowcasting Cases Using %CLI 

## Goal of this case study

[**Goal**]{.primary}: Nowcast COVID-19 Cases for MA using the estimated percentage of COVID-related doctor's visits (%CLI), based on outpatient data from Optum.

* %CLI is contained in the Epidata API.
* Cases by specimen collection date are not. They are from the MA gov website.
* Cases in the API (JHU) are aligned by report date, not specimen collection/test date.
* Working with cases aligned by [**test date**]{.primary} allows us to avoid the more unpredictable delays introduced by the [**report date**]{.primary}.

## Summary of main steps

The workflow is similar to the previous example where we nowcasted using two variables, only more involved. 
The main steps are...

1. [**Fetch Data**]{.primary}: Retrieve %CLI and COVID-19 case data (by specimen collection date) for MA.

2. [**Merge Data**]{.primary}: Align %CLI and case data using `epix_merge`, filling missing values via last observation carried forward (LOCF).

3. [**Model & Prediction**]{.primary}: Fit a linear model to predict cases based on %CLI, trained on a 30-day rolling window.

4. [**Nowcast Execution**]{.primary}: Use `epix_slide` to nowcast the cases dynamically. 

5. [**Visualization**]{.primary}: Plot actual vs. nowcasted cases with confidence intervals to assess model accuracy.

So the first step is to fetch the data...

## Construct an `epi_archive` from scratch

[Here's]("https://www.mass.gov/info-details/archive-of-covid-19-cases-2020-2021") the archive of COVID-19 case excel files from the MA gov website, which we'll use to construct our own `epi_archive`.
<br>
<br>
Brief summary of this data:

* [**First release**]{.primary}: Raw .xlsx data was first released early January 2021.

* [**Change in reporting**]{.primary}: Starting [**July 1, 2021**]{.primary}, the dashboard shifted from [**7 days/week**]{.primary} to [**5 days/week**]{.primary} (Monday-Friday).

* [**Friday, Saturday, and Sunday**]{.primary} data is included in the [**Monday**]{.primary} dashboard.

* When [**Monday**]{.primary} is a holiday, the [**Friday through Monday**]{.primary} data is posted on [**Tuesday**]{.primary}.


## Construct an `epi_archive` from scratch

* [**Purpose**]{.primary}: To create an `epi_archive` object for storing versioned time series data.
* [**Required Columns**]{.primary}:
  * `geo_value`: Geographic data (e.g., region).
  * `time_value`: Time-related data (e.g., date, time).
  * `version`: Tracks when the data was available (enables version-aware forecasting).
* [**Constructor**]{.primary}:
  * `new_epi_archive()`: For manual construction of `epi_archive` (assumes validation of inputs).
* [**Recommended Method**]{.primary}:
  * `as_epi_archive()`: Simplifies the creation process, ensuring proper formatting and validation. We'll use this one when we download some data from the MA gov website!


## Main steps to construct the `epi_archive`

1. [**Load necessary Libraries**]{.primary}: Such as `tidyverse`, `readxl`, `epiprocess`.
2. [**Process Each Date's Data**]{.primary}: 
   * A function we'll make (`process_covid_data`) downloads and processes daily COVID-19 data from the MA gov Excel files on their website.
   * The data is cleaned and formatted with columns: `geo_value`, `time_value`, `version`, and values.
3. [**Handle Missing Data**]{.primary}: Checks if a date's data is available (handle 404 errors).
4. [**Create `epi_archive`**]{.primary}: 
   * Combine processed data into a tibble.
   * Convert the tibble to an `epi_archive` object using `as_epi_archive()`.


## Fetch Data - Code for one date
```{r fetch-web-data-one-date}
#| echo: true
# Load required libraries
library(tidyverse)
library(readxl)
library(httr)
library(tibble)
library(epiprocess)

# Function to download and process each Excel file for a given date
process_covid_data <- function(Date) {
  # Generate the URL for the given date
  url <- paste0("https://www.mass.gov/doc/covid-19-raw-data-", tolower(gsub("-0", "-", format(Date, "%B-%d-%Y"))), "/download") 
  # Applies gsub("-0", "-", ...) to replace any occurrence of -0 (such as in "April-01") with just - (resulting in "April-1").
  
  # Check if the URL exists (handle the 404 error by skipping that date)
  response <- GET(url)
  
  if (status_code(response) != 200) {
    return(NULL)  # Skip if URL doesn't exist (404)
  }
  
  # Define the destination file path for the Excel file
  file_path <- tempfile(fileext = ".xlsx")
  
  # Download the Excel file
  GET(url, write_disk(file_path, overwrite = TRUE))
  
  # Read the relevant sheet from the Excel file
  data <- read_excel(file_path, sheet = "CasesByDate (Test Date)")
  
  # Process the data: rename columns and convert Date
  data <- data |>
    rename(
      Date = `Date`,
      Positive_Total = `Positive Total`,
      Positive_New = `Positive New`,
      Case_Average_7day = `7-day confirmed case average`
    ) |>
    mutate(Date = as.Date(Date))  # Convert to Date class
  
  # Create a tibble with the required columns for the epi_archive
  tib <- tibble(
    geo_value = "ma",  # Massachusetts (geo_value)
    time_value = data$Date,  # Date from the data
    version = Date,  # The extracted version date
    case_rate_7d_av = data$Case_Average_7day  # 7-day average case value
  )
  
  return(tib)
}
```

## Fetch Data - Code breakdown 

* This purpose of this function is to download and process each Excel file as of a date.
* [**URL Creation**]{.primary}: Dynamically generates the URL based on the date, removing leading zeros in day values (e.g., "April-01" → "April-1").
* [**Check URL**]{.primary}: Sends a request (`GET(url)`) and skips the date if the URL returns a non-200 status (e.g., 404 error).
* [**Download File**]{.primary}: Saves the Excel file to a temporary path using `tempfile()` and `GET()`.
* [**Read Data**]{.primary}: Loads the relevant sheet ("CasesByDate") from the Excel file using `read_excel()`.
* [**Tibble Creation**]{.primary}: Constructs a tibble with `geo_value`, `time_value`, `version`, and `case_rate_7d_av` to later compile into an `epi_archive` (you can think of an `epi_archive` as being a comprised of many `epi_df`s).


## Fetch Data - Process eange of dates
* Note that `process_covid_data()` works on one date at a time.
* So now, we need a function that iterates over a date range and applies `process_covid_data()` to each date & combines the resulting tibbles into an `epi_archive`.
* We call this function `process_data_for_date_range()`...

## Fetch Data - Process range of dates
```{r process-range-of-dates}
#| echo: true
# Function to process data for a range of dates
process_data_for_date_range <- function(start_date, end_date) {
  # Generate a sequence of dates between start_date and end_date
  date_sequence <- seq(as.Date(start_date), as.Date(end_date), by = "day")
  
  # Process data for each date and combine results
  covid_data_list <- lapply(date_sequence, function(Date) {
    process_covid_data(Date)  # Skip over dates with no data (NULLs will be ignored)
  })
  
  # Combine all non-null individual tibbles into one data frame
  combined_data <- bind_rows(covid_data_list[!sapply(covid_data_list, is.null)])
  
  # Convert the combined data into an epi_archive object
  if (nrow(combined_data) > 0) {
    epi_archive_data <- combined_data |>
      as_epi_archive(compactify = FALSE)
    
    return(epi_archive_data)
  } else {
    message("No valid data available for the given date range.")
    return(NULL)
  }
}
```

## Fetch Data - Code breakdown
Here's a summary of what `process_data_for_date_range()` does:
1. [**Generates Date Range**]{.primary}: Creates a sequence of dates between `start_date` and `end_date`.

2. [**Processes Data**]{.primary}: Applies the `process_covid_data` function to each date in the range (skip over dates with no data).

3. [**Combines Results**]{.primary}: Combines all valid (non-NULL) tibbles into one single data frame.

4. [**Creates `epi_archive`**]{.primary}: Converts the combined data into an `epi_archive` object.

## Fetch Data - Run the function & inspect archive

* Now, let's run the function & inspect the resulting `epi_archive` of 7-day avg. COVID-19 case counts:
* Expect building the archive to some time (enough for a cup of coffee or to meditate on life).

<!-- To wonder why you chose Expect building the archive to take a nontrivial amount of time (enough for a cup of coffee or to wonder why you chose coding in the first place). -->
```{r run-fun-process-data-range}
#| echo: true
# Example usage: process data between Jan. 10, 2021, and Dec. 1, 2021
# y <- process_data_for_date_range("2021-01-10", "2021-12-01")  # Raw .xlsx data is first released on Jan. 4, 2021
# y
```

* Alternatively, you may run the following to load `y` that was previously saved as an RDS file: 
```{r load-ma-case-data}
#| echo: true
#| eval: true
#| results: hide
y <- readRDS("_data/ma_case_archive.rds")
```


## Fetch Data - % Outpatient doctors visits for CLI

* Now, from the Epidata API, let's download the [estimated percentage of outpatient doctor visits]("https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/doctor-visits.html") primarily for COVID-related symptoms, based on health system data.
* Comes pre-smoothed in time using a Gaussian linear smoother
* This will be the predictor when we nowcast COVID-19 cases in MA.

```{r fetch-outpatient-cli}
#| echo: true
# Step 1: Fetch Versioned Data 
x <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  geo_type = "state",
  time_type = "day",
  geo_values = "ma", # Just for MA to keep it simple (& to go with the case data by test date for that state)
  time_values = epirange(20210301, 20211231),
  issues = epirange(20210301, 20211231)
) |>
  select(geo_value, time_value,
         version = issue,
         percent_cli = value
  ) |>
  as_epi_archive(compactify = FALSE)
```

## Use `epix_merge()` to merge the two archives
Now we'll use `epix_merge()` to combine the two `epi_archive`s that share the same `geo_value` & `time_value`.

<!-- LOCF is used to ensure missing data is handled by filling forward. -->
```{r merge-archives-one-pred}
#| echo: true
archive <- epix_merge(
  x, y,
  sync = "locf",
  compactify = FALSE
)
archive
```

## Fitting and predicting with linear model

* Define `lm_mod_pred()`: A function that fits a linear model to forecast cases based on the `percent_cli` predictor.
* Use `predict()` with a 90% prediction interval.
* Save the actual cases to compare to the nowcasts later.

```{r simple-lr-mod-predict}
#| echo: true
lm_mod_pred <- function(data, ...) {
  # Linear model
  model <- lm(case_rate_7d_av ~ percent_cli, data = data)

  # Make predictions
  predictions = predict(model,
                        newdata = data |>
                          fill(percent_cli, .direction = "down") |> 
                          filter(time_value == max(time_value)),
                        interval = "prediction", level = 0.9)
  
  # Pull off real-time value for later comparison to the nowcast value
  real_time_val = data |> filter(time_value == max(time_value)) |> pull(case_rate_7d_av)
  
  # Could clip predictions and bounds at 0
  return(data.frame(predictions, actual_nowcast_date = max(data$time_value), real_time_val = real_time_val)) 
}
```

## Nowcasting with `epix_slide()`
* [**Specify targets**]{.primary}: Define the target dates for nowcasting (e.g., 1st of each month) & adjust training data to include the lag for the latent case data.
* [**Sliding window**]{.primary}: Use `epix_slide()` to apply the linear model across a sliding window of data for each region.
* [**Training-test split**]{.primary}: Use the last 30 days of data to train and predict cases for each target nowcast date.

## Nowcasting with `epix_slide()`
```{r nowcasting-epix-slide-cases}
#| echo: true
# Define the reference time points (to give the training/test split)
targeted_nowcast_dates <- seq(as.Date("2021-04-01"), as.Date("2021-11-01"), by = "1 month") 
ref_time_values = targeted_nowcast_dates + 1 # + 1 because the case data is 1 day latent. 
# Determine this from revision_summary(y)

# Use epix_slide to perform the nowcasting with a training-test split
nowcast_res <- archive |>
  group_by(geo_value) |>
  epix_slide(
    .f = lm_mod_pred,  # Pass the function defined above
    .before = 30,   # Training period of 30 days
    .versions = ref_time_values, # Determines the day where training data goes up to (not inclusive)
    .new_col_name = "res"
  ) |>
  unnest() |>
  mutate(targeted_nowcast_date = targeted_nowcast_dates,
         time_value = actual_nowcast_date)

# Take a peek at the results
head(nowcast_res, n = 1)
```

## Visualizing nowcasts vs. actual values
Merge the nowcast results with the latest data for more direct comparison:

```{r join-with-actual-cases}
#| echo: true
x_latest <- epix_as_of(archive, max(archive$DT$version)) |>
  select(-percent_cli) 

res <- nowcast_res |> left_join(x_latest, by = join_by(geo_value, time_value))

res
```

## Visualizing nowcasts vs. actual values

Now, plot the predictions & real-time values on top of latest COVID-19 cases using `ggplot2`:

```{r plot-simple-lr-nowcast-res}
#| echo: false
ggplot(res, aes(x = time_value)) +
  geom_line(aes(y = case_rate_7d_av, color = "Finalized Cases (7-dav)"), size = 1.2) +
  geom_point(aes(y = fit, color = "Nowcast"), size = 3) +
  # Plot the real-time values
  geom_point(aes(y = real_time_val, color = "Real Time"), size = 3) +
  geom_line(aes(y = fit, color = "Nowcast"), size = 1.2, linetype = "dashed") +
  geom_ribbon(aes(ymin = lwr, ymax = upr, fill = "Pred. Interval"), alpha = 0.3) +
  # Title and labels
  labs(title = "",
       x = "Date",
       y = "Case Count",
       color = "Legend",
       fill = "Legend") +
  # Adjust colors
  scale_color_manual(values = c("Finalized Cases (7-dav)" = "black",
                                "Nowcast" = "#1f78b4",
                                "Real Time" = "darkred")) + 
  scale_fill_manual(values = c("Pred. Interval" = "#a6cee3")) + # Light blue
  # Improve the theme
  theme_minimal() +
  theme(
    legend.position = "top",
    legend.title = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

## Evaluation using MAE

* Finally, we numerically evaluate our nowcasts using MAE.

* Shows that the nowcast errors are lower than those of the real-time estimates.

```{r mae-code-cases}
#| echo: true
# Calculate the absolute error between actual and nowcasted COVID-19 cases
mae_data_cases <- res |> 
  mutate(nc_abs_error = abs(case_rate_7d_av - fit),  # Nowcast vs Finalized cases (7-day average)
         rt_abs_error = abs(case_rate_7d_av - real_time_val))  # Real-Time vs Finalized cases

# Compute the MAE (mean of absolute errors)
mae_value_cases <- mae_data_cases |> 
  summarise(nc_MAE = mean(nc_abs_error),
            rt_MAE = mean(rt_abs_error))
knitr::kable(mae_value_cases)
```


## Evaluation using MAE

```{r abs-error-plot-cases}
# Plot the absolute errors for Nowcast and Real-Time vs Finalized COVID-19 cases
ggplot(mae_data_cases) +
  # Nowcast Absolute Error 
  geom_point(aes(x = time_value, y = nc_abs_error), color = "#1f78b4", size = 2) +
  geom_line(aes(x = time_value, y = nc_abs_error), color = "#1f78b4") +
  # Horizontal line for Nowcast Mean Absolute Error
  geom_hline(aes(yintercept = mean(nc_abs_error), color = "Nowcast MAE"), linetype = "dashed") +
  
  # Real-Time Absolute Error 
  geom_point(aes(x = time_value, y = rt_abs_error), color = "darkred", size = 2) +
  geom_line(aes(x = time_value, y = rt_abs_error), color = "darkred") +
  # Horizontal line for Real-Time Mean Absolute Error
  geom_hline(aes(yintercept = mean(rt_abs_error), color = "Real-Time MAE"), linetype = "dashed") +
  
  # Customize the x and y axes labels, legend & add title
  xlab("Date") + ylab("Absolute Error") +
  scale_color_manual(values = c("Real-Time MAE" = "darkred", "Nowcast MAE" = "#1f78b4")) +
  labs(color = "Mean Absolute Error") +
  theme_minimal() +
  theme(legend.position = "top", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

The elevated errors at both ends highlight periods where the discrepancies between the real-time and nowcast estimates are most pronounced.

## Takeaways

[**Goal**]{.primary}: Predict COVID-19 cases using %CLI, overcoming delays in report data.

Main Steps:

1. [**Fetch Data**]{.primary}: Collect case and %CLI data.

2. [**Merge Data**]{.primary}: Align datasets with `epix_merge()` and fill missing values.

3. [**Model**]{.primary}: Fit a linear model to predict cases.

4. [**Nowcast**]{.primary}: Apply dynamic forecasting with `epix_slide()`.

5. [**Evaluate**]{.primary}: Calculate error measures and numerically and visually assess the results.

Overall, nowcasting, based on the linear model, provided a closer approximation of true cases compared to the real-time values.


## Bonus


## Aside on nowcasting

* To some Epis, "nowcasting" can be equated with "estimate the time-varying instantaneous reproduction number, $R_t$"

* Ex. using the number of reported COVID-19 cases in British Columbia between Jan. 2020 and Apr. 15, 2023. 

<!-- This data is the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. The values are.up-to-date as of August 2023. -->
```{r rtestim}
#| fig-width: 9
#| fig-height: 3
#| out-height: "400px"
#| label: nowcasting
library(rtestim)
source(here::here("_code/bccovid.R"))

p1 <- bccovid|>
  ggplot(aes(date, cases)) + 
  geom_line(colour = primary) +
  geom_vline(xintercept = ymd("2023-04-15"), colour = secondary,
             linewidth = 2) +
  labs(y = "BC Covid-19 cases", x = "Date") +
  scale_y_continuous(expand = expansion(c(0, NA)))
bc_rt <- estimate_rt(bccovid$cases, x = bccovid$date, 
                     lambda = c(1e6, 1e5))
p2 <- plot(confband(bc_rt, lambda = 1e5)) + 
  coord_cartesian(ylim = c(0.5, 2)) +
  scale_y_continuous(expand = expansion(0))
cowplot::plot_grid(p1, p2)
```

* Group built [`{rtestim}`](https://dajmcdon.github.io/rtestim) doing for this nonparametrically.


