---
talk-title: "Forecasting and Time-Series Models"
talk-short-title: "{{< meta talk-title >}}"
talk-subtitle: ""
author: ""
other-authors: ""
repo-address: "cmu-delphi/insightnet-workshop-2024"
talk-date: "Venue -- dd Somemonth yyyy"
format: revealjs
execute:
  cache: false
---

<!-- Set any of the above to "" to omit them -->

<!-- Or adjust the formatting in _titleslide.qmd -->
{{< include _titleslide.qmd >}}

```{r theme}
theme_set(theme_bw())
```

## Outline

1. Linear Regression for Time Series Data

1. Evaluation Methods

1. ARX Models

1. Overfitting and Regularization

1. Prediction Intervals

1. Forecasting with Versioned Data

1. Geo-pooling


# Linear Regression for Time Series Data

## Basics of linear regression 

* Assume we observe a predictor $x_i$ and an outcome $y_i$ for $i = 1, \dots, n$.

* Linear regression seeks coefficients $\beta_0$ and $\beta_1$ such that

$$y_i \approx \beta_0 + \beta_1 x_i$$

is a good approximation for every $i = 1, \dots, n$.

* In R, the coefficients are found by running `lm(y ~ x)`, where `y` is the vector 
of responses and `x` the vector of predictors.


## Multiple linear regression 

* Given $p$ different predictors, we seek $(p+1)$ coefficients such that

$$y_i \approx \beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip}$$
is a good approximation for every $i = 1, \dots, n$.


## Linear regression with lagged predictor

* In time series models, the outcomes and predictors are usually indexed by time 
$t$. 

* Often we want to predict a future value of $y$, given present and past 
values of $x$. 

* For this purpose, we introduce linear regression with lagged 
predictors 

$$y_t \approx \beta_0 + \beta_1 x_{t-k}$$

i.e. we regress the outcome $y$ at time $t$ on the predictor $x$ at time $t-k$.

## Example: COVID cases and deaths in California {.smaller}

::: flex
::: w-50

```{r plot-ca-cases-deaths}
#| fig-width: 8
#| fig-height: 5
source("../_code/ca_cases_deaths.R")
ggplot(ca %>% 
         mutate(deaths = trans21(deaths)) %>%
         pivot_longer(cols = c(cases, deaths), names_to = 'name'),
       aes(x = time_value, y = value)) + 
  geom_line(aes(color = name)) +
  scale_color_manual(values = palette()[c(2,4)]) +
  scale_y_continuous(
    name = "Reported Covid-19 cases per 100k people", 
    limits = range1,
    sec.axis = sec_axis(
      trans = trans12, 
      name = "Reported Deaths per 100k people")) +
  labs(title = "Covid-19 cases and deaths in California", x = "Date") +
  theme(legend.position = "bottom", legend.title = element_blank()) 
```

:::
::: w-50

```{r ca data}
#| echo: true
head(ca)
```
:::
:::

* Cases seem to be highly correlated with deaths several weeks later

* What is the lag $k$ for which the correlation between cases at $t-k$ and 
deaths at $t$ is maximized?

* Given that lag, we can fit linear regression with a lagged predictor where

$$y_t = \text{deaths at time } t \quad\quad x_{t-k} = \text{cases at time } t-k$$


## Choosing the lag $k$

* Let’s split the data into a training and a test set (before/after 2021-03-01).

* The lag leading to largest correlation between lagged cases and deaths on
the training set is 
$k = 26$.

```{r correlation-cases-deaths}
t0_date <- as.Date('2021-03-01') #split date

train <- ca %>% filter(time_value <= t0_date)
test <- ca %>% filter(time_value > t0_date)

t0 <- nrow(train)  #split index

# look at cases and deaths (where we move deaths forward by 1, 2, ..., 35 days)
lags = 1:35
cor_deaths_cases <- lapply(lags, 
                             function(x) epi_cor(train, deaths, cases, 
                                                 cor_by = geo_value, dt1 = x))

cor_deaths_cases <- list_rbind(cor_deaths_cases, names_to = 'Lag') 

# lag leading to maximum correlation
k <- which.max(cor_deaths_cases$cor)

cor_deaths_cases %>%
  ggplot(aes(Lag, cor)) +
  geom_point() +
  geom_line() +
  labs(x = "Lag", y = "Correlation") +
  geom_vline(xintercept = k) +
  ggtitle('Correlation between cases and deaths by lag')
```


* Let's use (base) R to prepare the data and fit 

$$y_t \approx \beta_0 + \beta_1 x_{t-26}$$

## Preparing the data

```{r lag-cases}
#| echo: true
# Add column with cases lagged by k
ca$lagged_cases <- dplyr::lag(ca$cases, n = k)

# Split into train and test (before/after t0_date)
t0_date <- as.Date('2021-03-01')
train <- ca %>% filter(time_value <= t0_date)
test <- ca %>% filter(time_value > t0_date)
```

* Check if `deaths` is approximately linear in `lagged_cases`:

```{r plot-lag-cases}
ggplot(train, aes(lagged_cases, deaths)) + 
  geom_point(alpha = .5) +
  labs(x = "Lagged cases", y = "Deaths") + 
  ggtitle("Deaths vs cases lagged by 26 days")
```

## Fitting lagged linear regression in R

```{r lagged-lm}
#| echo: true
reg_lagged = lm(deaths ~ lagged_cases, data = train)
coef(reg_lagged)
```

```{r plot-linear-fit}
ggplot(train, aes(lagged_cases, deaths)) +
  geom_point(alpha = .5) +
  geom_abline(intercept = coef(reg_lagged)[1], slope = coef(reg_lagged)[2],
              col = 'red') +
  labs(x = "Lagged cases", y = "Deaths") +
  ggtitle("Deaths vs cases lagged by 26 days with regression line")

```

# Evaluation

## Error metrics

* Assume we have predictions $\hat y_{new, t}$ for the unseen observations 
$y_{new,t}$ over times $t = 1, \dots, N$.

* Four commonly used error metrics are:

  * mean squared error (MSE)

  * mean absolute error (MAE)

  * mean absolute percentage error (MAPE)

  * mean absolute scaled error (MASE)

## Error metrics: MSE and MAE

$$MSE = \frac{1}{N} \sum_{t=1}^N (y_{new, t}- \hat y_{new, t})^2$$
$$MAE = \frac{1}{N} \sum_{t=1}^N |y_{new, t}- \hat y_{new, t}|$$

* MAE gives less importance to extreme errors than MSE.

* [Drawback]{.primary}: both metrics are scale-dependent, so they are not universally 
interpretable.
(For example, if $y$ captures height, MSE and MAE will vary depending on whether we measure in feet or meters.)

## Error metrics: MAPE

* Fixing scale-dependence:

$$MAPE = 100 \times \frac{1}{N} \sum_{t=1}^N 
\left|\frac{y_{new, t}- \hat y_{new, t}}{y_{new, t}}\right|$$

* [Drawbacks]{.primary}:

  * Erratic behavior when $y_{new, t}$ is close to zero

  * It assumes the unit of measurement has a meaningful zero (e.g. using 
Fahrenheit or Celsius to measure temperature will lead to different MAPE)


## Error metrics: MASE

$$MASE = 100 \times \frac{\frac{1}{N} \sum_{t=1}^N 
|y_{new, t}- \hat y_{new, t}|}
{\frac{1}{N-1} \sum_{t=2}^N 
|y_{new, t}- y_{new, t-1}|}$$

* [Advantages]{.primary}:

  * is universally interpretable (not scale dependent)

  * avoids the zero-pitfall

* MASE in words: we normalize the error of our forecasts by that of a naive method 
which always predicts the last observation.


## Defining the error metrics in R

```{r error functions}
#| echo: true
MSE <- function(truth, prediction) {
  mean((truth - prediction)^2)}

MAE <- function(truth, prediction) {
  mean(abs(truth - prediction))}

MAPE <- function(truth, prediction) {
  100 * mean(abs(truth - prediction) / truth)}

MASE <- function(truth, prediction) {
  100 * MAE(truth, prediction) / mean(abs(diff(truth)))}
```

## Estimating the prediction error

* Given an error metric (e.g. MSE), we want to estimate the prediction error under that metric. 

* This can be accomplished in different ways, using the

  * Training error

  * Split-sample error

  * Time series cross-validation error (using all past data or a trailing window)


## Training error

* The easiest but [worst]{.primary} approach to estimate the prediction error is 
to use the training error, i.e. the average error on the training set that was 
used to fit the model.

* The training error is

  * generally too optimistic as an estimate of prediction error

  * [more optimistic the more complex the model!]{.primary}


## Training error
#### Linear regression of COVID deaths on lagged cases

```{r pred-train}
#| echo: true
# Getting the predictions for the training set
pred_train <- predict(reg_lagged)
```

```{r plot-train-predictions}
#| fig-align: left
train %>% 
  mutate(observed = deaths, predicted = c(rep(NA, k), pred_train)) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r function errors}
getErrors <- function(truth, prediction, type) {
  return(data.frame("MSE" = MSE(truth, prediction), 
                    "MAE"= MAE(truth, prediction), 
                    "MAPE" = MAPE(truth, prediction), 
                    "MASE" = MASE(truth, prediction), 
                    row.names = type))
}
```


```{r training-error}
errors <- getErrors(train$deaths[-(1:k)], pred_train, "training")
errors
```


## Split-sample error {.smaller}

* To compute the split-sample error  

  1. Split the data into training (up to time $t_0$), and test set (after $t_0$)

  1. Fit the model to the training data only

  1. Make predictions for the test set

  1. Compute the selected error metric on the test set only

* Formally, the split-sample MSE is

$$\text{SplitMSE} = \frac{1}{n-t_0} \sum_{t = t_0 +1}^n (\hat y_t - y_t)^2$$

* In practice, split-sample estimates of prediction error are generally 
[pessimistic]{.primary}, as they mimic a situation where we would never refit
the model in the future. 

## Split-sample error
#### Linear regression of COVID deaths on lagged cases

```{r test-pred}
#| echo: true
# Getting the predictions for the test set
pred_test <- predict(reg_lagged, newdata = test)
```

```{r plot-test-predictions}
#| fig-align: left
test %>% 
  mutate(observed = deaths, predicted = pred_test) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```


```{r test-error}
errors <- rbind(errors, getErrors(test$deaths, pred_test, "split-sample"))
errors
```

## Time-series cross-validation (CV) {.smaller}
#### 1-step ahead predictions

* If we will refit the model in the future once new data become available, a more 
appropriate way to estimate the prediction error is time-series cross-validation.

* Assume we want to make 1-step ahead predictions (i.e. at time $t-1$ we want to 
make a forecast for time $t$). Then, for $t = t_0+1, t_0+2, \dots$, we proceed
as follows:

  1. Fit the model using data up to time $t-1$

  1. Make a prediction for $t$ 

  1. Record the prediction error

The cross-validation MSE is then

$$CVMSE = \frac{1}{n-t_0} \sum_{t = t_0+1}^n (\hat y_{t|t-1} - y_t)^2$$

where	$\hat y_{t|t-1}$ indicates a prediction for $y$ at time $t$ that was made 
with data available up to time $t-1$.

## Time-series cross-validation (CV) {.smaller}
#### $h$-step ahead predictions

* More in general, if we want to make $h$-step ahead predictions (i.e. at time 
$t-h$ we want to make a forecast for time $t$), we proceed as follows 
for $t = t_0+1, t_0+2, \dots$

  * Fit the model using data up to time $t-h$

  * Make a prediction for $t$ 

  * Record the prediction error

* The cross-validation MSE is then

$$CVMSE = \frac{1}{n-t_0} \sum_{t = t_0+1}^n (\hat y_{t|t-h} - y_t)^2$$

where	$\hat y_{t|t-h}$ indicates a prediction for $y$ at time $t$ that was made 
with data available up to time $t-h$.

## Time-series cross-validation (CV) 
#### Linear regression of COVID deaths on lagged cases

Getting the predictions requires slightly more code:

```{r time-series-cv}
#| echo: true
n <- nrow(ca)                               #length of time series
pred_all_past <- rep(NA, length = n - t0)   #initialize vector of predictions
h <- 1                                      #number of days ahead for which prediction is wanted

for (t in (t0+1):n) {
  # fit to all past data and make 1-step ahead prediction
  reg_all_past = lm(deaths ~ lagged_cases, data = ca, subset = (1:n) <= (t-h)) 
  pred_all_past[t-t0] = predict(reg_all_past, newdata = data.frame(ca[t, ]))
}

```

::: {.callout-important icon="false"}
## Note

In general, we can predict at most $k$ days ahead (where $k$ = number of days by which predictor is lagged)!
:::


## Time-series cross-validation (CV)
#### Linear regression of COVID deaths on lagged cases

```{r plot-cv-predictions}
#| fig-align: left
test %>% 
  mutate(observed = deaths, 
         `predicted (CV)` = pred_all_past) %>%
  pivot_longer(cols = c(observed, `predicted (CV)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r cv-error}
errors <- rbind(errors, getErrors(test$deaths, pred_all_past, "time series CV"))
errors
```

## Time-series CV on a trailing window

* So far, when making $h$-step ahead predictions for time $t$, we have fitted the 
model on all the data available up to time $t-h$. We can instead use a trailing 
window, i.e. fit the model on only a window of data of length $w$, starting at 
time $t-h-w$ and ending at time $t-h$.

* [Advantage]{.primary}: if the relationship between predictors and outcome changes over time,
training the forecaster on a window of recent data can better capture the recent relationship which might be more relevant to events in the near future.

* Window length $w$ considerations: 

  * if $w$ is too big, the model can't adapt to the recent predictors-outcome relation 

  * if $w$ is too small, the fitted model may be too volatile (trained on too 
little data)

## Time-series CV on a trailing window
#### Linear regression of COVID deaths on lagged cases

```{r time-series-cv-trailing}
#| echo: true
# Getting the predictions through CV with trailing window
w <- 30                                     #trailing window size
pred_trailing <- rep(NA, length = n - t0)   #initialize vector of predictions
h <- 1                                      #number of days ahead for which prediction is wanted

for (t in (t0+1):n) {
  # fit to a trailing window of size w and make 1-step ahead prediction
  reg_trailing = lm(deaths ~ lagged_cases, data = ca, 
                    subset = (1:n) <= (t-h) & (1:n) > (t-h-w)) 
  pred_trailing[t-t0] = predict(reg_trailing, newdata = data.frame(ca[t, ]))
}
```

## Time-series CV: all past vs trailing window
#### Linear regression of COVID deaths on lagged cases

```{r plot-cv-predictions-trailing}
#| fig-align: left
test %>% 
  mutate(observed = deaths, 
         `predicted (CV)` = pred_all_past, 
         `predicted (trailing + CV)` = pred_trailing) %>%
  pivot_longer(cols = c(observed, `predicted (CV)`, `predicted (trailing + CV)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```


```{r cv-trailing-error}
errors <- rbind(errors, getErrors(test$deaths, pred_trailing, 
                                  "time series CV + trailing"))
errors
```





# ARX Models

## Autoregressive (AR) model

* [Idea]{.primary}: predicting the outcome via a linear combination of its lags 

$$y_t \approx \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p}$$

* In R, the coefficients $\phi_1, \phi_2, \dots, \phi_p$ can be estimated using `lm`.

## AR model for COVID deaths

* Let's disregard `cases`, and only use past `deaths` to predict future `deaths`. 

* For now we use one lag only, the one for which deaths and lagged deaths have largest correlation.

```{r auto-cor-deaths}
lags <- 1:8
auto_cor_deaths <- lapply(lags, 
                          function(x) epi_cor(train, deaths, deaths, 
                                              cor_by = geo_value, dt1 = x))

auto_cor_deaths <- list_rbind(auto_cor_deaths, names_to = 'Lag') 

auto_cor_deaths %>%
  ggplot(aes(Lag, cor)) +
  geom_point() +
  geom_line() +
  labs(x = "Lag", y = "Correlation") +
  ggtitle('Auto-correlation for deaths by lag')
```

* We will fit the model: $\quad y_t \approx \beta_0 + \beta_1 y_{t-1}$

## Preparing the data

```{r lag-deaths}
#| echo: true
# Add column with deaths lagged by 1
ca$lagged_deaths <- dplyr::lag(ca$deaths, n = 1)

# Split into train and test (before/after t0_date)
train <- ca %>% filter(time_value <= t0_date)
test <- ca %>% filter(time_value > t0_date)
```

Check that the relationship between COVID deaths and lagged COVID deaths is approximately linear (on the training set):

```{r ar-plot-deaths-and-lagged-cases}
ggplot(train, aes(lagged_deaths, deaths)) + 
  geom_point(alpha = .5) +
  labs(x = "Lagged deaths", y = "Deaths")
```


## Fitting the AR model for COVID deaths

```{r ar-lm}
#| echo: true
ar_fit = lm(deaths ~ lagged_deaths, data = train)
coef(ar_fit)
```

::: {.callout-important icon="false"}
## Note

The intercept is $\approx 0$ and the coefficient is $\approx 1$. 
This means that we are naively predicting the number of deaths tomorrow with the
number of deaths observed today.
:::

## Predictions on training and test sets (AR model)

```{r ar-pred}
#| echo: true
pred_train <- predict(ar_fit)                 #get training predictions
pred_test <- predict(ar_fit, newdata = test)  #get test predictions
```

```{r ar-plot-train-predictions}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         predicted = c(NA, pred_train, pred_test)) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r ar-train-test-error}
errors_ar <- rbind(getErrors(train$deaths[-1], pred_train, "training"),
                   getErrors(test$deaths, pred_test, "split-sample"))
errors_ar
```


## Time-Series CV: all past and trailing (AR model)

```{r ar-time-series-cv}
#| echo: true
# Getting 1-step ahead predictions through CV 
pred_all_past = pred_trailing <- rep(NA, length = n - t0) #initialize vectors of predictions
w <- 30                                                   #trailing window size
h <- 1                                                    #number of days ahead for which prediction is wanted

for (t in (t0+1):n) {
  # fit to all past data 
  ar_all_past = lm(deaths ~ lagged_deaths, data = ca, subset = (1:n) <= (t-h)) 
  # fit to trailing window of data
  ar_trailing = lm(deaths ~ lagged_deaths, data = ca, subset = (1:n) <= (t-h) & (1:n) > (t-h-w)) 
  # make 1-step ahead predictions
  pred_all_past[t-t0] = predict(ar_all_past, newdata = data.frame(ca[t, ]))
  pred_trailing[t-t0] = predict(ar_trailing, newdata = data.frame(ca[t, ]))
}
```

::: {.callout-important icon="false"}
## Reminder

We can predict at most 1-day ahead in this case, because the predictor is only lagged 
by 1 with respect to the outcome.
:::

## Time-Series CV: all past and trailing (AR model)

```{r ar-plot-cv-predictions}
#| fig-align: left
test %>% 
  mutate(observed = deaths, 
         `predicted (CV)` = pred_all_past, 
         `predicted (trailing + CV)` = pred_trailing) %>%
  pivot_longer(cols = c(observed, `predicted (CV)`, `predicted (trailing + CV)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r ar-cv-error}
errors_ar <- rbind(errors_ar, 
                getErrors(test$deaths, pred_all_past, "time series CV")) %>%
  rbind(getErrors(test$deaths, pred_trailing, "time series CV + trailing"))

errors_ar
```

## Autoregressive exogenous input (ARX) model

* [Idea]{.primary}: predicting the outcome via a linear combination of its lags and a set of exogenous (i.e. external) input variables

* Example of ARX model 

$$y_t \approx \sum_{i=1}^p \phi_i y_{t-i} + \sum_{j=1}^q \psi_j x_{t-j}$$

* We can construct more complex ARX models with multiple lags of several exogenous 
variables

## ARX model for COVID deaths

* To improve our predictions for COVID deaths, we could merge the two models 
considered so far (i.e. linear regression on cases lagged by k = `r k`, and 
linear regression on deaths lagged by 1).

* This leads us to the ARX model

$$y_t \approx \beta_0 + \beta_1 y_{t-1} + \beta_2 x_{t-k}$$

* We can fit it on the training set by running

```{r arx-lm}
#| echo: true
arx_fit = lm(deaths ~ lagged_deaths + lagged_cases, data = train)
coef(arx_fit)
```

## Predictions on training and test sets (ARX model)

```{r arx-training-error}
#| echo: true
pred_train <- predict(arx_fit)                  #get training predictions
pred_test <- predict(arx_fit, newdata = test)   #get test predictions
```

```{r arx-plot-train-test-predictions}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         predicted = c(rep(NA, k), pred_train, pred_test)) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r arx-train-test-errors}
errors_arx <- rbind(getErrors(train$deaths[-(1:k)], pred_train, "training"),
                    getErrors(test$deaths, pred_test, "split-sample"))
errors_arx
```

## Time-Series CV: all past and trailing (ARX model)

```{r arx-time-series-cv}
#| echo: true
# Getting 1-step ahead predictions through CV 
pred_all_past = pred_trailing <- rep(NA, length = n - t0) #initialize vector of predictions
w <- 30                                                   #trailing window size
h <- 1                                                    #number of days ahead for which prediction is wanted

for (t in (t0+1):n) {
  # fit to all past data
  arx_all_past = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, subset = (1:n) <= (t-h)) 
  # fit to trailing window of data
  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, 
                    subset = (1:n) <= (t-h) & (1:n) > (t-h-w)) 
  # make 1-step ahead prediction
  pred_all_past[t-t0] = predict(arx_all_past, newdata = data.frame(ca[t, ]))
  pred_trailing[t-t0] = predict(arx_trailing, newdata = data.frame(ca[t, ]))
}
```

## Time-Series CV: all past and trailing (ARX model)

```{r arx-plot-cv-predictions}
#| fig-align: left
test %>% 
  mutate(observed = deaths, 
         `predicted (CV)` = pred_all_past, 
         `predicted (trailing + CV)` = pred_trailing) %>%
  pivot_longer(cols = c(observed, `predicted (CV)`, `predicted (trailing + CV)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```


```{r arx-cv-error}
errors_arx <- rbind(errors_arx, 
                    getErrors(test$deaths, pred_all_past, "time series CV")) %>%
  rbind(getErrors(test$deaths, pred_trailing, "time series CV + trailing"))
errors_arx
```



# Overfitting and Regularization

## Too many predictors

* What if we try to incorporate past information extensively by fitting a model 
with a very large number of predictors?

  * The estimated coefficients will be chosen to mimic the observed data very 
  closely on the training set, leading to [small training error]{.primary}

  * The predictive performance on the test set might be very poor, 
  producing [large split-sample and CV error]{.primary}

::: {.callout-important icon="false"}
## Issue
Overfitting!
:::

## ARX model for COVID deaths with many predictors

* When predicting COVID deaths at time $t$, we can try to use more past 
information by fitting a model that includes the past two months of COVID deaths 
and cases as predictors

$$y_t \approx \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_{60} y_{t-60} + 
\psi_1 yx_{t-1} + \psi_2 x_{t-2} + \dots + \psi_{60} x_{t-60}$$

## Preparing the data

```{r overfit-data}
#| echo: true
y <- ca$deaths  #outcome
lags <- 1:60    #lags used for predictors (deaths and cases)

# Build predictor matrix with 60 columns
X <- data.frame(matrix(NA, nrow = length(y), ncol = 2*length(lags)))
colnames(X) <- paste('X', 1:ncol(X), sep = '')

for (j in 1:length(lags)) {
  # first 60 columns contain deaths lagged by 1, 2,..., 60
  X[, j] = dplyr::lag(ca$deaths, lags[j])
  # last 60 columns contain cases lagged by 1, 2,..., 60
  X[, length(lags) + j] = dplyr::lag(ca$cases, lags[j])
}

X[1:5, 1:5] #look at first few entries of predictor matrix
```

## Fitting the ARX model

```{r overfit-lm}
#| echo: true
# Train/test split
y_train <- y[1:t0]
X_train <- X[1:t0, ]
y_test <- y[(t0+1):length(y)]
X_test <- X[(t0+1):length(y), ]

# Fitting the ARX model
reg = lm(y_train ~ ., data = X_train)
coef(reg)
```

## Predictions on training and test set 

```{r overfit-pred}
#| echo: true
pred_train <- predict(reg)                    #get training predictions
pred_test <- predict(reg, newdata = X_test)   #get test predictions
```

```{r overfit-plot-train-test}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         predicted = c(rep(NA, max(lags)), pred_train, pred_test)) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())

```

```{r overfit-train-test-error}
errors_ar <- rbind(getErrors(y_train[-(1:max(lags))], pred_train, "training"),
                   getErrors(y_test, pred_test, "split-sample"))
errors_ar
```


## Regularization

* If we want to consider a large number of predictors, 
how can we avoid overfitting?

* [Idea]{.primary}: introduce a regularization parameter $\lambda$ that [shrinks or sets]{.primary} some 
of the estimated coefficients to zero, i.e. some predictors are estimated to 
have limited or no predictive power

* Most common regularization methods

  * [Ridge]{.primary}: shrinks coefficients to zero
  
  * [Lasso]{.primary}: sets some coefficients to zero

## Choosing $\lambda$

* The regularization parameter $\lambda$ can be selected by cross-validation:

  1. Select a sequence of $\lambda$'s
  
  1. Fit and predict for each such $\lambda$
  
  1. Select the $\lambda$ that leads to smaller error
  
* The R library `glmnet` implements ridge and lasso regression, 
and can perform step 1. automatically.



## Fit ARX + ridge/lasso for COVID deaths

```{r ridge-lasso-fit}
#| echo: true
library(glmnet) # Implements ridge and lasso

# We'll need to omit NA values explicitly, as otherwise glmnet will complain
na_obs <- 1:max(lags)
X_train <- X_train[-na_obs, ]
y_train <- y_train[-na_obs]

# Ridge regression: set alpha = 0, lambda sequence will be chosen automatically
ridge <- glmnet(X_train, y_train, alpha = 0)
beta_ridge <- coef(ridge)       # matrix of estimated coefficients 
lambda_ridge <- ridge$lambda    # sequence of lambdas used to fit ridge 

# Lasso regression: set alpha = 1, lambda sequence will be chosen automatically
lasso <- glmnet(X_train, y_train, alpha = 1)
beta_lasso <- coef(lasso)       # matrix of estimated coefficients 
lambda_lasso <- lasso$lambda    # sequence of lambdas used to fit lasso 

dim(beta_lasso)      # One row per coefficient, one column per lambda value
```


## Predictions on test set and selection of $\lambda$

```{r lasso-ridge-predictions}
#| echo: true
# Predict values for second half of the time series
yhat_ridge <- predict(ridge, newx = as.matrix(X_test))
yhat_lasso <- predict(lasso, newx = as.matrix(X_test))

# Compute MAE 
mae_ridge <- colMeans(abs(yhat_ridge - y_test))
mae_lasso <- colMeans(abs(yhat_lasso - y_test))

# Select index of lambda vector which gives lowest MAE
min_ridge <- which.min(mae_ridge)
min_lasso <- which.min(mae_lasso)
paste('Best MAE ridge:', round(min(mae_ridge), 3),
      '; Best MAE lasso:', round(min(mae_lasso), 3))

# Get predictions for train and test sets
pred_train_ridge <- predict(ridge, newx = as.matrix(X_train))[, min_ridge] 
pred_test_ridge <- yhat_ridge[, min_ridge]
pred_train_lasso <- predict(lasso, newx = as.matrix(X_train))[, min_lasso] 
pred_test_lasso <- yhat_lasso[, min_lasso]
```

## Estimated coefficients: shrinkage vs sparsity

::: flex
::: w-50
```{r lasso-ridge-coeff}
# Estimated coefficients
cbind('ridge' = beta_ridge[, min_ridge], 
      'lasso' = beta_lasso[, min_lasso])
```

:::

::: w-50

```{r plot-ridge-lasso-coeff}
#| out-height: "600px"
data.frame('x'= 1:(2 * max(lags) + 1),
           'ridge' = beta_ridge[, min_ridge], 
           'lasso' = beta_lasso[, min_lasso]) %>%
  pivot_longer(cols = c('ridge', 'lasso'), names_to = 'Method') %>%
  ggplot(aes(x, value, col = Method)) +
  geom_point(alpha = .8) +
  geom_line() +
  xlab('Regressor') + 
  ylab('Estimated coefficient')
```

:::
:::

## Predictions: ARX + ridge/lasso (train and test set)

```{r shrinkage-sparsity}
#| fig-align: left
ca %>% 
  mutate(observed = deaths, 
         `predicted (ridge)` = c(rep(NA, max(lags)), pred_train_ridge, 
                                 pred_test_ridge),
         `predicted (lasso)` = c(rep(NA, max(lags)), pred_train_lasso, 
                                 pred_test_lasso)) %>%
  pivot_longer(cols = c(observed, `predicted (ridge)`, 
                        `predicted (lasso)`), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r regularization-train-test-errors}
errors_glmnet <- rbind(getErrors(y_train, pred_train_ridge, 
                                "ridge training"),
                       getErrors(y_test, pred_test_ridge, "ridge split-sample"),
                       getErrors(y_train, pred_train_lasso, 
                                 "lasso training"),
                       getErrors(y_test, pred_test_lasso, "lasso split-sample"))
errors_glmnet
```

## Time-series CV for ARX + ridge/lasso (trailing)

```{r regularization-cv}
#| echo: true
# Initialize matrices for predictions (one column per lambda value)
yhat_ridge <- matrix(NA, ncol = length(lambda_ridge), nrow = n-t0) 
yhat_lasso <- matrix(NA, ncol = length(lambda_lasso), nrow = n-t0) 

h <- 1 #number of days ahead for which prediction is wanted

for (t in (t0+1):n) {
  # Indices of data within window
  inds = t-h-w < 1:n & 1:n <= t-h
  # Fit ARX + ridge/lasso
  ridge_trail = glmnet(X[inds, ], y[inds], alpha = 0, lambda = lambda_ridge)
  lasso_trail = glmnet(X[inds, ], y[inds], alpha = 1, lambda = lambda_lasso)
  # Predict
  yhat_ridge[t-t0, ] = predict(ridge_trail, newx = as.matrix(X[t, ]))
  yhat_lasso[t-t0, ] = predict(lasso_trail, newx = as.matrix(X[t, ]))
}

# MAE values for each lambda
mae_ridge <- colMeans(abs(yhat_ridge - y_test))
mae_lasso <- colMeans(abs(yhat_lasso - y_test))

# Select lambda that minimizes MAE and save corresponding predictions
min_ridge <- which.min(mae_ridge)
min_lasso <- which.min(mae_lasso)
pred_cv_ridge <- yhat_ridge[, min_ridge]
pred_cv_lasso <- yhat_lasso[, min_lasso]

paste('Best MAE ridge:', round(min(mae_ridge), 3))
paste('Best MAE lasso:', round(min(mae_lasso), 3))
```


## Predictions: time-series CV for ARX + ridge/lasso (trailing)

```{r plot-regularization-cv}
#| fig-align: left
test %>% 
  mutate(observed = deaths, 
         `predicted (ridge)` = pred_cv_ridge,
         `predicted (lasso)` = pred_cv_lasso) %>%
  pivot_longer(cols = c(observed, `predicted (ridge)`, 
                        `predicted (lasso)`), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value, col = Deaths)) +
  geom_line() + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r regularization-cv-errors}
errors_glmnet_cv <- rbind(
                       getErrors(y_test, pred_cv_ridge, "ridge CV + trailing"),
                       getErrors(y_test, pred_cv_lasso, "lasso CV + trailing"))
errors_glmnet_cv
```

# Prediction Intervals

## Point predictions vs intervals 

* So far, we have only considered [point predictions]{.primary}, i.e. 
we have fitted models 
to provide our [best guess on the outcome]{.primary} at time $t$. 

::: {.callout-important icon="false"}
## 
* What if we want to provide a [measure of uncertainty]{.primary} around our point 
prediction or a [likely range of values]{.primary} for the outcome at time $t$?

:::

* For each target time $t$, we can construct [prediction intervals]{.primary}, i.e. provide 
ranges of values that are expected to cover the true outcome value a fixed 
fraction of times.

## Prediction intervals for `lm` fits

* To get prediction intervals for the models we previously fitted, 
we only need to tweak our call to `predict` by adding as an input: 

  `interval = "prediction", level = p`

  where $p \in (0, 1)$ is the desired coverage.

* The output from `predict` will then be a matrix with 

  * first column a point estimate
  
  * second column the lower limit of the interval
  
  * third column the upper limit of the interval

## Prediction intervals for ARX (test)

```{r arx-intervals-test}
#| echo: true
#| output-location: column
pred_test_ci <- predict(arx_fit, 
                        newdata = test, 
                        interval = "prediction", 
                        level = 0.95)

head(pred_test_ci)
```


```{r plot-arx-intervals}
test %>% 
  mutate(observed = deaths, 
         predicted = pred_test_ci[, 1], 
         lower = pred_test_ci[, 2], 
         upper = pred_test_ci[, 3]) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(col = Deaths)) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Prediction intervals for ARX (time-series CV)

```{r arx-intervals-time-series-cv}
#| echo: true
# Initialize matrices to store predictions 
# 3 columns: point estimate, lower limit, and upper limit
pred_all_past = pred_trailing <- matrix(NA, nrow = n - t0, ncol = 3)
colnames(pred_all_past) = colnames(pred_trailing) <- c('prediction', 'lower', 'upper')

for (t in (t0+1):n) {
  # Fit ARX 
  arx_all_past = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, 
                    subset = (1:n) <= (t-h)) 
  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca, 
                    subset = (1:n) <= (t-h) & (1:n) > (t-h-w)) 
  # Predict
  pred_all_past[t-t0, ] = predict(arx_all_past, newdata = data.frame(ca[t, ]),
                                interval = "prediction", level = 0.95)
  pred_trailing[t-t0, ] = predict(arx_trailing, newdata = data.frame(ca[t, ]),
                                interval = "prediction", level = 0.95)
}

lm_pred_all_past <- cbind(test, pred_all_past)
lm_pred_trailing <- cbind(test, pred_trailing)
```

## Prediction intervals for ARX (CV, all past)

```{r plot arx-intervals-cv}
lm_pred_all_past %>%
  mutate(observed = deaths, 
         `predicted (CV)` = prediction) %>%
  pivot_longer(cols = c(observed, `predicted (CV)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, 
                  ymax = upper), alpha = .3) +
  geom_line(aes(col = Deaths)) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())

```

## Prediction intervals for ARX (CV, trailing window)

```{r plot arx-intervals-cv-trailing}
lm_pred_trailing %>%
  mutate(observed = deaths, 
         `predicted (CV + trailing)` = prediction) %>%
  pivot_longer(cols = c(observed, `predicted (CV + trailing)`), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, 
                  ymax = upper), alpha = .3) +
  geom_line(aes(col = Deaths)) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

[Note]{.primary}: the width of the prediction intervals varies substantially over time.


## Quantile regression

* So far we only considered different ways to apply linear regression.

* Quantile regression is a different estimation method, and it directly targets conditional 
quantiles of the outcome over time.

::: {.callout-note}
## Definition
Conditional quantile = value below which a given percentage (e.g. 25%, 50%, 
75%) of observations fall, given specific values of the predictor variables. 
:::

* [Advantage]{.primary}: it provides a more complete picture of the outcome distribution.

## ARX model for COVID deaths via quantile regression

```{r q-reg}
#| echo: true
#install.packages("quantreg")
library(quantreg)  #library to perform quantile regression

# Set quantiles of interest: we will focus on 2.5%, 50% (i.e. median), and 97.5% quantiles
quantiles <- c(0.025, 0.5, 0.975)  

# Fit quantile regression on training set
q_reg <- rq(deaths ~ lagged_deaths + lagged_cases, data = train, tau = quantiles)

# Estimated coefficients
coef(q_reg)

# Predict on test set
pred_test <- predict(q_reg, newdata = test)
```

## Predictions via quantile regression (test)

```{r q-reg-training}
test %>% 
  mutate(observed = deaths, 
         predicted = pred_test[, 2],
         lower = pred_test[, 1],
         upper = pred_test[, 3]) %>%
  pivot_longer(cols = c(predicted, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .4) +
  geom_line(aes(col = Deaths)) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Predictions via quantile regression (time-series CV)

```{r q-reg-time-series-cv}
#| echo: true
# Initialize matrices to store predictions 
# 3 columns: lower limit, median, and upper limit
pred_all_past = pred_trailing <- matrix(NA, nrow = n - t0, ncol = 3)
colnames(pred_all_past) = colnames(pred_trailing) <- c('lower', 'median', 'upper')

for (t in (t0+1):n) {
  # Fit quantile regression
  rq_all_past = rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles,
                   data = ca, subset = (1:n) <= (t-h)) 
  rq_trailing = rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles,
                   data = ca, subset = (1:n) <= (t-h) & (1:n) > (t-h-w)) 
  # Predict
  pred_all_past[t-t0, ] = predict(rq_all_past, newdata = data.frame(ca[t, ]))
  pred_trailing[t-t0, ] = predict(rq_trailing, newdata = data.frame(ca[t, ]))
}

rq_pred_all_past <- cbind(test, pred_all_past)
rq_pred_trailing <- cbind(test, pred_trailing)
```

## Predictions via quantile regression (CV, all past)

```{r q-reg-plot-cv-predictions}
rq_pred_all_past %>%
  mutate(observed = deaths, 
         `predicted (CV)` = median) %>%
  pivot_longer(cols = c(`predicted (CV)`, observed), names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(col = Deaths)) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Predictions via quantile regression (CV, trailing)

```{r q-reg-plot-cv-predictions-trailing}
rq_pred_trailing %>%
  mutate(observed = deaths, 
         `predicted (CV + trailing)` = median) %>%
  pivot_longer(cols = c(`predicted (CV + trailing)`, observed), 
               names_to = 'Deaths') %>%
  ggplot(aes(time_value, value)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(col = Deaths)) + 
  geom_vline(xintercept = t0_date, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Actual Coverage

* We would expect the ARX model fitted via `lm` and via `rq` to cover the truth
about 95\% of the times. Is this actually true in practice?

* The actual coverage of each predictive interval is  

```{r lm-coverage-all-past}
data.frame("lm all past" = mean(test$deaths >= lm_pred_all_past$lower & 
                                  test$deaths <= lm_pred_all_past$upper), 
           "lm trailing" = mean(test$deaths >= lm_pred_trailing$lower & 
                                  test$deaths <= lm_pred_trailing$upper), 
           "rq all past" = mean(test$deaths >= rq_pred_all_past$lower & 
                                  test$deaths <= rq_pred_all_past$upper), 
           "rq trailing" = mean(test$deaths >= rq_pred_trailing$lower & 
                                  test$deaths <= rq_pred_trailing$upper), 
           row.names = 'Coverage')

```

* Notice that the coverage of `lm` is close to 95\%, while `rq` has lower 
coverage, especially for the trailing window case.

## Evaluation

* Prediction intervals are “good” if they 

  * cover the truth most of the time
  
  * are not too wide
  
* Error metric that captures both desiderata: [Weighted Interval Score (WIS)]{.primary}

* $F$ = forecast composed of predicted quantiles $q_{\tau}$ for the set 
of quantile levels $\tau$. The WIS for target variable $Y$ is represented as 
([McDonald et al., 2021](https://www.pnas.org/doi/full/10.1073/pnas.2111453118)):

$$WIS(F, Y) = 2\sum_{\tau} \phi_{\tau} (Y - q_{\tau})$$

where $\phi_{\tau}(x) = \tau |x|$ for $x \geq 0$ and 
$\phi_{\tau}(x) = (1-\tau) |x|$ for $x < 0$.

## Computing the WIS 

```{r wis-function}
#| echo: true
WIS <- function(truth, estimates, quantiles) {
  2 * sum(pmax(
    quantiles * (truth - estimates),
    (1 - quantiles) * (estimates - truth),
    na.rm = TRUE
  ))
}
```

::: {.callout-important icon="false"}
## Note
WIS tends to prioritize sharpness (how wide the interval is) relative to 
coverage (if the interval contains the truth).
:::

## WIS for ARX fitted via `lm` and `rq`

* The lowest mean WIS is attained by quantile regression trained on all past data. 

* Notice that this method has coverage below 95\% but it is still preferred under WIS 
because its intervals are narrower than for linear regression.

::: flex

::: w-50

```{r wis-lm}
lm_pred_all_past$method <- 'lm.all.past'
lm_pred_trailing$method <- 'lm.trailing'
rq_pred_all_past$method <- 'rq.all.past'
rq_pred_trailing$method <- 'rq.trailing'

rbind(lm_pred_all_past, lm_pred_trailing) %>% 
  group_by(method) %>%
  rowwise() %>%
  mutate(wis = WIS(deaths, c(lower, prediction, upper), quantiles)) %>%
  ungroup() %>%
  group_by(method) %>%
  summarise(mean_wis = mean(wis, na.rm = T))

```
:::

::: w-50

```{r wis-rq}
rbind(rq_pred_all_past, rq_pred_trailing) %>% 
  group_by(method) %>%
  rowwise() %>%
  mutate(wis = WIS(deaths, c(lower, median, upper), quantiles)) %>%
  ungroup() %>%
  group_by(method) %>%
  summarise(mean_wis = mean(wis, na.rm = T))
```

:::
:::

# Forecasting with Versioned Data

## Versioned data

* In our forecasting examples, we have assumed the data are never revised 
(or have simply ignored revisions, and used data `as_of` today)

::: {.callout-important icon="false"}
## 
How can we train forecasters when dealing with versioned data?
:::

```{r get-versioned-data}
source("../_code/versioned_data.R")
```

```{r versioned-data}
#| echo: true
head(data_archive)
```

## Version-aware forecasting

```{r versioned-weekly-avg}
fc_time_values <- seq(
  from = t0_date,
  to = as.Date("2021-12-31"),
  by = "1 month"
)

data_archive <- data_archive %>%
  epix_slide(
    before = Inf, 
    ref_time_values = fc_time_values,
    function(x, gk, rtv) {
      x %>%
        group_by(geo_value) %>%
        epi_slide_mean(case_rate, before = 6L) %>%
        epi_slide_mean(death_rate, before = 6L) %>%
        ungroup() %>%
        rename(case_rate_7d_av = slide_value_case_rate,
               death_rate_7d_av = slide_value_death_rate)
    }
  ) %>%
  rename(version = time_value) %>%
  rename(
    time_value = slide_value_time_value,
    geo_value = slide_value_geo_value,
    cases = slide_value_case_rate_7d_av,
    deaths = slide_value_death_rate_7d_av
  ) %>%
  select(version, time_value, geo_value, cases, deaths) %>%
  as_epi_archive(compactify = TRUE)

ca_archive <- data_archive$DT %>% 
  filter(geo_value == "ca") %>%
  as_epi_archive()
```


```{r versioned-quantile-reg}
#| echo: true
# initialize dataframe for predictions
# 5 columns: forecast date, target date, 2.5%, 50%, and 97.5% quantiles
pred_all_past = pred_trailing <- data.frame(matrix(NA, ncol = 5, nrow = 0))
colnames(pred_all_past) = colnames(pred_trailing) <- c("forecast_date", "target_date",
                                                       'tau..0.025', 'tau..0.500', 'tau..0.975')

w <- 30         #trailing window size
h <- 7          #number of days ahead

# dates when predictions are made (set to be 1 month apart)
fc_time_values <- seq(from = t0_date, to = as.Date("2021-12-31"), by = "1 month")

for (fc_date in fc_time_values) {
  # get data version as_of forecast date
  data <- epix_as_of(ca_archive, max_version = as.Date(fc_date))
  # create lagged predictors
  data$lagged_deaths <- dplyr::lag(data$deaths, h) #since we want to predict h-ahead, 
                                                   #we need to lag deaths by h (at least)
  data$lagged_cases <- dplyr::lag(data$cases, k)
  # perform quantile regression
  rq_all_past <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, data = data) 
  rq_trailing <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, 
                    # only consider window of data
                    data = data %>% filter(time_value > (max(time_value) - w))) 
  # construct data.frame with the right predictors for the target date
  predictors <- data.frame(lagged_deaths = tail(data$deaths, 1), 
                           lagged_cases = (data %>% 
                                             filter(time_value == (max(time_value) + h - k)))$cases)
  # make predictions for target date and add them to matrix of predictions
  pred_all_past <- rbind(pred_all_past, 
                         data.frame('forecast_date' = max(data$time_value),
                                    'target_date' = max(data$time_value) + h, 
                                    predict(rq_all_past, newdata = predictors)))
  pred_trailing <- rbind(pred_trailing, 
                         data.frame('forecast_date' = max(data$time_value),
                                    'target_date' = max(data$time_value) + h, 
                                    predict(rq_trailing, newdata = predictors)))
}
```

## Version-aware predictions (CV, all past)

```{r clean-data-output}
# clean output and join it with finalized values (`ca` dataset)
pred_all_past <- pred_all_past %>%
  rename(median = `tau..0.500`,
         lower = `tau..0.025`,
         upper = `tau..0.975`) %>%
  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%
  arrange(target_date)

pred_trailing <- pred_trailing %>%
  rename(median = `tau..0.500`,
         lower = `tau..0.025`,
         upper = `tau..0.975`) %>%
  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%
  arrange(target_date)
```


```{r plot-versioned-cv}
pred_all_past %>%
  ggplot(aes(x = target_date)) +
  geom_line(aes(y = deaths), col = 'gray50') + 
  geom_point(aes(y = median), col = 'red') +
  geom_linerange(aes(ymin = lower, ymax = upper), col = 'red', alpha = .6) +
  geom_vline(xintercept = fc_time_values, lty = 2, col = 'gray50') +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Version-awere predictions (CV, trailing)

```{r plot-versioned-cv-trailing}
pred_trailing %>%
  ggplot(aes(x = target_date)) +
  geom_line(aes(y = deaths), col = 'gray50') + 
  geom_point(aes(y = median), col = 'blue') +
  geom_linerange(aes(ymin = lower, ymax = upper), col = 'blue', alpha = .6) +
  geom_vline(xintercept = fc_time_values, lty = 2, col = 'gray50') +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

# Geo-pooling

## Using geo information

* Assume we observe data over time from [multiple locations]{.primary}
(e.g. states or counties).

* We could

  * Estimate coefficients [separately]{.primary} for each location (as we have done so far).
  
  * Fit one model using all locations together at each time point ([geo-pooling]{.primary}). 
Estimated coefficients will not be location specific.

  * Estimate coefficients separately for each location, but include predictors capturing 
averages across locations ([partial geo-pooling]{.primary}).



## Geo-pooling: CV all past

```{r geo-pooling}
#| echo: true
usa_archive <- data_archive$DT %>% 
  as_epi_archive()

# initialize dataframe for predictions
# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles
pred_all_past <- data.frame(matrix(NA, ncol = 6, nrow = 0))
colnames(pred_all_past) <- c('geo_value', 'forecast_date', 'target_date',
                             'tau..0.025', 'tau..0.500', 'tau..0.975')

h <- 7     #number of days ahead

for (fc_date in fc_time_values) {
  # get data version as_of forecast date
  data <- epix_as_of(usa_archive, max_version = as.Date(fc_date))
  
  # create lagged predictors for each state 
  data <- data %>%
    arrange(geo_value, time_value) %>%  
    group_by(geo_value) %>%
    mutate(lagged_deaths = dplyr::lag(deaths, h),
           lagged_cases = dplyr::lag(cases, k)) %>%
    ungroup()
  
  # perform quantile regression
  rq_all_past <- rq(deaths ~ lagged_deaths + lagged_cases, tau = quantiles, data = data) 
  
  # construct dataframe with the right predictors for the target date
  new_lagged_deaths <- data %>% 
    filter(time_value == max(time_value)) %>%
    select(geo_value, deaths)
  
  new_lagged_cases <- data %>% 
    filter(time_value == max(time_value) + h - k) %>%
    select(geo_value, cases)
  
  predictors <- new_lagged_deaths %>%
    inner_join(new_lagged_cases, join_by(geo_value)) %>%
    rename(lagged_deaths = deaths,
           lagged_cases = cases)
  
  # make predictions for target date and add them to matrix of predictions
  pred_all_past <- rbind(pred_all_past, 
                         data.frame(
                           'geo_value' = predictors$geo_value,
                           'forecast_date' = max(data$time_value),
                           'target_date' = max(data$time_value) + h, 
                           predict(rq_all_past, newdata = predictors)))
}

# geo-pooled predictions for California
pred_ca <- pred_all_past %>%
  filter(geo_value == 'ca') %>%
  rename(median = `tau..0.500`,
         lower = `tau..0.025`,
         upper = `tau..0.975`) %>%
  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%
  arrange(target_date)

```

## Geo-pooled predictions for California

```{r plot-geo-pooling}
pred_ca %>%
  ggplot(aes(x = target_date)) +
  geom_line(aes(y = deaths), col = 'gray50') + 
  geom_point(aes(y = median), col = 'purple') +
  geom_linerange(aes(ymin = lower, ymax = upper), col = 'purple', alpha = .6) +
  geom_vline(aes(xintercept = forecast_date), lty = 2, col = 'gray50') +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

## Partial geo-pooling: CV all past

```{r partial-geo-pooling}
#| echo: true

# initialize dataframe for predictions
# 6 columns: geo value, forecast date, target date, 2.5%, 50%, and 97.5% quantiles
pred_all_past <- data.frame(matrix(NA, ncol = 6, nrow = 0))
colnames(pred_all_past) <- c('geo_value', 'forecast_date', 'target_date',
                             'tau..0.025', 'tau..0.500', 'tau..0.975')

h <- 7     #number of days ahead

for (fc_date in fc_time_values) {
  # get data version as_of forecast date
  data <- epix_as_of(usa_archive, max_version = as.Date(fc_date))
  
  # create lagged predictors 
  data <- data %>%
    arrange(geo_value, time_value) %>%  
    group_by(geo_value) %>%
    mutate(lagged_deaths = dplyr::lag(deaths, h),
           lagged_cases = dplyr::lag(cases, k)) %>%
    ungroup() %>%
    group_by(time_value) %>%
    mutate(avg_lagged_deaths = mean(lagged_deaths, na.rm = T),
           avg_lagged_cases = mean(lagged_cases, na.rm = T)) %>%
    ungroup() 
  
  # perform quantile regression
  rq_all_past <- rq(deaths ~ lagged_deaths + lagged_cases + avg_lagged_deaths +
                      avg_lagged_cases, tau = quantiles, 
                    data = (data %>% filter(geo_value == 'ca'))) 
  
  # construct data.frame with the right predictors for the target date
  new_lagged_deaths <- data %>% 
    filter(time_value == max(time_value)) %>%
    select(geo_value, deaths) %>%
    mutate(avg_lagged_deaths = mean(deaths, na.rm = T)) %>%
    filter(geo_value == 'ca')
  
  new_lagged_cases <- data %>% 
    filter(time_value == max(time_value) + h - k) %>%
    select(geo_value, cases) %>%
    mutate(avg_lagged_cases = mean(cases, na.rm = T)) %>%
    filter(geo_value == 'ca')
  
  predictors <- new_lagged_deaths %>%
    inner_join(new_lagged_cases, join_by(geo_value)) %>%
    rename(lagged_deaths = deaths,
           lagged_cases = cases)
  
  # make predictions for target date and add them to matrix of predictions
  pred_all_past <- rbind(pred_all_past, 
                         data.frame(
                           'geo_value' = predictors$geo_value,
                           'forecast_date' = max(data$time_value),
                           'target_date' = max(data$time_value) + h, 
                           predict(rq_all_past, newdata = predictors)))
}

# partially geo-pooled predictions for California
pred_ca <- pred_all_past %>%
  filter(geo_value == 'ca') %>%
  rename(median = `tau..0.500`,
         lower = `tau..0.025`,
         upper = `tau..0.975`) %>%
  full_join(ca %>% select(time_value, deaths), join_by(target_date == time_value)) %>%
  arrange(target_date)

```

## Partially geo-pooled predictions for California

```{r plot-partial-geo-pooling}
pred_ca %>%
  ggplot(aes(x = target_date)) +
  geom_line(aes(y = deaths), col = 'gray50') + 
  geom_point(aes(y = median), col = 'darkgreen') +
  geom_linerange(aes(ymin = lower, ymax = upper), col = 'darkgreen', alpha = .6) +
  geom_vline(aes(xintercept = forecast_date), lty = 2, col = 'gray50') +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "bottom", legend.title = element_blank())
```


## Final slide {.smaller}

### Thanks:

```{r qr-codes}
#| include: false
#| fig-format: png
# Code to generate QR codes to link to any external sources
qrdat <- function(text, ecl = c("L", "M", "Q", "H")) {
  x <- qrcode::qr_code(text, ecl)
  n <- nrow(x)
  s <- seq_len(n)
  tib <- tidyr::expand_grid(x = s, y = rev(s))
  tib$z <- c(x)
  tib
}
qr1 <- qrdat("https://cmu-delphi.github.io/epiprocess/")
qr2 <- qrdat("https://cmu-delphi.github.io/epipredict/")
ggplot(qr1, aes(x, y, fill = z)) +
  geom_raster() +
  ggtitle("{epiprocess}") +
  coord_equal(expand = FALSE) +
  scale_fill_manual(values = c("white", "black"), guide = "none") +
  theme_void(base_size = 18) +
  theme(plot.title = element_text(hjust = .5))
ggplot(qr2, aes(x, y, fill = z)) +
  geom_raster() +
  labs(title = "{epipredict}") +
  coord_equal(expand = FALSE) +
  scale_fill_manual(values = c("white", "black"), guide = "none") +
  theme_void(base_size = 18) +
  theme(plot.title = element_text(hjust = .5))
```

- The whole [CMU Delphi Team](https://delphi.cmu.edu/about/team/) (across many institutions)
- Optum/UnitedHealthcare, Change Healthcare.
- Google, Facebook, Amazon Web Services.
- Quidel, SafeGraph, Qualtrics.
- Centers for Disease Control and Prevention.
- Council of State and Territorial Epidemiologists


::: {layout-row=1 fig-align="center"}
![](gfx/delphi.jpg){height="100px"}
![](gfx/berkeley.jpg){height="100px"}
![](gfx/cmu.jpg){height="100px"}
![](gfx/ubc.jpg){width="250px"}
![](gfx/stanford.jpg){width="250px"}
:::


